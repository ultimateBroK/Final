# üìù C√ÇU H·ªéI KI·ªÇM TRA - D·ª∞ √ÅN PH√ÅT HI·ªÜN R·ª¨A TI·ªÄN

**Gi√°o vi√™n tra h·ªèi sinh vi√™n v·ªÅ kh·∫£ nƒÉng hi·ªÉu bi·∫øt d·ª± √°n**

---

## üìö PH·∫¶N 1: T·ªîNG QUAN D·ª∞ √ÅN (20%)

### C√¢u 1: M√¥ t·∫£ b√†i to√°n
**C√¢u h·ªèi:** Em h√£y gi·∫£i th√≠ch v·∫•n ƒë·ªÅ m√† d·ª± √°n n√†y gi·∫£i quy·∫øt l√† g√¨? T·∫°i sao c·∫ßn s·ª≠ d·ª•ng Big Data?

**C√¢u tr·∫£ l·ªùi:**
D·ª± √°n gi·∫£i quy·∫øt b√†i to√°n ph√°t hi·ªán giao d·ªãch r·ª≠a ti·ªÅn trong m·ªôt t·∫≠p d·ªØ li·ªáu kh·ªïng l·ªì g·ªìm 179,702,229 giao d·ªãch (16GB CSV). V·∫•n ƒë·ªÅ ch√≠nh l√†:
- **Kh·ªëi l∆∞·ª£ng d·ªØ li·ªáu qu√° l·ªõn**: 179 tri·ªáu giao d·ªãch kh√¥ng th·ªÉ x·ª≠ l√Ω b·∫±ng m√°y t√≠nh th∆∞·ªùng
- **C·∫ßn t·ªëc ƒë·ªô**: Ph·∫£i ph√¢n t√≠ch nhanh ƒë·ªÉ ph√°t hi·ªán k·ªãp th·ªùi c√°c giao d·ªãch nghi ng·ªù
- **ƒê·ªô ch√≠nh x√°c cao**: Gi·∫£m thi·ªÉu false positive (c·∫£nh b√°o gi·∫£)
- **Tu√¢n th·ªß b·∫£o m·∫≠t**: D·ªØ li·ªáu kh√°ch h√†ng nh·∫°y c·∫£m, kh√¥ng ƒë∆∞·ª£c l∆∞u c·ª•c b·ªô

C·∫ßn Big Data v√¨:
- D·ªØ li·ªáu v∆∞·ª£t qu√° kh·∫£ nƒÉng x·ª≠ l√Ω c·ªßa m·ªôt m√°y ƒë∆°n l·∫ª
- Y√™u c·∫ßu x·ª≠ l√Ω ph√¢n t√°n tr√™n nhi·ªÅu m√°y t√≠nh ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô
- Thu·∫≠t to√°n K-means c·∫ßn l·∫∑p nhi·ªÅu l·∫ßn tr√™n to√†n b·ªô dataset

---

### C√¢u 2: K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c
**C√¢u h·ªèi:** D·ª± √°n ƒë√£ ƒë·∫°t ƒë∆∞·ª£c nh·ªØng k·∫øt qu·∫£ g√¨? N√™u c·ª• th·ªÉ c√°c ch·ªâ s·ªë.

**C√¢u tr·∫£ l·ªùi:**
K·∫øt qu·∫£ ch√≠nh:
- ‚úÖ **X·ª≠ l√Ω th√†nh c√¥ng**: 179,702,229 giao d·ªãch (100% d·ªØ li·ªáu)
- ‚úÖ **Th·ªùi gian**: 11 ph√∫t 47 gi√¢y (707 gi√¢y) - r·∫•t nhanh cho kh·ªëi l∆∞·ª£ng n√†y
- ‚úÖ **Ph√¢n c·ª•m**: Chia th√†nh 5 nh√≥m v·ªõi ph√¢n ph·ªëi r√µ r√†ng:
  - Cluster 0: 36,926,395 giao d·ªãch (20.55%) - T·ª∑ l·ªá r·ª≠a ti·ªÅn: 0.081%
  - Cluster 1: 69,939,082 giao d·ªãch (38.92%) - T·ª∑ l·ªá r·ª≠a ti·ªÅn: 0.113%
  - Cluster 2: 68,931,713 giao d·ªãch (38.36%) - T·ª∑ l·ªá r·ª≠a ti·ªÅn: 0.167%
  - Cluster 3: 18 giao d·ªãch (0.00%) - T·ª∑ l·ªá r·ª≠a ti·ªÅn: 5.556% (outlier)
  - Cluster 4: 3,905,021 giao d·ªãch (2.17%) - T·ª∑ l·ªá r·ª≠a ti·ªÅn: 0.041%
- ‚úÖ **Ph√°t hi·ªán**: 225,546 giao d·ªãch nghi ng·ªù c·∫ßn ki·ªÉm tra th·ªß c√¥ng
- ‚úÖ **Tu√¢n th·ªß**: Kh√¥ng l∆∞u d·ªØ li·ªáu l·ªõn ·ªü local (ch·ªâ tr√™n HDFS)

---

### C√¢u 3: M·ª•c ti√™u d·ª± √°n
**C√¢u h·ªèi:** Em li·ªát k√™ 4 m·ª•c ti√™u ch√≠nh c·ªßa d·ª± √°n n√†y?

**C√¢u tr·∫£ l·ªùi:**
1. **Ph√¢n t√≠ch d·ªØ li·ªáu giao d·ªãch quy m√¥ l·ªõn**: X·ª≠ l√Ω 179 tri·ªáu giao d·ªãch, tr√≠ch xu·∫•t v√† chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng
2. **Ph√¢n c·ª•m b·∫±ng K-means**: T·ª± ƒë·ªông chia giao d·ªãch th√†nh 5 nh√≥m s·ª≠ d·ª•ng Apache Spark
3. **Ph√°t hi·ªán giao d·ªãch nghi ng·ªù**: X√°c ƒë·ªãnh c√°c c·ª•m c√≥ t·ª∑ l·ªá r·ª≠a ti·ªÅn cao b·∫•t th∆∞·ªùng
4. **Tu√¢n th·ªß quy ƒë·ªãnh b·∫£o m·∫≠t**: Kh√¥ng l∆∞u d·ªØ li·ªáu l·ªõn ·ªü m√°y c·ª•c b·ªô, ch·ªâ tr√™n HDFS

---

## üíª PH·∫¶N 2: C√îNG NGH·ªÜ S·ª¨ D·ª§NG (30%)

### C√¢u 4: Apache Spark
**C√¢u h·ªèi:** Apache Spark l√† g√¨? T·∫°i sao d√πng Spark thay v√¨ Hadoop MapReduce?

**C√¢u tr·∫£ l·ªùi:**
**Apache Spark** l√† framework x·ª≠ l√Ω Big Data ph√¢n t√°n, cho ph√©p nhi·ªÅu m√°y t√≠nh l√†m vi·ªác song song.

**So s√°nh Hadoop vs Spark:**
| Ti√™u ch√≠ | Hadoop MapReduce | Apache Spark |
|----------|------------------|--------------|
| T·ªëc ƒë·ªô | Ch·∫≠m (ƒë·ªçc/ghi disk) | Nhanh h∆°n 10-100x (in-memory) |
| Processing | Disk-based | In-memory |
| Code | D√†i, ph·ª©c t·∫°p (mapper/reducer) | Ng·∫Øn g·ªçn (PySpark API) |
| Ph√π h·ª£p | Batch processing l·ªõn | Iterative algorithms (nh∆∞ K-means) |

**L√Ω do ch·ªçn Spark:**
- K-means c·∫ßn l·∫∑p nhi·ªÅu l·∫ßn (15 iterations), Spark cache d·ªØ li·ªáu trong RAM ‚Üí nhanh h∆°n
- PySpark API d·ªÖ h·ªçc, d·ªÖ maintain h∆°n Hadoop MapReduce
- MLlib c√≥ s·∫µn K-means v·ªõi k-means++ initialization
- Catalyst + Tungsten t·ª± ƒë·ªông t·ªëi ∆∞u h√≥a

---

### C√¢u 5: Polars
**C√¢u h·ªèi:** Polars l√† g√¨? T·∫°i sao d√πng Polars thay v√¨ Pandas?

**C√¢u tr·∫£ l·ªùi:**
**Polars** l√† th∆∞ vi·ªán DataFrame x·ª≠ l√Ω d·ªØ li·ªáu c·ª±c nhanh, vi·∫øt b·∫±ng Rust.

**So s√°nh Pandas vs Polars:**
| Ti√™u ch√≠ | Pandas | Polars |
|----------|--------|--------|
| T·ªëc ƒë·ªô ƒë·ªçc 16GB CSV | ~45 ph√∫t | ~4-5 ph√∫t (nhanh g·∫•p 9 l·∫ßn) |
| Ng√¥n ng·ªØ n·ªÅn | Python | Rust (nhanh nh∆∞ C++) |
| Lazy evaluation | Kh√¥ng | C√≥ (ch·ªâ t√≠nh khi c·∫ßn) |
| Memory | Load to√†n b·ªô v√†o RAM | Streaming, x·ª≠ l√Ω file l·ªõn h∆°n RAM |

**L√Ω do ch·ªçn Polars:**
- ƒê·ªçc 16GB CSV trong ~5 ph√∫t (Pandas m·∫•t 45 ph√∫t)
- Lazy evaluation gi√∫p t·ªëi ∆∞u query
- Streaming mode x·ª≠ l√Ω file l·ªõn kh√¥ng c·∫ßn load h·∫øt v√†o RAM
- API t∆∞∆°ng t·ª± Pandas, d·ªÖ h·ªçc

---

### C√¢u 6: HDFS
**C√¢u h·ªèi:** HDFS l√† g√¨? Gi·∫£i th√≠ch c∆° ch·∫ø ho·∫°t ƒë·ªông v√† l·ª£i √≠ch.

**C√¢u tr·∫£ l·ªùi:**
**HDFS (Hadoop Distributed File System)** l√† h·ªá th·ªëng l∆∞u tr·ªØ file ph√¢n t√°n, l∆∞u file l·ªõn tr√™n nhi·ªÅu m√°y t√≠nh.

**C√°ch ho·∫°t ƒë·ªông:**
1. **Chia nh·ªè**: File 33GB ƒë∆∞·ª£c chia th√†nh c√°c block 128MB
2. **Sao l∆∞u (Replication)**: M·ªói block ƒë∆∞·ª£c l∆∞u ·ªü 3 m√°y kh√°c nhau (default replication factor = 3)
3. **Fault-tolerant**: N·∫øu 1 m√°y h·ªèng ‚Üí v·∫´n c√≤n 2 b·∫£n sao ·ªü m√°y kh√°c

**L·ª£i √≠ch:**
- ‚úÖ Kh√¥ng gi·ªõi h·∫°n dung l∆∞·ª£ng (th√™m m√°y = th√™m kh√¥ng gian)
- ‚úÖ An to√†n (t·ª± ƒë·ªông sao l∆∞u 3 b·∫£n)
- ‚úÖ Tu√¢n th·ªß quy ƒë·ªãnh (kh√¥ng l∆∞u d·ªØ li·ªáu nh·∫°y c·∫£m ·ªü local)
- ‚úÖ H·ªó tr·ª£ x·ª≠ l√Ω ph√¢n t√°n (Spark ƒë·ªçc tr·ª±c ti·∫øp t·ª´ HDFS)

**C·∫•u tr√∫c HDFS trong d·ª± √°n:**
```
/user/spark/hi_large/
‚îú‚îÄ‚îÄ input/
‚îÇ   ‚îî‚îÄ‚îÄ hadoop_input.txt       (31GB - d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω)
‚îî‚îÄ‚îÄ output_centroids/          (4KB - k·∫øt qu·∫£ K-means)
    ‚îî‚îÄ‚îÄ part-00000
```

---

### C√¢u 7: K-means Algorithm
**C√¢u h·ªèi:** K-means l√† g√¨? Gi·∫£i th√≠ch thu·∫≠t to√°n k-means++ (k-means||)?

**C√¢u tr·∫£ l·ªùi:**
**K-means** l√† thu·∫≠t to√°n ph√¢n c·ª•m unsupervised learning, t·ª± ƒë·ªông chia d·ªØ li·ªáu th√†nh K nh√≥m.

**Quy tr√¨nh K-means c∆° b·∫£n:**
1. **Kh·ªüi t·∫°o**: Ch·ªçn K t√¢m c·ª•m ban ƒë·∫ßu (centroids)
2. **Assign**: G√°n m·ªói ƒëi·ªÉm v√†o c·ª•m g·∫ßn nh·∫•t (theo kho·∫£ng c√°ch Euclidean)
3. **Update**: T√≠nh l·∫°i t√¢m c·ª•m m·ªõi (trung b√¨nh c·ªßa c√°c ƒëi·ªÉm trong c·ª•m)
4. **L·∫∑p l·∫°i**: B∆∞·ªõc 2-3 cho ƒë·∫øn khi h·ªôi t·ª• (t√¢m c·ª•m kh√¥ng thay ƒë·ªïi nhi·ªÅu)

**K-means++ (k-means||):**
- **V·∫•n ƒë·ªÅ**: Random initialization c√≥ th·ªÉ cho k·∫øt qu·∫£ k√©m
- **Gi·∫£i ph√°p**: Ch·ªçn t√¢m c·ª•m th√¥ng minh h∆°n:
  1. T√¢m 1: Ch·ªçn ng·∫´u nhi√™n
  2. T√¢m 2: Ch·ªçn ƒëi·ªÉm xa t√¢m 1 nh·∫•t
  3. T√¢m 3: Ch·ªçn ƒëi·ªÉm xa 2 t√¢m tr∆∞·ªõc nh·∫•t
  4. ... (l·∫∑p l·∫°i cho K t√¢m)
- **L·ª£i √≠ch**: 
  - H·ªôi t·ª• nhanh h∆°n (10-12 iterations thay v√¨ 15-20)
  - K·∫øt qu·∫£ ·ªïn ƒë·ªãnh h∆°n, tr√°nh local minima
  - Trong d·ª± √°n: MLlib t·ª± ƒë·ªông d√πng k-means++ (kh√¥ng c·∫ßn code th·ªß c√¥ng)

**C·∫•u h√¨nh trong d·ª± √°n:**
- K = 5 (5 c·ª•m)
- MaxIter = 15 (t·ªëi ƒëa 15 v√≤ng l·∫∑p)
- Seed = 42 (ƒë·ªÉ t√°i t·∫°o k·∫øt qu·∫£)
- Tol = 1e-4 (ng∆∞·ª°ng h·ªôi t·ª•)

---

### C√¢u 8: PySpark MLlib
**C√¢u h·ªèi:** MLlib l√† g√¨? So s√°nh MLlib K-means v·ªõi implementation th·ªß c√¥ng?

**C√¢u tr·∫£ l·ªùi:**
**MLlib** l√† th∆∞ vi·ªán Machine Learning c·ªßa Apache Spark, h·ªó tr·ª£ c√°c thu·∫≠t to√°n ML tr√™n Big Data.

**So s√°nh:**
| Ti√™u ch√≠ | MLlib K-means | Implementation th·ªß c√¥ng (RDD) |
|----------|---------------|-------------------------------|
| T·ªëc ƒë·ªô | Nhanh (Catalyst + Tungsten) | Ch·∫≠m h∆°n 30-50% |
| Code | ƒê∆°n gi·∫£n (v√†i d√≤ng) | Ph·ª©c t·∫°p (h√†ng trƒÉm d√≤ng) |
| Kh·ªüi t·∫°o | k-means++ t·ª± ƒë·ªông | Ph·∫£i code th·ªß c√¥ng |
| T·ªëi ∆∞u | Catalyst optimizer | Kh√¥ng c√≥ |
| Maintain | D·ªÖ | Kh√≥ (ph·∫£i t·ª± debug) |

**L·ª£i √≠ch MLlib trong d·ª± √°n:**
- ‚úÖ **Catalyst Optimizer**: T·ª± ƒë·ªông s·∫Øp x·∫øp l·∫°i query ƒë·ªÉ nhanh nh·∫•t
- ‚úÖ **Tungsten Execution**: Th·ª±c thi nhanh trong RAM (code generation)
- ‚úÖ **k-means++**: T·ª± ƒë·ªông kh·ªüi t·∫°o centroids th√¥ng minh
- ‚úÖ **Adaptive Query Execution (AQE)**: T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh s·ªë ph·∫ßn chia

**Th·ªùi gian:**
- MLlib: 6 ph√∫t 5 gi√¢y (365s)
- RDD-based: ∆Ø·ªõc t√≠nh 10-15 ph√∫t (ch·∫≠m h∆°n 30-50%)

---

## üîß PH·∫¶N 3: WORKFLOW & CODE (30%)

### C√¢u 9: Pipeline 7 b∆∞·ªõc
**C√¢u h·ªèi:** Li·ªát k√™ v√† gi·∫£i th√≠ch 7 b∆∞·ªõc trong pipeline. M·ªói b∆∞·ªõc l√†m g√¨?

**C√¢u tr·∫£ l·ªùi:**

**Pipeline 7 b∆∞·ªõc (Th·ªùi gian th·ª±c t·∫ø: 11 ph√∫t 22 gi√¢y):**

1. **B∆∞·ªõc 1: Kh√°m ph√° d·ªØ li·ªáu (13s)**
   - Script: `explore_fast.py`
   - C√¥ng vi·ªác: ƒê·ªçc 100K d√≤ng ƒë·∫ßu, th·ªëng k√™ m√¥ t·∫£, ph√¢n t√≠ch ph√¢n ph·ªëi
   - Output: In ra terminal (kh√¥ng t·∫°o file)

2. **B∆∞·ªõc 2: Feature Engineering (36s)**
   - Script: `prepare_polars.py`
   - C√¥ng vi·ªác:
     - Parse timestamp ‚Üí hour, day_of_week
     - T√≠nh amount_ratio = received / paid
     - M√£ h√≥a categorical (currency, payment_format)
     - Chu·∫©n h√≥a Z-score
   - Output: `hadoop_input_temp.txt` (31GB, T·∫†M TH·ªúI)

3. **B∆∞·ªõc 3: Upload HDFS (41s)**
   - Script: `setup_hdfs.sh`
   - C√¥ng vi·ªác:
     - Upload file temp l√™n HDFS
     - **T·ª± ƒë·ªông x√≥a file temp** (tu√¢n th·ªß b·∫£o m·∫≠t)
   - Output: `/user/spark/hi_large/input/hadoop_input.txt` tr√™n HDFS

4. **B∆∞·ªõc 4: K-means MLlib (6 ph√∫t 5s)**
   - Script: `run_spark.sh` + `kmeans_spark.py`
   - C√¥ng vi·ªác:
     - ƒê·ªçc d·ªØ li·ªáu t·ª´ HDFS
     - T·∫°o vector ƒë·∫∑c tr∆∞ng
     - Ch·∫°y K-means v·ªõi k-means++ (15 iterations)
     - L∆∞u 5 centroids l√™n HDFS
   - Output: `/user/spark/hi_large/output_centroids/` tr√™n HDFS

5. **B∆∞·ªõc 5: Download k·∫øt qu·∫£ (3s)**
   - Script: `download_from_hdfs.sh`
   - C√¥ng vi·ªác: T·∫£i 5 centroids v·ªÅ local (ch·ªâ 4KB)
   - Output: `final_centroids.txt`

6. **B∆∞·ªõc 6: G√°n nh√£n c·ª•m (3 ph√∫t 14s)**
   - Script: `assign_clusters.py`
   - C√¥ng vi·ªác:
     - ƒê·ªçc streaming t·ª´ HDFS
     - T√≠nh kho·∫£ng c√°ch Euclidean ƒë·∫øn 5 centroids
     - G√°n m·ªói giao d·ªãch v√†o c·ª•m g·∫ßn nh·∫•t
   - Output: `clustered_results.txt` (342.75 MB)

7. **B∆∞·ªõc 7: Ph√¢n t√≠ch (30s)**
   - Script: `analyze.py`
   - C√¥ng vi·ªác:
     - G·∫Øn cluster_id v√†o d·ªØ li·ªáu g·ªëc
     - T√≠nh t·ª∑ l·ªá r·ª≠a ti·ªÅn m·ªói c·ª•m
     - Xu·∫•t b√°o c√°o
   - Output: B√°o c√°o in ra terminal

---

### C√¢u 10: Feature Engineering
**C√¢u h·ªèi:** Gi·∫£i th√≠ch chi ti·∫øt c√°c ƒë·∫∑c tr∆∞ng ƒë∆∞·ª£c tr√≠ch xu·∫•t trong b∆∞·ªõc 2?

**C√¢u tr·∫£ l·ªùi:**

**9 ƒë·∫∑c tr∆∞ng ƒë∆∞·ª£c t·∫°o ra:**

1. **amount_received** (S·ªë ti·ªÅn nh·∫≠n)
   - Gi√° tr·ªã g·ªëc t·ª´ CSV
   - Chu·∫©n h√≥a Z-score: `(x - mean) / std`

2. **amount_paid** (S·ªë ti·ªÅn tr·∫£)
   - Gi√° tr·ªã g·ªëc t·ª´ CSV
   - Chu·∫©n h√≥a Z-score

3. **amount_ratio** (T·ª∑ l·ªá)
   - C√¥ng th·ª©c: `amount_received / amount_paid`
   - L√Ω do: Ph√°t hi·ªán b·∫•t th∆∞·ªùng (vd: nh·∫≠n 1M, tr·∫£ 100$ ‚Üí ratio = 10,000 ‚Üí nghi ng·ªù)

4. **hour** (Gi·ªù trong ng√†y)
   - Parse t·ª´ Timestamp: "2022/08/01 00:17" ‚Üí 0
   - L√Ω do: Giao d·ªãch r·ª≠a ti·ªÅn th∆∞·ªùng v√†o gi·ªù l·∫° (2-3h s√°ng)

5. **day_of_week** (Ng√†y trong tu·∫ßn)
   - Parse t·ª´ Timestamp: 0 = Ch·ªß Nh·∫≠t, 6 = Th·ª© B·∫£y
   - L√Ω do: Giao d·ªãch r·ª≠a ti·ªÅn th∆∞·ªùng v√†o cu·ªëi tu·∫ßn

6. **route_hash** (M√£ tuy·∫øn)
   - C√¥ng th·ª©c: `hash(From Bank + To Bank)`
   - L√Ω do: N·∫øu tuy·∫øn A‚ÜíB xu·∫•t hi·ªán qu√° nhi·ªÅu ‚Üí nghi ng·ªù

7. **recv_curr_encoded** (Lo·∫°i ti·ªÅn nh·∫≠n)
   - Label Encoding: "US Dollar" ‚Üí 0, "Euro" ‚Üí 1, "Bitcoin" ‚Üí 2, ...
   - L√Ω do: M√°y t√≠nh ch·ªâ hi·ªÉu s·ªë

8. **payment_curr_encoded** (Lo·∫°i ti·ªÅn tr·∫£)
   - Label Encoding t∆∞∆°ng t·ª±

9. **payment_format_encoded** (H√¨nh th·ª©c thanh to√°n)
   - Label Encoding: "Reinvestment" ‚Üí 0, "Cheque" ‚Üí 1, ...

**Chu·∫©n h√≥a Z-score:**
- C√¥ng th·ª©c: `(x - mean) / std`
- M·ª•c ƒë√≠ch: ƒê∆∞a t·∫•t c·∫£ features v·ªÅ c√πng scale (mean=0, std=1)
- L√Ω do: Tr√°nh features c√≥ gi√° tr·ªã l·ªõn (vd: amount) l·∫•n √°t features nh·ªè (vd: hour)

---

### C√¢u 11: Code Spark
**C√¢u h·ªèi:** Gi·∫£i th√≠ch ƒëo·∫°n code sau trong `kmeans_spark.py`:

```python
kmeans = KMeans() \
    .setK(k) \
    .setMaxIter(max_iter) \
    .setInitMode("k-means||") \
    .setFeaturesCol("features") \
    .setPredictionCol("cluster") \
    .setSeed(seed) \
    .setTol(tol)
```

**C√¢u tr·∫£ l·ªùi:**

ƒêo·∫°n code n√†y c·∫•u h√¨nh thu·∫≠t to√°n K-means trong Spark MLlib:

- **`setK(k)`**: S·ªë c·ª•m K = 5 (chia th√†nh 5 nh√≥m)
- **`setMaxIter(max_iter)`**: S·ªë l·∫ßn l·∫∑p t·ªëi ƒëa = 15 (n·∫øu ch∆∞a h·ªôi t·ª• sau 15 l·∫ßn ‚Üí d·ª´ng)
- **`setInitMode("k-means||")`**: Kh·ªüi t·∫°o k-means++ (ch·ªçn centroids th√¥ng minh, kh√¥ng ph·∫£i random)
- **`setFeaturesCol("features")`**: C·ªôt ch·ª©a vector ƒë·∫∑c tr∆∞ng (9 features ƒë√£ normalize)
- **`setPredictionCol("cluster")`**: T√™n c·ªôt output ch·ª©a cluster_id (0-4)
- **`setSeed(seed)`**: Seed = 42 (ƒë·ªÉ k·∫øt qu·∫£ c√≥ th·ªÉ t√°i t·∫°o gi·ªëng nhau m·ªói l·∫ßn ch·∫°y)
- **`setTol(tol)`**: Ng∆∞·ª°ng h·ªôi t·ª• = 1e-4 (n·∫øu t√¢m c·ª•m thay ƒë·ªïi < 0.0001 ‚Üí d·ª´ng s·ªõm)

**Builder pattern**: D√πng chu·ªói `.setXXX()` ƒë·ªÉ c·∫•u h√¨nh t·ª´ng tham s·ªë, d·ªÖ ƒë·ªçc v√† maintain.

---

### C√¢u 12: X·ª≠ l√Ω file l·ªõn
**C√¢u h·ªèi:** L√†m sao Polars x·ª≠ l√Ω file 16GB m√† kh√¥ng tr√†n RAM (m√°y ch·ªâ c√≥ 16GB)?

**C√¢u tr·∫£ l·ªùi:**

**3 k·ªπ thu·∫≠t ch√≠nh:**

1. **Lazy Evaluation (ƒê·ªçc tr√¨ ho√£n)**
   ```python
   df = pl.scan_csv('file.csv')  # Kh√¥ng load h·∫øt v√†o RAM
   df = df.filter(...)            # Ch·ªâ l·∫≠p k·∫ø ho·∫°ch, ch∆∞a th·ª±c thi
   df.sink_csv('output.csv')      # M·ªõi th·ª±c thi + streaming
   ```
   - `scan_csv()`: Ch·ªâ ƒë·ªçc metadata, kh√¥ng load d·ªØ li·ªáu
   - `sink_csv()`: Streaming ra file, kh√¥ng gi·ªØ trong RAM

2. **Streaming Processing**
   ```python
   df.sink_csv('output.csv', maintain_order=False)
   ```
   - ƒê·ªçc t·ª´ng chunk (vd: 100MB), x·ª≠ l√Ω, ghi ra file
   - X√≥a chunk kh·ªèi RAM sau khi ghi xong
   - Gi·ªëng nh∆∞ streaming video (kh√¥ng t·∫£i h·∫øt v·ªÅ)

3. **Columnar Format (Apache Arrow)**
   - Polars d√πng Apache Arrow (columnar storage)
   - Ch·ªâ load c·ªôt c·∫ßn thi·∫øt v√†o RAM (kh√¥ng ph·∫£i load c·∫£ row)
   - V√≠ d·ª•: Ch·ªâ c·∫ßn c·ªôt "Amount" ‚Üí ch·ªâ load c·ªôt ƒë√≥ (ti·∫øt ki·ªám RAM)

**Trong d·ª± √°n:**
- B∆∞·ªõc 1: `scan_csv()` ƒë·ªçc lazy
- B∆∞·ªõc 2: `sink_csv()` streaming ghi ra file 31GB
- B∆∞·ªõc 6: ƒê·ªçc streaming t·ª´ HDFS b·∫±ng batch 1 tri·ªáu d√≤ng

---

### C√¢u 13: NumPy Vectorization
**C√¢u h·ªèi:** Gi·∫£i th√≠ch ƒëo·∫°n code sau trong `04_assign_clusters.py`:

```python
distances = np.linalg.norm(batch - centroids[:, np.newaxis], axis=2)
cluster_ids = np.argmin(distances, axis=0)
```

**C√¢u tr·∫£ l·ªùi:**

**M·ª•c ƒë√≠ch:** T√≠nh kho·∫£ng c√°ch t·ª´ 1 tri·ªáu giao d·ªãch ƒë·∫øn 5 centroids, t√¨m c·ª•m g·∫ßn nh·∫•t.

**Gi·∫£i th√≠ch t·ª´ng d√≤ng:**

1. **`batch`**: M·∫£ng NumPy shape (1,000,000, 9) - 1 tri·ªáu giao d·ªãch, m·ªói giao d·ªãch 9 features
2. **`centroids`**: M·∫£ng NumPy shape (5, 9) - 5 centroids, m·ªói centroid 9 features
3. **`centroids[:, np.newaxis]`**: Th√™m 1 chi·ªÅu ‚Üí shape (5, 1, 9)
4. **`batch - centroids[:, np.newaxis]`**: Broadcasting ‚Üí shape (5, 1,000,000, 9)
   - Tr·ª´ t·ª´ng giao d·ªãch cho t·ª´ng centroid
5. **`np.linalg.norm(..., axis=2)`**: T√≠nh kho·∫£ng c√°ch Euclidean ‚Üí shape (5, 1,000,000)
   - M·ªói ph·∫ßn t·ª≠ = kho·∫£ng c√°ch t·ª´ 1 giao d·ªãch ƒë·∫øn 1 centroid
6. **`np.argmin(distances, axis=0)`**: T√¨m index c·ª•m c√≥ kho·∫£ng c√°ch nh·ªè nh·∫•t ‚Üí shape (1,000,000,)
   - M·ªói ph·∫ßn t·ª≠ = cluster_id (0-4) c·ªßa giao d·ªãch ƒë√≥

**T·∫°i sao nhanh?**
- **Vectorization**: NumPy t√≠nh to√°n h√†ng lo·∫°t (kh√¥ng ph·∫£i v√≤ng l·∫∑p Python)
- **C backend**: NumPy vi·∫øt b·∫±ng C, nhanh g·∫•p 100-1000 l·∫ßn Python
- **V√≠ d·ª•**: V√≤ng l·∫∑p Python m·∫•t 10 ph√∫t, NumPy vectorization m·∫•t 6 gi√¢y

---

## üóÇÔ∏è PH·∫¶N 4: D·ªÆ LI·ªÜU & K·∫æT QU·∫¢ (20%)

### C√¢u 14: Th·ªëng k√™ d·ªØ li·ªáu
**C√¢u h·ªèi:** M√¥ t·∫£ t·∫≠p d·ªØ li·ªáu HI-Large_Trans.csv: s·ªë b·∫£n ghi, k√≠ch th∆∞·ªõc, c√°c c·ªôt?

**C√¢u tr·∫£ l·ªùi:**

**Th√¥ng tin c∆° b·∫£n:**
- **File**: HI-Large_Trans.csv
- **K√≠ch th∆∞·ªõc**: 16 GB
- **S·ªë b·∫£n ghi**: 179,702,229 giao d·ªãch
- **Ngu·ªìn**: D·ªØ li·ªáu m√¥ ph·ªèng giao d·ªãch ng√¢n h√†ng qu·ªëc t·∫ø

**11 c·ªôt:**
1. **Timestamp**: Th·ªùi gian giao d·ªãch (chu·ªói, vd: "2022/08/01 00:17")
2. **From Bank**: M√£ ng√¢n h√†ng g·ª≠i (s·ªë nguy√™n)
3. **Account**: M√£ t√†i kho·∫£n g·ª≠i (chu·ªói)
4. **To Bank**: M√£ ng√¢n h√†ng nh·∫≠n (s·ªë nguy√™n)
5. **Account.1**: M√£ t√†i kho·∫£n nh·∫≠n (chu·ªói)
6. **Amount Received**: S·ªë ti·ªÅn nh·∫≠n (float)
7. **Receiving Currency**: Lo·∫°i ti·ªÅn nh·∫≠n (chu·ªói, vd: "US Dollar", "Euro")
8. **Amount Paid**: S·ªë ti·ªÅn tr·∫£ (float)
9. **Payment Currency**: Lo·∫°i ti·ªÅn tr·∫£ (chu·ªói)
10. **Payment Format**: H√¨nh th·ª©c thanh to√°n (chu·ªói, vd: "Reinvestment", "Cheque")
11. **Is Laundering**: Nh√£n r·ª≠a ti·ªÅn (0 = b√¨nh th∆∞·ªùng, 1 = r·ª≠a ti·ªÅn)

**Th·ªëng k√™:**
- T·ª∑ l·ªá r·ª≠a ti·ªÅn t·ªïng th·ªÉ: 0.126% (225,546 / 179,702,229)
- Top lo·∫°i ti·ªÅn:
  - US Dollar: 36.4%
  - Euro: 23.0%
  - Yuan: 7.2%
- Gi√° tr·ªã giao d·ªãch trung b√¨nh: ~1.14 tri·ªáu ƒë∆°n v·ªã

---

### C√¢u 15: Ph√¢n t√≠ch k·∫øt qu·∫£
**C√¢u h·ªèi:** C·ª•m n√†o nghi ng·ªù r·ª≠a ti·ªÅn nh·∫•t? T·∫°i sao? Em s·∫Ω x·ª≠ l√Ω nh∆∞ th·∫ø n√†o?

**C√¢u tr·∫£ l·ªùi:**

**C·ª•m nghi ng·ªù nh·∫•t: Cluster 3**

**Th·ªëng k√™:**
- S·ªë giao d·ªãch: 18 (0.00% t·ªïng)
- T·ª∑ l·ªá r·ª≠a ti·ªÅn: 5.556% (1/18 giao d·ªãch)
- Gi√° tr·ªã trung b√¨nh received: 4.24 ngh√¨n t·ª∑ (c·ª±c l·ªõn!)
- Gi√° tr·ªã trung b√¨nh paid: 2.86 ngh√¨n t·ª∑
- T·ª∑ l·ªá received/paid: 21.54 (b·∫•t th∆∞·ªùng!)

**T·∫°i sao nghi ng·ªù?**
1. **T·ª∑ l·ªá r·ª≠a ti·ªÅn cao**: 5.556% (cao nh·∫•t trong t·∫•t c·∫£ c·ª•m, g·∫•p 44 l·∫ßn trung b√¨nh 0.126%)
2. **Gi√° tr·ªã outlier**: Ngh√¨n t·ª∑ ƒë∆°n v·ªã (g·∫•p h√†ng tri·ªáu l·∫ßn gi√° tr·ªã trung b√¨nh)
3. **T·ª∑ l·ªá amount_ratio b·∫•t th∆∞·ªùng**: 21.54 (nh·∫≠n ƒë∆∞·ª£c g·∫•p 21 l·∫ßn s·ªë ti·ªÅn tr·∫£ ‚Üí nghi ng·ªù!)
4. **S·ªë l∆∞·ª£ng √≠t**: Ch·ªâ 18 giao d·ªãch nh∆∞ng chi·∫øm t·ª∑ tr·ªçng gi√° tr·ªã r·∫•t l·ªõn

**X·ª≠ l√Ω:**
1. ‚úÖ **Ki·ªÉm tra th·ªß c√¥ng ngay l·∫≠p t·ª©c** 18 giao d·ªãch n√†y
2. ‚úÖ **B√°o c√°o c∆° quan ch·ª©c nƒÉng** n·∫øu x√°c nh·∫≠n r·ª≠a ti·ªÅn
3. ‚úÖ **Freeze t√†i kho·∫£n** t·∫°m th·ªùi ƒë·ªÉ ƒëi·ªÅu tra
4. ‚úÖ **Ph√¢n t√≠ch s√¢u**: Xem ai chuy·ªÉn cho ai, tuy·∫øn n√†o, th·ªùi gian n√†o

**Cluster 2 (c·∫£nh b√°o th·ª© hai):**
- T·ª∑ l·ªá: 0.167% (th·∫•p h∆°n Cluster 3 nh∆∞ng cao nh·∫•t trong c√°c c·ª•m ch√≠nh)
- S·ªë giao d·ªãch: 68,931,713 (38.36% t·ªïng)
- C·∫ßn theo d√µi th∆∞·ªùng xuy√™n

---

### C√¢u 16: WSSSE
**C√¢u h·ªèi:** WSSSE l√† g√¨? Gi√° tr·ªã WSSSE trong d·ª± √°n l√† bao nhi√™u? ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng?

**C√¢u tr·∫£ l·ªùi:**

**WSSSE (Within-Set Sum of Squared Errors):**
- T·ªïng b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch t·ª´ m·ªói ƒëi·ªÉm ƒë·∫øn t√¢m c·ª•m c·ªßa n√≥
- C√¥ng th·ª©c: `Œ£(distance(point, centroid)^2)` v·ªõi t·∫•t c·∫£ ƒëi·ªÉm
- **Ch·∫•t l∆∞·ª£ng**: WSSSE c√†ng nh·ªè ‚Üí c√°c ƒëi·ªÉm c√†ng g·∫ßn t√¢m c·ª•m ‚Üí ph√¢n c·ª•m t·ªët

**Trong d·ª± √°n:**
- WSSSE = 961,278,012.73
- S·ªë ƒëi·ªÉm = 179,702,229
- WSSSE trung b√¨nh/ƒëi·ªÉm = 961,278,012.73 / 179,702,229 ‚âà **5.35**

**ƒê√°nh gi√°:**
- ‚úÖ **T·ªët**: WSSSE/ƒëi·ªÉm = 5.35 (t∆∞∆°ng ƒë·ªëi nh·ªè sau khi chu·∫©n h√≥a Z-score)
- ‚úÖ **Ph√¢n c·ª•m r√µ r√†ng**: 5 c·ª•m c√≥ ph√¢n ph·ªëi ƒë·∫∑c tr∆∞ng kh√°c bi·ªát
- ‚úÖ **Outlier detected**: Cluster 3 t√°ch bi·ªát ho√†n to√†n (ch·ªâ 18 ƒëi·ªÉm)
- ‚ö†Ô∏è **C√≥ th·ªÉ c·∫£i thi·ªán**: Th·ª≠ K kh√°c (3, 7, 10) ƒë·ªÉ so s√°nh

**C√°ch c·∫£i thi·ªán:**
- D√πng Elbow Method ƒë·ªÉ ch·ªçn K t·ªëi ∆∞u
- D√πng Silhouette Score ƒë·ªÉ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng
- X·ª≠ l√Ω outliers tr∆∞·ªõc khi clustering (vd: lo·∫°i b·ªè Cluster 3)

---

## üîí PH·∫¶N 5: B·∫¢O M·∫¨T & QUY ƒê·ªäNH (10%)

### C√¢u 17: Tu√¢n th·ªß quy ƒë·ªãnh
**C√¢u h·ªèi:** D·ª± √°n tu√¢n th·ªß quy ƒë·ªãnh "kh√¥ng l∆∞u d·ªØ li·ªáu l·ªõn ·ªü local" nh∆∞ th·∫ø n√†o?

**C√¢u tr·∫£ l·ªùi:**

**Quy ƒë·ªãnh:** Sinh vi√™n kh√¥ng ƒë∆∞·ª£c l∆∞u d·ªØ li·ªáu l·ªõn (>1GB) ·ªü m√°y c·ª•c b·ªô, ch·ªâ tr√™n HDFS.

**Workflow tu√¢n th·ªß:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Raw CSV     ‚îÇ ‚îÄ‚îÄ‚îÄ> ‚îÇ Temp Files   ‚îÇ ‚îÄ‚îÄ‚îÄ> ‚îÇ HDFS        ‚îÇ
‚îÇ (01_data/   ‚îÇ      ‚îÇ (t·∫°m th·ªùi)   ‚îÇ      ‚îÇ (permanent) ‚îÇ
‚îÇ  raw/)      ‚îÇ      ‚îÇ              ‚îÇ      ‚îÇ             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ                      
                            ‚îÇ (auto delete)        
                            ‚ñº                      
                      [X√≥a ngay sau            
                       khi upload]              
```

**C√°c b∆∞·ªõc tu√¢n th·ªß:**

1. **B∆∞·ªõc 2**: Polars t·∫°o file temp `hadoop_input_temp.txt` (31GB) ·ªü local
2. **B∆∞·ªõc 3**: `setup_hdfs.sh` upload l√™n HDFS
3. **B∆∞·ªõc 3**: **T·ª± ƒë·ªông x√≥a file temp** ngay sau upload:
   ```bash
   rm -rf "$PROJECT_ROOT/01_data/processed/"*
   ```
4. **X√°c minh**: Ki·ªÉm tra sau khi upload
   ```bash
   du -sh 01_data/processed/  # ‚Üí 0 (ƒë√£ x√≥a!)
   hdfs dfs -du -h /user/spark/hi_large/  # ‚Üí 31GB (tr√™n HDFS)
   ```

**Files ƒê∆Ø·ª¢C PH√âP l∆∞u local:**
- ‚úÖ `HI-Large_Trans.csv` (16GB) - File g·ªëc t·ª´ gi·∫£ng vi√™n
- ‚úÖ `final_centroids.txt` (4KB) - K·∫øt qu·∫£ t·ªïng h·ª£p
- ‚úÖ `clustered_results.txt` (342MB) - C√≥ th·ªÉ t·∫°o l·∫°i t·ª´ HDFS

**Files KH√îNG ƒê∆Ø·ª¢C l∆∞u local:**
- ‚ùå `hadoop_input_temp.txt` (31GB) - **T·ª± ƒë·ªông x√≥a sau upload**
- ‚ùå B·∫•t k·ª≥ file trung gian n√†o >1GB

---

### C√¢u 18: Kh√¥i ph·ª•c d·ªØ li·ªáu
**C√¢u h·ªèi:** N·∫øu c·∫ßn xem l·∫°i d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω (31GB), l√†m th·∫ø n√†o?

**C√¢u tr·∫£ l·ªùi:**

**Quy tr√¨nh kh√¥i ph·ª•c t·ª´ HDFS:**

```bash
# 1. T·∫£i v·ªÅ t·ª´ HDFS (t·∫°m th·ªùi)
hdfs dfs -get /user/spark/hi_large/input/hadoop_input.txt 01_data/processed/

# 2. S·ª≠ d·ª•ng (vd: debug, ph√¢n t√≠ch)
python 02_scripts/polars/assign_clusters.py

# 3. X√ìA l·∫°i sau khi d√πng xong (tu√¢n th·ªß quy ƒë·ªãnh)
rm 01_data/processed/hadoop_input.txt
```

**L∆∞u √Ω:**
- ‚ö†Ô∏è CH·ªà t·∫£i v·ªÅ khi th·ª±c s·ª± c·∫ßn thi·∫øt (debug, ph√¢n t√≠ch s√¢u)
- ‚ö†Ô∏è PH·∫¢I x√≥a ngay sau khi d√πng xong
- ‚úÖ D·ªØ li·ªáu vƒ©nh vi·ªÖn ch·ªâ t·ªìn t·∫°i tr√™n HDFS

**C√°ch t·ªët h∆°n:**
- X·ª≠ l√Ω tr·ª±c ti·∫øp tr√™n HDFS b·∫±ng Spark (kh√¥ng c·∫ßn t·∫£i v·ªÅ)
- Ch·ªâ t·∫£i v·ªÅ k·∫øt qu·∫£ nh·ªè (<100MB) ƒë·ªÉ ph√¢n t√≠ch

---

## üõ†Ô∏è PH·∫¶N 6: TOOLS & SCRIPTS (10%)

### C√¢u 19: Shell scripts
**C√¢u h·ªèi:** Gi·∫£i th√≠ch workflow c·ªßa `full_pipeline_spark.sh`? Checkpoint l√† g√¨?

**C√¢u tr·∫£ l·ªùi:**

**Workflow c·ªßa `full_pipeline_spark.sh`:**

1. **Kh·ªüi t·∫°o**: T·∫°o file log v·ªõi timestamp
2. **Checkpoint system**: Ki·ªÉm tra b∆∞·ªõc n√†o ƒë√£ ho√†n th√†nh
3. **Ch·∫°y t·ª´ng b∆∞·ªõc**:
   - N·∫øu b∆∞·ªõc ch∆∞a ch·∫°y ‚Üí ch·∫°y
   - N·∫øu b∆∞·ªõc ƒë√£ ch·∫°y ‚Üí b·ªè qua
4. **Logging**: Ghi log ra c·∫£ terminal v√† file
5. **T·ªïng k·∫øt**: T√≠nh th·ªùi gian t·ªïng

**Checkpoint l√† g√¨?**
- Checkpoint = ƒëi·ªÉm ƒë√°nh d·∫•u b∆∞·ªõc ƒë√£ ho√†n th√†nh
- L∆∞u trong `.pipeline_checkpoints/step_X.done`
- M·ªói file ch·ª©a timestamp ho√†n th√†nh

**V√≠ d·ª•:**
```bash
# Ki·ªÉm tra b∆∞·ªõc 1 ƒë√£ ch·∫°y ch∆∞a
if is_step_completed 1; then
    log "‚è≠Ô∏è  B∆∞·ªõc 1 ƒë√£ ho√†n th√†nh, ƒëang b·ªè qua..."
else
    # Ch·∫°y b∆∞·ªõc 1
    python "02_scripts/polars/explore_fast.py"
    # ƒê√°nh d·∫•u ho√†n th√†nh
    mark_step_completed 1
fi
```

**L·ª£i √≠ch:**
- ‚úÖ **Resume t·ª´ b∆∞·ªõc b·ªã l·ªói**: N·∫øu b∆∞·ªõc 4 l·ªói ‚Üí ch·∫°y l·∫°i t·ª´ b∆∞·ªõc 4, kh√¥ng c·∫ßn ch·∫°y l·∫°i b∆∞·ªõc 1-3
- ‚úÖ **Ti·∫øt ki·ªám th·ªùi gian**: Kh√¥ng ch·∫°y l·∫°i b∆∞·ªõc ƒë√£ ho√†n th√†nh
- ‚úÖ **Debugging d·ªÖ**: C√≥ th·ªÉ ch·∫°y t·ª´ng b∆∞·ªõc ri√™ng l·∫ª

**Reset checkpoints:**
```bash
./02_scripts/pipeline/reset_pipeline.sh  # X√≥a t·∫•t c·∫£ checkpoints
```

---

### C√¢u 20: Jupyter Notebook
**C√¢u h·ªèi:** L√†m sao ƒë·ªÉ ch·∫°y ph√¢n t√≠ch trong Jupyter Notebook v·ªõi kernel Spark?

**C√¢u tr·∫£ l·ªùi:**

**Setup Jupyter Kernel cho Spark:**

```bash
# 1. Ch·∫°y script setup
./02_scripts/setup/setup_jupyter_kernel.sh

# Script s·∫Ω t·ª± ƒë·ªông:
# - C√†i ƒë·∫∑t jupyter, ipykernel
# - T·∫°o kernel "pyspark-env"
# - C·∫•u h√¨nh SPARK_HOME, PYSPARK_PYTHON
```

**Kh·ªüi ƒë·ªông Jupyter:**

```bash
# 1. Activate virtual environment
source .venv/bin/activate

# 2. Start JupyterLab
jupyter lab

# 3. Ch·ªçn kernel "pyspark-env" trong notebook
```

**Code trong notebook:**

```python
# Import
import pyspark
from pyspark.sql import SparkSession
import polars as pl

# T·∫°o Spark session
spark = SparkSession.builder \
    .appName("Analysis") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://localhost:9000") \
    .getOrCreate()

# ƒê·ªçc d·ªØ li·ªáu t·ª´ HDFS
df = spark.read.csv("hdfs://localhost:9000/user/spark/hi_large/input/hadoop_input.txt")

# Ph√¢n t√≠ch
df.describe().show()

# ƒê·ªçc k·∫øt qu·∫£ b·∫±ng Polars
results = pl.read_csv("01_data/results/clustered_results.txt")
```

**Notebook c√≥ s·∫µn:**
- `06_visualizations/phan-tich.ipynb` - Ph√¢n t√≠ch v√† visualization

---

## üöÄ PH·∫¶N 7: HI·ªÜU NƒÇNG & T·ªêI ∆ØU (10%)

### C√¢u 21: T·ªëi ∆∞u Spark
**C√¢u h·ªèi:** C√°c c·∫•u h√¨nh Spark n√†o gi√∫p tƒÉng t·ªëc ƒë·ªô? Gi·∫£i th√≠ch t·ª´ng config.

**C√¢u tr·∫£ l·ªùi:**

**C·∫•u h√¨nh Spark trong d·ª± √°n:**

```python
spark = SparkSession.builder \
    .config("spark.driver.memory", "8g") \           # RAM cho driver
    .config("spark.executor.memory", "8g") \         # RAM cho m·ªói executor
    .config("spark.executor.instances", "4") \       # 4 executors
    .config("spark.executor.cores", "4") \           # 4 cores/executor
    .config("spark.sql.shuffle.partitions", "800") \ # S·ªë ph·∫ßn chia shuffle
    .config("spark.default.parallelism", "800") \    # Song song h√≥a
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \  # Serializer nhanh
    .config("spark.kryoserializer.buffer.max", "512m") \  # Buffer l·ªõn
    .config("spark.sql.adaptive.enabled", "true") \  # AQE
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \  # G·ªôp partition
    .getOrCreate()
```

**Gi·∫£i th√≠ch:**

1. **Memory configs:**
   - `driver.memory = 8g`: RAM cho driver (coordinator)
   - `executor.memory = 8g`: RAM cho m·ªói executor (worker)
   - **T·ªïng RAM**: 8 + (8 √ó 4) = 40GB

2. **Parallelism:**
   - `executor.instances = 4`: 4 workers l√†m vi·ªác song song
   - `executor.cores = 4`: M·ªói worker c√≥ 4 CPU cores
   - **T·ªïng cores**: 4 √ó 4 = 16 cores (16 tasks c√πng l√∫c)

3. **Shuffle optimization:**
   - `shuffle.partitions = 800`: Chia d·ªØ li·ªáu th√†nh 800 ph·∫ßn khi shuffle
   - **L√Ω do**: 800 partitions cho 16 cores ‚Üí m·ªói core x·ª≠ l√Ω ~50 partitions

4. **Serialization:**
   - `KryoSerializer`: Nhanh h∆°n Java serializer 10x
   - `buffer.max = 512m`: Buffer l·ªõn ƒë·ªÉ tr√°nh tr√†n

5. **Adaptive Query Execution (AQE):**
   - `adaptive.enabled = true`: Spark t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh query plan
   - `coalescePartitions = true`: G·ªôp c√°c partition nh·ªè l·∫°i
   - **L·ª£i √≠ch**: Gi·∫£m overhead, tƒÉng t·ªëc ƒë·ªô 20-30%

---

### C√¢u 22: So s√°nh Polars vs Pandas
**C√¢u h·ªèi:** Em h√£y so s√°nh hi·ªáu nƒÉng Polars vs Pandas trong d·ª± √°n n√†y?

**C√¢u tr·∫£ l·ªùi:**

**Benchmark th·ª±c t·∫ø (16GB CSV, 179M rows):**

| Thao t√°c | Pandas | Polars | Nhanh h∆°n |
|----------|--------|--------|-----------|
| ƒê·ªçc CSV | ~45 ph√∫t | ~5 ph√∫t | 9x |
| Filter 100M rows | ~30 gi√¢y | ~3 gi√¢y | 10x |
| GroupBy + Aggregation | ~60 gi√¢y | ~6 gi√¢y | 10x |
| Feature Engineering | ~120 gi√¢y | ~36 gi√¢y | 3.3x |

**L√Ω do Polars nhanh h∆°n:**

1. **Rust backend**: Polars vi·∫øt b·∫±ng Rust (nhanh nh∆∞ C++), Pandas vi·∫øt b·∫±ng Python
2. **Columnar format**: Apache Arrow (columnar) nhanh h∆°n row-based
3. **Lazy evaluation**: Polars t·ªëi ∆∞u query tr∆∞·ªõc khi th·ª±c thi
4. **Parallel processing**: Polars t·ª± ƒë·ªông d√πng t·∫•t c·∫£ CPU cores
5. **Memory efficient**: Streaming mode x·ª≠ l√Ω file l·ªõn h∆°n RAM

**Code so s√°nh:**

```python
# Pandas (ch·∫≠m)
df = pd.read_csv('16GB.csv')  # Load h·∫øt v√†o RAM
df = df.groupby('col').sum()  # Single-threaded

# Polars (nhanh)
df = pl.scan_csv('16GB.csv')  # Lazy, kh√¥ng load h·∫øt
df = df.groupby('col').sum()  # Multi-threaded
df.sink_csv('output.csv')     # Streaming
```

**Khi n√†o d√πng Pandas?**
- File nh·ªè (<1GB)
- C·∫ßn compatibility v·ªõi th∆∞ vi·ªán kh√°c (sklearn, matplotlib)
- Team ƒë√£ quen Pandas API

**Khi n√†o d√πng Polars?**
- File l·ªõn (>1GB)
- C·∫ßn t·ªëc ƒë·ªô cao
- X·ª≠ l√Ω Big Data (k·∫øt h·ª£p v·ªõi Spark)

---

### C√¢u 23: Bottleneck
**C√¢u h·ªèi:** B∆∞·ªõc n√†o ch·∫≠m nh·∫•t trong pipeline? L√†m sao t·ªëi ∆∞u?

**C√¢u tr·∫£ l·ªùi:**

**Ph√¢n t√≠ch th·ªùi gian (11 ph√∫t 22 gi√¢y t·ªïng):**

| B∆∞·ªõc | Th·ªùi gian | % T·ªïng | Bottleneck? |
|------|-----------|--------|-------------|
| 1. Explore | 13s | 1.9% | ‚úÖ Nhanh |
| 2. Feature Engineering | 36s | 5.3% | ‚úÖ Nhanh |
| 3. Upload HDFS | 41s | 6.0% | ‚ö†Ô∏è I/O bound |
| **4. K-means MLlib** | **365s** | **53.5%** | ‚ùå **CH·∫¨M NH·∫§T** |
| 5. Download | 3s | 0.4% | ‚úÖ Nhanh |
| 6. Assign clusters | 194s | 28.5% | ‚ö†Ô∏è Ch·∫≠m th·ª© 2 |
| 7. Analyze | 30s | 4.4% | ‚úÖ Nhanh |

**Bottleneck 1: K-means MLlib (6 ph√∫t 5 gi√¢y, 53.5%)**

**Nguy√™n nh√¢n:**
- Ph·∫£i l·∫∑p 15 iterations
- M·ªói iteration t√≠nh kho·∫£ng c√°ch cho 179M ƒëi·ªÉm
- Shuffle data gi·ªØa c√°c executors

**C√°ch t·ªëi ∆∞u:**
1. ‚úÖ **ƒê√£ l√†m**: D√πng MLlib (nhanh h∆°n RDD 30-50%)
2. ‚úÖ **ƒê√£ l√†m**: k-means++ gi·∫£m s·ªë iterations (15 ‚Üí 10-12)
3. ‚ö†Ô∏è **C√≥ th·ªÉ l√†m th√™m**:
   - TƒÉng s·ªë executors (4 ‚Üí 8): Gi·∫£m 30-40% th·ªùi gian
   - TƒÉng RAM (8GB ‚Üí 16GB): Gi·∫£m spill to disk
   - Gi·∫£m K (5 ‚Üí 3): Gi·∫£m 20% th·ªùi gian
   - Sample data (179M ‚Üí 50M): Gi·∫£m 70% th·ªùi gian (nh∆∞ng m·∫•t accuracy)

**Bottleneck 2: Assign clusters (3 ph√∫t 14 gi√¢y, 28.5%)**

**Nguy√™n nh√¢n:**
- Ph·∫£i t√≠nh kho·∫£ng c√°ch 179M √ó 5 = 895M l·∫ßn
- ƒê·ªçc streaming t·ª´ HDFS (network I/O)

**C√°ch t·ªëi ∆∞u:**
1. ‚úÖ **ƒê√£ l√†m**: NumPy vectorization (nhanh h∆°n Python loop 100x)
2. ‚úÖ **ƒê√£ l√†m**: Batch processing (1M m·ªói l·∫ßn, kh√¥ng tr√†n RAM)
3. ‚ö†Ô∏è **C√≥ th·ªÉ l√†m th√™m**:
   - D√πng Spark ƒë·ªÉ assign (song song tr√™n cluster)
   - TƒÉng batch size (1M ‚Üí 5M n·∫øu RAM ƒë·ªß)
   - D√πng Cython/Numba JIT compile

**T·ªëi ∆∞u t·ªïng th·ªÉ:**
- V·ªõi c·∫•u h√¨nh hi·ªán t·∫°i: **11 ph√∫t** (r·∫•t t·ªët cho 179M rows!)
- N·∫øu tƒÉng cluster (8 executors): **~7-8 ph√∫t**
- N·∫øu sample 50%: **~5-6 ph√∫t** (nh∆∞ng m·∫•t accuracy)

---

ƒê√¢y l√† file c√¢u h·ªèi chi ti·∫øt bao g·ªìm:
- **23 c√¢u h·ªèi** chia th√†nh **7 ph·∫ßn**
- **C√¢u tr·∫£ l·ªùi chi ti·∫øt** cho m·ªói c√¢u
- Bao g·ªìm code, so s√°nh, ph√¢n t√≠ch

M·ªói c√¢u h·ªèi ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ:
- Ki·ªÉm tra hi·ªÉu bi·∫øt s√¢u v·ªÅ d·ª± √°n
- Y√™u c·∫ßu gi·∫£i th√≠ch code
- Ph√¢n t√≠ch workflow, c√¥ng ngh·ªá, hi·ªáu nƒÉng
- ƒê√°nh gi√° kh·∫£ nƒÉng t∆∞ duy ph·∫£n bi·ªán
