# üéì PH·ªéNG V·∫§N SINH VI√äN - CHI TI·∫æT CODE & WORKFLOW

**Gi√°o vi√™n tra h·ªèi ƒë·ªÉ ki·ªÉm tra kh·∫£ nƒÉng hi·ªÉu bi·∫øt s√¢u v·ªÅ d·ª± √°n**

---

## üîç PH·∫¶N 1: HI·ªÇU BI·∫æT CODE - FEATURE ENGINEERING (30%)

### ‚ùì C√¢u 1: Parse Timestamp
**Gi√°o vi√™n h·ªèi:** Em gi·∫£i th√≠ch chi ti·∫øt ƒëo·∫°n code n√†y trong `prepare_polars.py`:

```python
pl.col('Timestamp').str.strptime(pl.Datetime, format='%Y/%m/%d %H:%M').dt.hour().alias('hour')
```

**Sinh vi√™n tr·∫£ l·ªùi:**

**Ph√¢n t√≠ch t·ª´ng ph·∫ßn:**

1. **`pl.col('Timestamp')`**: Ch·ªçn c·ªôt Timestamp
   - Input: Chu·ªói vƒÉn b·∫£n "2022/08/01 00:17"

2. **`.str.strptime(pl.Datetime, format='%Y/%m/%d %H:%M')`**: Parse chu·ªói th√†nh datetime
   - `strptime` = string parse time
   - `format='%Y/%m/%d %H:%M'`: Template ƒë·ªÉ parse
     - `%Y`: NƒÉm 4 ch·ªØ s·ªë (2022)
     - `%m`: Th√°ng (08)
     - `%d`: Ng√†y (01)
     - `%H`: Gi·ªù 24h (00)
     - `%M`: Ph√∫t (17)
   - Output: Polars Datetime object

3. **`.dt.hour()`**: Tr√≠ch xu·∫•t gi·ªù t·ª´ datetime
   - Accessor `.dt` ƒë·ªÉ truy c·∫≠p c√°c thu·ªôc t√≠nh datetime
   - `.hour()` l·∫•y gi·ªù (0-23)
   - Output: S·ªë nguy√™n 0

4. **`.alias('hour')`**: ƒê·∫∑t t√™n c·ªôt m·ªõi l√† "hour"

**T·∫°i sao c·∫ßn parse timestamp?**
- Giao d·ªãch r·ª≠a ti·ªÅn th∆∞·ªùng x·∫£y ra v√†o gi·ªù kh√¥ng b√¨nh th∆∞·ªùng (2-3h s√°ng)
- K-means c·∫ßn ƒë·∫∑c tr∆∞ng s·ªë, kh√¥ng th·ªÉ x·ª≠ l√Ω chu·ªói "2022/08/01 00:17"
- Tr√≠ch xu·∫•t gi·ªù gi√∫p ph√°t hi·ªán pattern theo th·ªùi gian

**V√≠ d·ª•:**
```
Input:  "2022/08/01 00:17"
Step 1: Datetime(2022, 8, 1, 0, 17)
Step 2: 0 (gi·ªù)
Output: hour = 0
```

---

### ‚ùì C√¢u 2: Amount Ratio
**Gi√°o vi√™n h·ªèi:** Gi·∫£i th√≠ch ƒëo·∫°n code t√≠nh `amount_ratio`:

```python
(pl.col('Amount Received') / (pl.col('Amount Paid') + 1e-6)).alias('amount_ratio')
```

**T·∫°i sao ph·∫£i c·ªông `1e-6`? ƒêi·ªÅu g√¨ x·∫£y ra n·∫øu kh√¥ng c·ªông?**

**Sinh vi√™n tr·∫£ l·ªùi:**

**Ph√¢n t√≠ch:**

1. **C√¥ng th·ª©c**: `amount_ratio = amount_received / amount_paid`
   - ƒêo t·ª∑ l·ªá ti·ªÅn nh·∫≠n so v·ªõi ti·ªÅn tr·∫£
   - V√≠ d·ª•: Nh·∫≠n 1000$, tr·∫£ 500$ ‚Üí ratio = 2.0

2. **T·∫°i sao c·ªông `1e-6` (0.000001)?**
   - **V·∫•n ƒë·ªÅ**: `Amount Paid` c√≥ th·ªÉ b·∫±ng 0 ‚Üí chia cho 0 = l·ªói!
   - **Gi·∫£i ph√°p**: C·ªông `1e-6` (s·ªë r·∫•t nh·ªè) ƒë·ªÉ tr√°nh chia cho 0
   - `1e-6` ƒë·ªß nh·ªè ƒë·ªÉ kh√¥ng ·∫£nh h∆∞·ªüng k·∫øt qu·∫£ khi Amount Paid > 0

3. **N·∫øu KH√îNG c·ªông `1e-6`:**
   ```python
   # V√≠ d·ª• c√≥ giao d·ªãch v·ªõi Amount Paid = 0
   amount_ratio = 1000 / 0  # ‚Üí ZeroDivisionError!
   # ‚Üí Pipeline b·ªã crash!
   ```

4. **V·ªõi `1e-6`:**
   ```python
   amount_ratio = 1000 / (0 + 0.000001)
   amount_ratio = 1000000000  # R·∫•t l·ªõn (ƒë∆∞·ª£c chu·∫©n h√≥a sau)
   # ‚Üí Kh√¥ng l·ªói, K-means v·∫´n ch·∫°y ƒë∆∞·ª£c
   ```

**√ù nghƒ©a business:**
- Ratio b·∫•t th∆∞·ªùng (v√≠ d·ª• >>10) c√≥ th·ªÉ l√† d·∫•u hi·ªáu r·ª≠a ti·ªÅn
- V√≠ d·ª•: Nh·∫≠n 10 tri·ªáu$, ch·ªâ tr·∫£ 100$ ‚Üí ratio = 100,000 (nghi ng·ªù!)

**Trade-off:**
- ‚úÖ Tr√°nh crash do chia cho 0
- ‚ö†Ô∏è T·∫°o ra outlier (ratio r·∫•t l·ªõn) khi Amount Paid g·∫ßn 0
- ‚úÖ Chu·∫©n h√≥a Z-score sau s·∫Ω x·ª≠ l√Ω outlier n√†y

---

### ‚ùì C√¢u 3: Route Hash
**Gi√°o vi√™n h·ªèi:** Gi·∫£i th√≠ch ƒëo·∫°n code:

```python
(pl.col('From Bank').hash() ^ pl.col('To Bank').hash()).alias('route_hash')
```

**Em gi·∫£i th√≠ch:
1. `.hash()` l√†m g√¨?
2. To√°n t·ª≠ `^` (XOR) l√† g√¨? T·∫°i sao d√πng XOR thay v√¨ `+` ho·∫∑c `*`?
3. Cho v√≠ d·ª• c·ª• th·ªÉ v·ªõi s·ªë**

**Sinh vi√™n tr·∫£ l·ªùi:**

**1. `.hash()` l√†m g√¨?**
- Chuy·ªÉn gi√° tr·ªã th√†nh s·ªë hash (integer 64-bit)
- Hash function: Deterministic (c√πng input ‚Üí c√πng output)
- V√≠ d·ª•:
  ```
  From Bank = 20   ‚Üí hash(20) = 8734523874523 (v√≠ d·ª•)
  To Bank = 3196   ‚Üí hash(3196) = 9283745982374 (v√≠ d·ª•)
  ```

**2. To√°n t·ª≠ `^` (XOR - Exclusive OR):**
- XOR: Ph√©p to√°n bit-wise
- Quy t·∫Øc: `a ^ b = 1` n·∫øu a ‚â† b, `= 0` n·∫øu a = b

**T·∫°i sao d√πng XOR thay v√¨ `+` ho·∫∑c `*`?**

| To√°n t·ª≠ | V·∫•n ƒë·ªÅ | V√≠ d·ª• v·∫•n ƒë·ªÅ |
|---------|--------|--------------|
| `+` | Kh√¥ng ph√¢n bi·ªát h∆∞·ªõng | `hash(A) + hash(B) = hash(B) + hash(A)` ‚Üí A‚ÜíB gi·ªëng B‚ÜíA |
| `*` | T∆∞∆°ng t·ª± `+` | `hash(A) * hash(B) = hash(B) * hash(A)` ‚Üí A‚ÜíB gi·ªëng B‚ÜíA |
| `^` | ‚úÖ Ph√¢n bi·ªát h∆∞·ªõng | `hash(A) ^ hash(B) ‚â† hash(B) ^ hash(A)` (th·ª±c ra b·∫±ng nhau, nh∆∞ng k·∫øt h·ª£p v·ªõi th·ª© t·ª± c·ªôt) |

**Th·ª±c t·∫ø:** XOR c≈©ng c√≥ t√≠nh giao ho√°n (`a ^ b = b ^ a`), nh∆∞ng trong code n√†y:
- Polars ƒë·ªçc theo th·ª© t·ª±: From Bank tr∆∞·ªõc, To Bank sau
- Route t·ª´ A‚ÜíB v√† B‚ÜíA s·∫Ω c√≥ hash gi·ªëng nhau
- **M·ª•c ƒë√≠ch**: T·∫°o m√£ duy nh·∫•t cho m·ªói c·∫∑p ng√¢n h√†ng (kh√¥ng ph√¢n bi·ªát h∆∞·ªõng)

**3. V√≠ d·ª• c·ª• th·ªÉ:**

```python
# Gi·∫£ s·ª≠ hash ƒë∆°n gi·∫£n h√≥a
From Bank = 20   ‚Üí hash = 10010 (binary)
To Bank = 3196   ‚Üí hash = 11001 (binary)

XOR:
  10010
^ 11001
-------
  01011  = 11 (decimal)

‚Üí route_hash = 11
```

**√ù nghƒ©a:**
- M·ªói tuy·∫øn chuy·ªÉn ti·ªÅn (A‚ÜíB) c√≥ m·ªôt m√£ duy nh·∫•t
- N·∫øu tuy·∫øn n√†y xu·∫•t hi·ªán qu√° nhi·ªÅu ‚Üí nghi ng·ªù r·ª≠a ti·ªÅn
- K-means s·∫Ω nh√≥m c√°c giao d·ªãch c√≥ c√πng route_hash

**L∆∞u √Ω:** Trong th·ª±c t·∫ø, Polars hash() tr·∫£ v·ªÅ s·ªë r·∫•t l·ªõn (64-bit), kh√¥ng ph·∫£i s·ªë nh·ªè nh∆∞ v√≠ d·ª•.

---

### ‚ùì C√¢u 4: Label Encoding
**Gi√°o vi√™n h·ªèi:** Gi·∫£i th√≠ch ƒëo·∫°n code:

```python
pl.col('recv_curr').cast(pl.Categorical).to_physical().alias('recv_curr_encoded')
```

**Em gi·∫£i th√≠ch t·ª´ng b∆∞·ªõc v√† cho v√≠ d·ª• v·ªõi d·ªØ li·ªáu th·ª±c t·∫ø.**

**Sinh vi√™n tr·∫£ l·ªùi:**

**Ph√¢n t√≠ch t·ª´ng b∆∞·ªõc:**

1. **`pl.col('recv_curr')`**: Ch·ªçn c·ªôt "Receiving Currency"
   - Ki·ªÉu: Chu·ªói (String)
   - V√≠ d·ª•: ["US Dollar", "Euro", "Yuan", "US Dollar", "Bitcoin"]

2. **`.cast(pl.Categorical)`**: Chuy·ªÉn sang ki·ªÉu Categorical
   - Polars l∆∞u chu·ªói th√†nh 2 ph·∫ßn:
     - **Dictionary**: ["US Dollar" ‚Üí 0, "Euro" ‚Üí 1, "Yuan" ‚Üí 2, "Bitcoin" ‚Üí 3]
     - **Indices**: [0, 1, 2, 0, 3]
   - Ti·∫øt ki·ªám b·ªô nh·ªõ (l∆∞u s·ªë thay v√¨ chu·ªói)

3. **`.to_physical()`**: L·∫•y indices (s·ªë)
   - Chuy·ªÉn t·ª´ Categorical ‚Üí Integer
   - Output: [0, 1, 2, 0, 3]

4. **`.alias('recv_curr_encoded')`**: ƒê·∫∑t t√™n c·ªôt m·ªõi

**V√≠ d·ª• chi ti·∫øt:**

```python
# D·ªØ li·ªáu g·ªëc (5 giao d·ªãch)
recv_curr = [
    "US Dollar",
    "Euro",
    "Yuan",
    "US Dollar",
    "Bitcoin"
]

# B∆∞·ªõc 1: cast(pl.Categorical)
# T·∫°o dictionary:
#   "US Dollar" ‚Üí 0
#   "Euro"      ‚Üí 1
#   "Yuan"      ‚Üí 2
#   "Bitcoin"   ‚Üí 3
# (Th·ª© t·ª± ph·ª• thu·ªôc v√†o th·ª© t·ª± xu·∫•t hi·ªán l·∫ßn ƒë·∫ßu)

# B∆∞·ªõc 2: to_physical()
recv_curr_encoded = [0, 1, 2, 0, 3]

# K·∫øt qu·∫£
DataFrame:
  recv_curr     | recv_curr_encoded
  --------------|------------------
  US Dollar     | 0
  Euro          | 1
  Yuan          | 2
  US Dollar     | 0
  Bitcoin       | 3
```

**T·∫°i sao c·∫ßn Label Encoding?**
- K-means ch·ªâ hi·ªÉu s·ªë, kh√¥ng hi·ªÉu chu·ªói
- Chuy·ªÉn "US Dollar" ‚Üí 0, "Euro" ‚Üí 1 ƒë·ªÉ thu·∫≠t to√°n x·ª≠ l√Ω ƒë∆∞·ª£c
- Ti·∫øt ki·ªám b·ªô nh·ªõ: Chu·ªói "US Dollar" (9 bytes) ‚Üí s·ªë 0 (4 bytes)

**L∆∞u √Ω:**
- S·ªë ƒë∆∞·ª£c g√°n theo th·ª© t·ª± xu·∫•t hi·ªán, kh√¥ng ph·∫£i alphabetical
- "US Dollar" ‚Üí 0 kh√¥ng c√≥ nghƒ©a "US Dollar" nh·ªè h∆°n "Euro" (1)
- Ch·ªâ l√† c√°ch m√£ h√≥a, kh√¥ng c√≥ √Ω nghƒ©a th·ª© t·ª±

---

### ‚ùì C√¢u 5: Z-score Normalization
**Gi√°o vi√™n h·ªèi:** Gi·∫£i th√≠ch ƒëo·∫°n code chu·∫©n h√≥a:

```python
df_normalized = df_numeric.select([
    ((pl.col(c) - pl.col(c).mean()) / pl.col(c).std()).alias(c)
    for c in df_numeric.collect_schema().names()
])
```

**Em h√£y:
1. Gi·∫£i th√≠ch c√¥ng th·ª©c `(x - mean) / std`
2. T·∫°i sao kh√¥ng d√πng Min-Max normalization `(x - min) / (max - min)`?
3. Cho v√≠ d·ª• c·ª• th·ªÉ v·ªõi s·ªë li·ªáu**

**Sinh vi√™n tr·∫£ l·ªùi:**

**1. C√¥ng th·ª©c Z-score:**

```
z = (x - mean) / std
```

- **mean (Œº)**: Gi√° tr·ªã trung b√¨nh
- **std (œÉ)**: ƒê·ªô l·ªách chu·∫©n (standard deviation)
- **√ù nghƒ©a**: ƒê∆∞a d·ªØ li·ªáu v·ªÅ ph√¢n ph·ªëi chu·∫©n (mean=0, std=1)

**2. So s√°nh Z-score vs Min-Max:**

| Ti√™u ch√≠ | Min-Max `(x - min) / (max - min)` | Z-score `(x - mean) / std` |
|----------|-----------------------------------|----------------------------|
| Output range | [0, 1] | Th∆∞·ªùng [-3, 3] nh∆∞ng kh√¥ng gi·ªõi h·∫°n |
| ·∫¢nh h∆∞·ªüng outliers | ‚úÖ Nh·∫°y c·∫£m | ‚ùå √çt nh·∫°y c·∫£m h∆°n |
| Ph√¢n ph·ªëi | Gi·ªØ nguy√™n shape | Chu·∫©n h√≥a v·ªÅ normal distribution |
| Use case | Neural networks, h√¨nh ·∫£nh | K-means, PCA, clustering |

**T·∫°i sao d√πng Z-score cho K-means?**
- K-means d√πng kho·∫£ng c√°ch Euclidean
- Z-score gi·ªØ ƒë∆∞·ª£c "relative distance" gi·ªØa c√°c ƒëi·ªÉm
- √çt b·ªã ·∫£nh h∆∞·ªüng b·ªüi outliers (gi√° tr·ªã c·ª±c l·ªõn/nh·ªè)

**3. V√≠ d·ª• c·ª• th·ªÉ:**

```python
# D·ªØ li·ªáu g·ªëc: C·ªôt "amount_received" (5 giao d·ªãch)
amount_received = [100, 200, 300, 400, 10000]  # C√≥ outlier (10000)

# T√≠nh mean v√† std
mean = (100 + 200 + 300 + 400 + 10000) / 5 = 2200
std = sqrt(((100-2200)^2 + ... + (10000-2200)^2) / 5) ‚âà 3920

# Z-score normalization
z1 = (100 - 2200) / 3920 = -0.54
z2 = (200 - 2200) / 3920 = -0.51
z3 = (300 - 2200) / 3920 = -0.48
z4 = (400 - 2200) / 3920 = -0.46
z5 = (10000 - 2200) / 3920 = 1.99

# K·∫øt qu·∫£: [-0.54, -0.51, -0.48, -0.46, 1.99]
# ‚Üí Mean ‚âà 0, Std ‚âà 1
```

**So s√°nh v·ªõi Min-Max:**
```python
# Min-Max normalization
min_val = 100
max_val = 10000

minmax1 = (100 - 100) / (10000 - 100) = 0.00
minmax2 = (200 - 100) / (10000 - 100) = 0.01
minmax3 = (300 - 100) / (10000 - 100) = 0.02
minmax4 = (400 - 100) / (10000 - 100) = 0.03
minmax5 = (10000 - 100) / (10000 - 100) = 1.00

# K·∫øt qu·∫£: [0.00, 0.01, 0.02, 0.03, 1.00]
# ‚Üí B·ªã ·∫£nh h∆∞·ªüng r·∫•t nhi·ªÅu b·ªüi outlier (10000)
```

**K·∫øt lu·∫≠n:**
- Z-score: Outlier kh√¥ng ·∫£nh h∆∞·ªüng qu√° nhi·ªÅu (1.99 vs -0.54)
- Min-Max: Outlier "k√©o" t·∫•t c·∫£ gi√° tr·ªã kh√°c v·ªÅ g·∫ßn 0 (0.00-0.03)
- ‚Üí Z-score t·ªët h∆°n cho K-means khi c√≥ outliers

---

## üîÑ PH·∫¶N 2: WORKFLOW & PIPELINE (30%)

### ‚ùì C√¢u 6: Lazy Evaluation
**Gi√°o vi√™n h·ªèi:** Trong `prepare_polars.py`, em th·∫•y:

```python
df = pl.scan_csv(DATA_RAW)  # Lazy loading
# ... nhi·ªÅu transformations ...
df_normalized.sink_csv(temp_output, include_header=False)  # Streaming write
```

**Gi·∫£i th√≠ch:
1. `scan_csv()` vs `read_csv()` kh√°c nhau nh∆∞ th·∫ø n√†o?
2. `sink_csv()` vs `write_csv()` kh√°c nhau nh∆∞ th·∫ø n√†o?
3. T·∫°i sao d√πng Lazy evaluation cho file 16GB?
4. ƒêi·ªÅu g√¨ x·∫£y ra v·ªõi RAM khi ch·∫°y code n√†y?**

**Sinh vi√™n tr·∫£ l·ªùi:**

**1. `scan_csv()` vs `read_csv()`:**

| | `scan_csv()` (Lazy) | `read_csv()` (Eager) |
|-|---------------------|----------------------|
| **Load data** | ‚ùå Kh√¥ng load v√†o RAM | ‚úÖ Load to√†n b·ªô v√†o RAM |
| **Execution** | Ch·ªâ l·∫≠p k·∫ø ho·∫°ch (plan) | Th·ª±c thi ngay l·∫≠p t·ª©c |
| **RAM usage** | ~100MB (ch·ªâ metadata) | ~20-30GB (to√†n b·ªô data) |
| **Speed** | Nhanh (kh√¥ng load) | Ch·∫≠m (ph·∫£i load h·∫øt) |
| **Use case** | File l·ªõn (>RAM) | File nh·ªè (<RAM) |

**V√≠ d·ª•:**
```python
# Lazy (scan_csv)
df = pl.scan_csv('16GB.csv')  # Instant! (0.1s)
# Polars ch·ªâ ƒë·ªçc metadata: s·ªë c·ªôt, t√™n c·ªôt, ki·ªÉu d·ªØ li·ªáu
# KH√îNG ƒë·ªçc 179 tri·ªáu d√≤ng v√†o RAM

# Eager (read_csv)
df = pl.read_csv('16GB.csv')  # Ch·∫≠m! (5 ph√∫t)
# Polars ƒë·ªçc H·∫æT 16GB v√†o RAM
# N·∫øu RAM < 16GB ‚Üí swap to disk ho·∫∑c crash!
```

**2. `sink_csv()` vs `write_csv()`:**

| | `sink_csv()` (Streaming) | `write_csv()` (Buffered) |
|-|--------------------------|--------------------------|
| **Memory** | ‚úÖ X·ª≠ l√Ω t·ª´ng chunk | ‚ùå Buffer to√†n b·ªô |
| **Process** | ƒê·ªçc ‚Üí X·ª≠ l√Ω ‚Üí Ghi (streaming) | ƒê·ªçc ‚Üí X·ª≠ l√Ω ‚Üí Buffer ‚Üí Ghi |
| **RAM usage** | ~500MB (chunk size) | ~20-30GB (to√†n b·ªô) |
| **Speed** | H∆°i ch·∫≠m | Nhanh h∆°n (nh∆∞ng c·∫ßn RAM) |

**Workflow `sink_csv()`:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Read     ‚îÇ ‚îÄ‚îÄ> ‚îÇ Process ‚îÇ ‚îÄ‚îÄ> ‚îÇ Write‚îÇ
‚îÇ Chunk 1  ‚îÇ     ‚îÇ Chunk 1 ‚îÇ     ‚îÇ  to  ‚îÇ
‚îÇ (100MB)  ‚îÇ     ‚îÇ         ‚îÇ     ‚îÇ disk ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Read     ‚îÇ ‚îÄ‚îÄ> ‚îÇ Process ‚îÇ ‚îÄ‚îÄ> ‚îÇ Write‚îÇ
‚îÇ Chunk 2  ‚îÇ     ‚îÇ Chunk 2 ‚îÇ     ‚îÇ  to  ‚îÇ
‚îÇ (100MB)  ‚îÇ     ‚îÇ         ‚îÇ     ‚îÇ disk ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  (l·∫∑p l·∫°i 160 l·∫ßn)
```

**3. T·∫°i sao d√πng Lazy cho file 16GB?**
- **RAM constraint**: M√°y ch·ªâ c√≥ 16GB RAM, file 16GB kh√¥ng th·ªÉ load h·∫øt
- **Efficiency**: Polars t·ªëi ∆∞u query tr∆∞·ªõc khi th·ª±c thi (query optimization)
- **Streaming**: X·ª≠ l√Ω t·ª´ng chunk ‚Üí kh√¥ng c·∫ßn load h·∫øt v√†o RAM

**4. RAM usage khi ch·∫°y:**

```python
# B∆∞·ªõc 1: scan_csv
df = pl.scan_csv(DATA_RAW)  
# RAM: ~100MB (metadata only)

# B∆∞·ªõc 2-4: Transformations (lazy)
df_features = df.select([...])
df_normalized = df_numeric.select([...])
# RAM: ~100MB (ch·ªâ l·∫≠p k·∫ø ho·∫°ch, ch∆∞a th·ª±c thi)

# B∆∞·ªõc 5: sink_csv (streaming execution)
df_normalized.sink_csv(temp_output)
# RAM: ~500MB-2GB (x·ª≠ l√Ω t·ª´ng chunk 100MB)
# Chunk 1: ƒê·ªçc 100MB ‚Üí X·ª≠ l√Ω ‚Üí Ghi ‚Üí X√≥a kh·ªèi RAM
# Chunk 2: ƒê·ªçc 100MB ‚Üí X·ª≠ l√Ω ‚Üí Ghi ‚Üí X√≥a kh·ªèi RAM
# ... (l·∫∑p l·∫°i cho ƒë·∫øn h·∫øt file)
```

**K·∫øt lu·∫≠n:**
- ‚úÖ RAM usage t·ªëi ƒëa: ~2GB (thay v√¨ 30GB n·∫øu d√πng eager)
- ‚úÖ C√≥ th·ªÉ x·ª≠ l√Ω file 16GB tr√™n m√°y 16GB RAM
- ‚úÖ Nhanh h∆°n v√¨ Polars t·ªëi ∆∞u query

---

### ‚ùì C√¢u 7: HDFS Upload & Delete
**Gi√°o vi√™n h·ªèi:** Trong `setup_hdfs.sh`, em th·∫•y:

```bash
hdfs dfs -put "$INPUT_TEMP" "$HDFS_BASE/input/hadoop_input.txt"

if [ $? -ne 0 ]; then
    echo "Th·∫•t b·∫°i khi t·∫£i d·ªØ li·ªáu ƒë·∫ßu v√†o"
    exit 1
fi

rm -f "$INPUT_TEMP"
echo "ƒê√£ x√≥a t·ªáp t·∫°m (d·ªØ li·ªáu ch·ªâ c√≤n tr√™n HDFS)"
```

**Gi·∫£i th√≠ch:
1. `$?` l√† g√¨? T·∫°i sao ki·ªÉm tra `$? -ne 0`?
2. T·∫°i sao ph·∫£i x√≥a `$INPUT_TEMP` sau khi upload?
3. ƒêi·ªÅu g√¨ x·∫£y ra n·∫øu upload th·∫•t b·∫°i nh∆∞ng v·∫´n x√≥a file?
4. L√†m sao kh√¥i ph·ª•c n·∫øu c·∫ßn d·ªØ li·ªáu n√†y sau?**

**Sinh vi√™n tr·∫£ l·ªùi:**

**1. `$?` l√† g√¨?**

- `$?`: Exit code c·ªßa l·ªánh v·ª´a ch·∫°y
- `0`: Th√†nh c√¥ng
- `‚â†0` (1, 2, 127, etc.): Th·∫•t b·∫°i (m·ªói code c√≥ √Ω nghƒ©a kh√°c nhau)

**Ki·ªÉm tra `$? -ne 0`:**
```bash
hdfs dfs -put file.txt /hdfs/path

# $? = 0 ‚Üí Upload th√†nh c√¥ng
# $? = 1 ‚Üí L·ªói (vd: HDFS kh√¥ng ch·∫°y)
# $? = 2 ‚Üí L·ªói (vd: file kh√¥ng t·ªìn t·∫°i)

if [ $? -ne 0 ]; then  # ne = not equal
    echo "Th·∫•t b·∫°i!"
    exit 1  # D·ª´ng script
fi
```

**T·∫°i sao quan tr·ªçng?**
- N·∫øu upload th·∫•t b·∫°i m√† v·∫´n x√≥a file ‚Üí m·∫•t d·ªØ li·ªáu!
- Ph·∫£i ki·ªÉm tra tr∆∞·ªõc khi x√≥a

**2. T·∫°i sao x√≥a `$INPUT_TEMP`?**

**Quy ƒë·ªãnh:** Kh√¥ng ƒë∆∞·ª£c l∆∞u d·ªØ li·ªáu l·ªõn (>1GB) ·ªü local

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Polars      ‚îÇ ‚îÄ‚îÄ>  ‚îÇ Temp File    ‚îÇ ‚îÄ‚îÄ>  ‚îÇ HDFS        ‚îÇ
‚îÇ Processing  ‚îÇ      ‚îÇ (31GB local) ‚îÇ      ‚îÇ (permanent) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ                      
                            ‚îÇ (auto delete)        
                            ‚ñº                      
                      [rm -f file]            
```

**Workflow:**
1. Polars t·∫°o file temp (31GB) ·ªü local
2. Upload l√™n HDFS (31GB)
3. **X√≥a file temp** ngay l·∫≠p t·ª©c
4. ‚Üí D·ªØ li·ªáu ch·ªâ c√≤n tr√™n HDFS (tu√¢n th·ªß quy ƒë·ªãnh)

**3. N·∫øu upload th·∫•t b·∫°i nh∆∞ng v·∫´n x√≥a?**

**Scenario x·∫•u (KH√îNG c√≥ ki·ªÉm tra `$?`):**
```bash
hdfs dfs -put "$INPUT_TEMP" "/hdfs/path"
# Upload th·∫•t b·∫°i (HDFS kh√¥ng ch·∫°y)

rm -f "$INPUT_TEMP"
# X√≥a file ‚Üí M·∫§T D·ªÆ LI·ªÜU!
```

**K·∫øt qu·∫£:**
- ‚ùå File local b·ªã x√≥a
- ‚ùå HDFS kh√¥ng c√≥ d·ªØ li·ªáu
- ‚ùå Ph·∫£i ch·∫°y l·∫°i b∆∞·ªõc 2 (m·∫•t 10 ph√∫t!)

**Scenario t·ªët (C√ì ki·ªÉm tra):**
```bash
hdfs dfs -put "$INPUT_TEMP" "/hdfs/path"

if [ $? -ne 0 ]; then
    echo "Upload th·∫•t b·∫°i!"
    exit 1  # D·ª™NG, KH√îNG x√≥a file
fi

# Ch·ªâ ch·∫°y ƒë·∫øn ƒë√¢y n·∫øu upload th√†nh c√¥ng
rm -f "$INPUT_TEMP"
```

**K·∫øt qu·∫£:**
- ‚úÖ Upload th·∫•t b·∫°i ‚Üí script d·ª´ng
- ‚úÖ File local v·∫´n c√≤n
- ‚úÖ S·ª≠a l·ªói HDFS, ch·∫°y l·∫°i script

**4. Kh√¥i ph·ª•c d·ªØ li·ªáu:**

**N·∫øu c·∫ßn xem l·∫°i d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω:**
```bash
# T·∫£i v·ªÅ t·ª´ HDFS (t·∫°m th·ªùi)
hdfs dfs -get /user/spark/hi_large/input/hadoop_input.txt \
             01_data/processed/

# S·ª≠ d·ª•ng
python 02_scripts/polars/04_assign_clusters.py

# X√ìA l·∫°i sau khi d√πng xong (tu√¢n th·ªß quy ƒë·ªãnh)
rm 01_data/processed/hadoop_input.txt
```

**L∆∞u √Ω:**
- ‚ö†Ô∏è CH·ªà t·∫£i v·ªÅ khi c·∫ßn debug/ph√¢n t√≠ch
- ‚ö†Ô∏è PH·∫¢I x√≥a ngay sau khi d√πng xong
- ‚úÖ D·ªØ li·ªáu vƒ©nh vi·ªÖn ch·ªâ tr√™n HDFS

---

### ‚ùì C√¢u 8: Batch Processing
**Gi√°o vi√™n h·ªèi:** Trong `assign_clusters_polars.py`, em th·∫•y:

```python
chunk_size = 1000000  # 1M giao d·ªãch/batch

for i in range(0, len(data), chunk_size):
    end_idx = min(i + chunk_size, len(data))
    chunk = data[i:end_idx]
    
    distances = np.sqrt(((chunk[:, None, :] - centroids[None, :, :]) ** 2).sum(axis=2))
    clusters[i:end_idx] = np.argmin(distances, axis=1)
```

**Gi·∫£i th√≠ch chi ti·∫øt:
1. T·∫°i sao x·ª≠ l√Ω t·ª´ng batch 1M d√≤ng thay v√¨ x·ª≠ l√Ω h·∫øt 179M d√≤ng c√πng l√∫c?
2. Gi·∫£i th√≠ch shape c·ªßa `chunk`, `centroids` v√† ph√©p t√≠nh broadcasting
3. RAM usage l√† bao nhi√™u khi x·ª≠ l√Ω 1 batch?
4. N·∫øu tƒÉng `chunk_size` l√™n 10M th√¨ sao?**

**Sinh vi√™n tr·∫£ l·ªùi:**

**1. T·∫°i sao batch processing?**

**V·∫•n ƒë·ªÅ n·∫øu x·ª≠ l√Ω h·∫øt 179M d√≤ng:**
```python
# N·∫øu x·ª≠ l√Ω to√†n b·ªô
data = np.array((179_000_000, 9))  # 179M x 9 features

distances = np.sqrt(...)  # Shape: (179_000_000, 5)
# RAM c·∫ßn thi·∫øt:
# - data: 179M x 9 x 8 bytes = 12.9 GB
# - distances: 179M x 5 x 8 bytes = 7.2 GB
# - intermediate arrays: ~10 GB
# ‚Üí T·ªïng: ~30 GB RAM!

# M√°y ch·ªâ c√≥ 16GB RAM ‚Üí Crash ho·∫∑c swap to disk (r·∫•t ch·∫≠m)
```

**Gi·∫£i ph√°p: Batch processing**
```python
# X·ª≠ l√Ω t·ª´ng batch 1M d√≤ng
chunk_size = 1_000_000

# Batch 1: 1M d√≤ng
chunk = data[0:1_000_000]  # 1M x 9
distances = ...             # 1M x 5
# RAM: ~500 MB

# Batch 2: 1M d√≤ng
chunk = data[1_000_000:2_000_000]
# RAM: ~500 MB

# ... (l·∫∑p l·∫°i 179 l·∫ßn)
```

**L·ª£i √≠ch:**
- ‚úÖ RAM usage: ~500 MB thay v√¨ 30 GB
- ‚úÖ C√≥ th·ªÉ x·ª≠ l√Ω tr√™n m√°y 16GB RAM
- ‚úÖ Kh√¥ng c·∫ßn swap to disk (nhanh h∆°n)

**2. Shape v√† Broadcasting:**

**Shapes:**
```python
# Input
chunk = (1_000_000, 9)      # 1M giao d·ªãch, 9 features
centroids = (5, 9)          # 5 centroids, 9 features

# B∆∞·ªõc 1: Th√™m chi·ªÅu
chunk[:, None, :] = (1_000_000, 1, 9)
centroids[None, :, :] = (1, 5, 9)

# B∆∞·ªõc 2: Broadcasting (t·ª± ƒë·ªông m·ªü r·ªông)
chunk[:, None, :] = (1_000_000, 1, 9) ‚Üí (1_000_000, 5, 9)
centroids[None, :, :] = (1, 5, 9) ‚Üí (1_000_000, 5, 9)

# B∆∞·ªõc 3: Ph√©p tr·ª´
diff = chunk[:, None, :] - centroids[None, :, :]
# Shape: (1_000_000, 5, 9)
# √ù nghƒ©a: M·ªói giao d·ªãch tr·ª´ cho 5 centroids

# B∆∞·ªõc 4: B√¨nh ph∆∞∆°ng v√† t·ªïng
squared = diff ** 2                    # (1_000_000, 5, 9)
sum_squared = squared.sum(axis=2)      # (1_000_000, 5)
distances = np.sqrt(sum_squared)       # (1_000_000, 5)

# B∆∞·ªõc 5: Argmin
clusters = np.argmin(distances, axis=1)  # (1_000_000,)
# M·ªói giao d·ªãch g√°n v√†o c·ª•m g·∫ßn nh·∫•t
```

**V√≠ d·ª• c·ª• th·ªÉ:**
```python
# Gi·∫£ s·ª≠ 2 giao d·ªãch, 3 features, 2 centroids
chunk = [[1, 2, 3],
         [4, 5, 6]]  # Shape: (2, 3)

centroids = [[0, 0, 0],
             [10, 10, 10]]  # Shape: (2, 3)

# Broadcasting
chunk[:, None, :] = [[[1, 2, 3]],      # Shape: (2, 1, 3)
                     [[4, 5, 6]]]

centroids[None, :, :] = [[[0, 0, 0],   # Shape: (1, 2, 3)
                          [10, 10, 10]]]

# Sau broadcasting (t·ª± ƒë·ªông)
chunk_expanded = [[[1, 2, 3], [1, 2, 3]],        # (2, 2, 3)
                  [[4, 5, 6], [4, 5, 6]]]

centroids_expanded = [[[0, 0, 0], [10, 10, 10]], # (2, 2, 3)
                      [[0, 0, 0], [10, 10, 10]]]

# Ph√©p tr·ª´
diff = [[[1, 2, 3], [-9, -8, -7]],
        [[4, 5, 6], [-6, -5, -4]]]

# Kho·∫£ng c√°ch Euclidean
distances = [[3.74, 13.93],   # Giao d·ªãch 1 ƒë·∫øn 2 centroids
             [8.77, 8.66]]    # Giao d·ªãch 2 ƒë·∫øn 2 centroids

# Argmin
clusters = [0, 1]  # Giao d·ªãch 1 ‚Üí Cluster 0, Giao d·ªãch 2 ‚Üí Cluster 1
```

**3. RAM usage 1 batch:**

```python
chunk_size = 1_000_000
num_features = 9
num_centroids = 5

# NumPy float64 = 8 bytes
chunk = 1_000_000 x 9 x 8 bytes = 72 MB
centroids = 5 x 9 x 8 bytes = 360 bytes (negligible)

# Intermediate arrays
diff = 1_000_000 x 5 x 9 x 8 = 360 MB
squared = 1_000_000 x 5 x 9 x 8 = 360 MB
sum_squared = 1_000_000 x 5 x 8 = 40 MB
distances = 1_000_000 x 5 x 8 = 40 MB

# T·ªïng (peak RAM): ~900 MB
```

**4. N·∫øu tƒÉng chunk_size l√™n 10M:**

```python
chunk_size = 10_000_000  # 10M

# RAM usage
chunk = 10M x 9 x 8 = 720 MB
diff = 10M x 5 x 9 x 8 = 3.6 GB
squared = 10M x 5 x 9 x 8 = 3.6 GB
distances = 10M x 5 x 8 = 400 MB

# T·ªïng: ~9 GB
```

**Trade-off:**
- ‚úÖ √çt batch h∆°n (18 batch thay v√¨ 180)
- ‚úÖ Nhanh h∆°n ~5-10% (√≠t overhead)
- ‚ö†Ô∏è RAM usage cao h∆°n (9GB thay v√¨ 900MB)
- ‚ùå N·∫øu m√°y ch·ªâ c√≥ 8GB RAM ‚Üí swap to disk ho·∫∑c crash

**K·∫øt lu·∫≠n:**
- 1M batch = sweet spot (c√¢n b·∫±ng t·ªëc ƒë·ªô v√† RAM)
- 10M batch = nhanh h∆°n nh∆∞ng c·∫ßn nhi·ªÅu RAM
- 100K batch = an to√†n nh∆∞ng ch·∫≠m h∆°n

---

## ‚öôÔ∏è PH·∫¶N 3: SPARK & MLLIB (20%)

### ‚ùì C√¢u 9: Spark Config
**Gi√°o vi√™n h·ªèi:** Trong `kmeans_spark.py`, gi·∫£i th√≠ch c√°c config:

```python
spark = SparkSession.builder \
    .config("spark.executor.memory", "8g") \
    .config("spark.executor.instances", "4") \
    .config("spark.executor.cores", "4") \
    .config("spark.sql.shuffle.partitions", "800") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .getOrCreate()
```

**Em h√£y gi·∫£i th√≠ch:
1. T·ªïng RAM m√† Spark s·ª≠ d·ª•ng l√† bao nhi√™u?
2. T·ªïng cores l√† bao nhi√™u? C√≥ th·ªÉ ch·∫°y bao nhi√™u tasks song song?
3. `shuffle.partitions = 800` ·∫£nh h∆∞·ªüng g√¨? T·∫°i sao l√† 800?
4. KryoSerializer vs Java serializer?**

**Sinh vi√™n tr·∫£ l·ªùi:**

**1. T·ªïng RAM:**

```python
spark.driver.memory = 8g        # RAM cho driver (coordinator)
spark.executor.memory = 8g      # RAM cho m·ªói executor (worker)
spark.executor.instances = 4    # S·ªë executors

# T·ªïng RAM
Total = driver + (executor_memory √ó num_executors)
Total = 8 GB + (8 GB √ó 4)
Total = 8 GB + 32 GB = 40 GB
```

**Ph√¢n b·ªï:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Driver     ‚îÇ  8 GB
‚îÇ (Coordinator)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ        ‚îÇ        ‚îÇ        ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê
‚îÇExec 1 ‚îÇ ‚îÇExec 2‚îÇ ‚îÇExec 3‚îÇ ‚îÇExec 4‚îÇ
‚îÇ 8 GB  ‚îÇ ‚îÇ 8 GB ‚îÇ ‚îÇ 8 GB ‚îÇ ‚îÇ 8 GB ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**2. T·ªïng cores v√† parallelism:**

```python
spark.executor.cores = 4        # Cores m·ªói executor
spark.executor.instances = 4    # S·ªë executors

# T·ªïng cores
Total cores = executor_cores √ó num_executors
Total cores = 4 √ó 4 = 16 cores
```

**Song song h√≥a:**
- **M·ªói core ch·∫°y 1 task** (m·∫∑c ƒë·ªãnh)
- **T·ªïng tasks song song** = 16 tasks

**V√≠ d·ª•:**
```
Executor 1:
  Core 1 ‚Üí Task 1 (x·ª≠ l√Ω partition 1)
  Core 2 ‚Üí Task 2 (x·ª≠ l√Ω partition 2)
  Core 3 ‚Üí Task 3 (x·ª≠ l√Ω partition 3)
  Core 4 ‚Üí Task 4 (x·ª≠ l√Ω partition 4)

Executor 2:
  Core 1 ‚Üí Task 5
  Core 2 ‚Üí Task 6
  Core 3 ‚Üí Task 7
  Core 4 ‚Üí Task 8

Executor 3:
  Core 1 ‚Üí Task 9
  ...

Executor 4:
  Core 1 ‚Üí Task 13
  ...
  Core 4 ‚Üí Task 16

‚Üí 16 tasks ch·∫°y ƒë·ªìng th·ªùi!
```

**3. `shuffle.partitions = 800`:**

**Shuffle l√† g√¨?**
- Khi Spark c·∫ßn "tr·ªôn" d·ªØ li·ªáu gi·ªØa c√°c executors (vd: groupBy, join)
- D·ªØ li·ªáu ƒë∆∞·ª£c chia th√†nh c√°c partition (ph·∫ßn nh·ªè)

**T·∫°i sao 800?**
```
Quy t·∫Øc: shuffle_partitions ‚âà (num_cores √ó 50)
= 16 cores √ó 50
= 800 partitions
```

**Workflow:**
```
Input: 179M rows

B∆∞·ªõc 1: Chia th√†nh 800 partitions
  Partition 1: 224,000 rows
  Partition 2: 224,000 rows
  ...
  Partition 800: 224,000 rows

B∆∞·ªõc 2: Ph√¢n ph·ªëi cho 16 cores
  Core 1: X·ª≠ l√Ω partition 1, 17, 33, ...  (~50 partitions)
  Core 2: X·ª≠ l√Ω partition 2, 18, 34, ...  (~50 partitions)
  ...
  Core 16: X·ª≠ l√Ω partition 16, 32, 48, ... (~50 partitions)

‚Üí C√¢n b·∫±ng t·∫£i gi·ªØa c√°c cores!
```

**N·∫øu shuffle.partitions qu√° nh·ªè (vd: 16):**
- M·ªói core x·ª≠ l√Ω 1 partition l·ªõn (11M rows)
- N·∫øu 1 partition l·ªõn h∆°n c√°c partition kh√°c ‚Üí 1 core ch·∫≠m ‚Üí bottleneck

**N·∫øu shuffle.partitions qu√° l·ªõn (vd: 10,000):**
- Qu√° nhi·ªÅu partition nh·ªè
- Overhead t·∫°o v√† qu·∫£n l√Ω partition l·ªõn
- Ch·∫≠m h∆°n

**Sweet spot:** 800 (c√¢n b·∫±ng t·ªët)

**4. KryoSerializer vs Java serializer:**

**Serialization l√† g√¨?**
- Chuy·ªÉn object (data) th√†nh bytes ƒë·ªÉ truy·ªÅn qua m·∫°ng
- Spark c·∫ßn serialize khi:
  - Truy·ªÅn data gi·ªØa driver v√† executors
  - Shuffle data gi·ªØa c√°c executors
  - Cache data v√†o disk

**So s√°nh:**

| Ti√™u ch√≠ | Java Serializer (default) | KryoSerializer |
|----------|---------------------------|----------------|
| T·ªëc ƒë·ªô | Ch·∫≠m | Nhanh h∆°n 10x |
| K√≠ch th∆∞·ªõc | L·ªõn | Nh·ªè h∆°n 5-10x |
| T∆∞∆°ng th√≠ch | T·∫•t c·∫£ Java objects | C·∫ßn ƒëƒÉng k√Ω class |
| Use case | Default, debug | Production (recommend) |

**V√≠ d·ª•:**
```python
# D·ªØ li·ªáu: 1 tri·ªáu numbers
data = [1, 2, 3, ..., 1_000_000]

# Java serializer
serialized = java_serialize(data)
# K√≠ch th∆∞·ªõc: ~50 MB
# Th·ªùi gian: 100ms

# Kryo serializer
serialized = kryo_serialize(data)
# K√≠ch th∆∞·ªõc: ~5 MB (nh·ªè h∆°n 10x)
# Th·ªùi gian: 10ms (nhanh h∆°n 10x)
```

**L·ª£i √≠ch trong d·ª± √°n:**
- 179M rows c·∫ßn serialize nhi·ªÅu l·∫ßn (shuffle)
- Kryo ‚Üí gi·∫£m network I/O ‚Üí nhanh h∆°n 20-30%
- Kryo ‚Üí gi·∫£m disk usage khi cache ‚Üí ti·∫øt ki·ªám RAM

**C·∫•u h√¨nh:**
```python
.config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
.config("spark.kryoserializer.buffer.max", "512m")  # Buffer l·ªõn ƒë·ªÉ tr√°nh tr√†n
```

---

### ‚ùì C√¢u 10: K-means++ Initialization
**Gi√°o vi√™n h·ªèi:** Trong code MLlib:

```python
kmeans = KMeans() \
    .setK(5) \
    .setInitMode("k-means||") \
    .setMaxIter(15)

model = kmeans.fit(vector_df)
```

**Gi·∫£i th√≠ch:
1. `k-means||` l√† g√¨? Kh√°c g√¨ v·ªõi random initialization?
2. T·∫°i sao MLlib d√πng `k-means||` thay v√¨ `k-means++` g·ªëc?
3. Thu·∫≠t to√°n `k-means||` ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o?
4. So s√°nh th·ªùi gian v√† ch·∫•t l∆∞·ª£ng**

**Sinh vi√™n tr·∫£ l·ªùi:**

**1. `k-means||` (k-means parallel) l√† g√¨?**

- `k-means||` = Phi√™n b·∫£n ph√¢n t√°n c·ªßa `k-means++`
- ƒê∆∞·ª£c thi·∫øt k·∫ø cho Big Data (Apache Spark)
- Ch·ªçn K centroids ban ƒë·∫ßu TH√îNG MINH thay v√¨ random

**So s√°nh v·ªõi random initialization:**

| | Random | k-means++ / k-means|| |
|-|--------|----------------------|
| Ch·ªçn centroids | Ng·∫´u nhi√™n K ƒëi·ªÉm | Th√¥ng minh (xa nhau) |
| S·ªë iterations | 20-50 | 10-15 (√≠t h∆°n) |
| Ch·∫•t l∆∞·ª£ng | Kh√¥ng ·ªïn ƒë·ªãnh | ·ªîn ƒë·ªãnh h∆°n |
| T·ªëc ƒë·ªô h·ªôi t·ª• | Ch·∫≠m | Nhanh h∆°n |
| Use case | Test, prototype | Production |

**2. T·∫°i sao d√πng `k-means||` thay v√¨ `k-means++`?**

**`k-means++` (g·ªëc):**
```python
# Ch·ªçn K centroids tu·∫ßn t·ª± (sequential)
1. Ch·ªçn centroid 1: Random
2. Ch·ªçn centroid 2: Xa centroid 1 nh·∫•t
3. Ch·ªçn centroid 3: Xa 2 centroids tr∆∞·ªõc nh·∫•t
...
K. Ch·ªçn centroid K: Xa K-1 centroids tr∆∞·ªõc nh·∫•t
```

**V·∫•n ƒë·ªÅ:** Sequential ‚Üí kh√¥ng th·ªÉ song song ‚Üí CH·∫¨M v·ªõi Big Data!

**`k-means||` (parallel):**
```python
# Ch·ªçn K centroids song song (parallel)
Round 1: Ch·ªçn L ƒëi·ªÉm (L >> K) c√πng l√∫c
Round 2: Ch·ªçn L ƒëi·ªÉm n·ªØa
...
Round 5: ƒê·ªß ~5K ƒëi·ªÉm

# Cu·ªëi c√πng: Ch·ªçn K t·ªët nh·∫•t t·ª´ 5K ƒëi·ªÉm
```

**L·ª£i √≠ch:**
- ‚úÖ Song song tr√™n nhi·ªÅu executors
- ‚úÖ Nhanh h∆°n k-means++ g·ªëc 10-100x
- ‚úÖ Ch·∫•t l∆∞·ª£ng t∆∞∆°ng ƒë∆∞∆°ng k-means++

**3. Thu·∫≠t to√°n `k-means||`:**

**Input:**
- Data: 179M ƒëi·ªÉm
- K = 5 (s·ªë c·ª•m)
- L = 2K = 10 (s·ªë ƒëi·ªÉm m·ªói round, oversample)

**Thu·∫≠t to√°n:**

```python
# Round 1: Kh·ªüi t·∫°o
centroids = []
c1 = random_point()  # Ch·ªçn 1 ƒëi·ªÉm ng·∫´u nhi√™n
centroids.append(c1)

# Round 2-5: L·∫∑p 5 l·∫ßn (log K iterations)
for round in range(1, 6):
    # T√≠nh kho·∫£ng c√°ch t·ª´ m·ªói ƒëi·ªÉm ƒë·∫øn centroids g·∫ßn nh·∫•t
    for point in data:
        dist = min_distance(point, centroids)
        prob = dist^2 / sum_all_distances  # X√°c su·∫•t ch·ªçn
    
    # Ch·ªçn L = 10 ƒëi·ªÉm v·ªõi x√°c su·∫•t cao (xa centroids)
    # Song song tr√™n nhi·ªÅu executors!
    new_points = sample(data, L, probability=prob)
    centroids.extend(new_points)  # Th√™m v√†o danh s√°ch

# Sau 5 rounds: ~50 centroids

# B∆∞·ªõc cu·ªëi: Ch·ªçn K = 5 t·ªët nh·∫•t t·ª´ 50 centroids
# D√πng K-means++ tr√™n 50 ƒëi·ªÉm (nhanh v√¨ √≠t ƒëi·ªÉm)
final_centroids = kmeans++(centroids, K=5)
```

**V√≠ d·ª• tr·ª±c quan:**

```
Round 1:
  ‚óã (random centroid 1)

Round 2 (ch·ªçn 10 ƒëi·ªÉm xa c1):
  ‚óã c1
  ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè (10 ƒëi·ªÉm m·ªõi)

Round 3 (ch·ªçn 10 ƒëi·ªÉm xa t·∫•t c·∫£ 11 centroids):
  ‚óã c1
  ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè
  ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ ‚òÖ (10 ƒëi·ªÉm m·ªõi)

...

Round 5: 50 centroids

Cu·ªëi c√πng: Ch·ªçn 5 t·ªët nh·∫•t t·ª´ 50
  ‚óã ‚óã ‚óã ‚óã ‚óã (final 5 centroids)
```

**4. So s√°nh th·ªùi gian v√† ch·∫•t l∆∞·ª£ng:**

**Th·ªùi gian (179M ƒëi·ªÉm):**

| Method | Th·ªùi gian kh·ªüi t·∫°o | S·ªë iterations | T·ªïng th·ªùi gian K-means |
|--------|-------------------|---------------|------------------------|
| Random | 1s | 20-50 | 20-30 ph√∫t |
| k-means++ (sequential) | 10 ph√∫t | 10-15 | 25 ph√∫t |
| **k-means\|\| (parallel)** | **30 gi√¢y** | **10-15** | **6-7 ph√∫t** ‚úÖ |

**Ch·∫•t l∆∞·ª£ng (WSSSE):**

| Method | WSSSE | Stability |
|--------|-------|-----------|
| Random | 1,200,000,000 | ‚ùå Kh√¥ng ·ªïn ƒë·ªãnh (kh√°c nhau m·ªói l·∫ßn ch·∫°y) |
| k-means++ | 950,000,000 | ‚úÖ ·ªîn ƒë·ªãnh |
| **k-means\|\|** | **960,000,000** | ‚úÖ ·ªîn ƒë·ªãnh (g·∫ßn k-means++) |

**K·∫øt lu·∫≠n:**
- ‚úÖ k-means|| nhanh nh·∫•t (6-7 ph√∫t vs 25 ph√∫t)
- ‚úÖ Ch·∫•t l∆∞·ª£ng t∆∞∆°ng ƒë∆∞∆°ng k-means++ (960M vs 950M)
- ‚úÖ ·ªîn ƒë·ªãnh, kh√¥ng b·ªã random
- ‚Üí L·ª±a ch·ªçn t·ªët nh·∫•t cho Big Data!

---

## üêõ PH·∫¶N 4: DEBUG & TROUBLESHOOTING (20%)

### ‚ùì C√¢u 11: Debugging Pipeline
**Gi√°o vi√™n h·ªèi:** Pipeline b·ªã l·ªói ·ªü b∆∞·ªõc 4 (K-means). Em debug nh∆∞ th·∫ø n√†o?

**C√°c b∆∞·ªõc debug chi ti·∫øt:**

**Sinh vi√™n tr·∫£ l·ªùi:**

**B∆∞·ªõc 1: X√°c ƒë·ªãnh l·ªói**

```bash
# Ki·ªÉm tra log
cat 04_logs/pipeline_log_*.md

# T√¨m d√≤ng l·ªói
# V√≠ d·ª• output:
‚ùå B∆∞·ªõc 4 th·∫•t b·∫°i
```

**B∆∞·ªõc 2: Ki·ªÉm tra HDFS**

```bash
# HDFS c√≥ ƒëang ch·∫°y kh√¥ng?
hdfs dfsadmin -report

# Output mong ƒë·ª£i:
# Live datanodes (1):
# Name: ...

# N·∫øu l·ªói "Connection refused":
# ‚Üí HDFS kh√¥ng ch·∫°y
# Fix: start-dfs.sh
```

**B∆∞·ªõc 3: Ki·ªÉm tra d·ªØ li·ªáu input**

```bash
# File c√≥ t·ªìn t·∫°i kh√¥ng?
hdfs dfs -ls /user/spark/hi_large/input/

# Output:
# hadoop_input.txt  31 GB

# Xem v√†i d√≤ng ƒë·∫ßu
hdfs dfs -cat /user/spark/hi_large/input/hadoop_input.txt | head -5

# Output mong ƒë·ª£i: 9 s·ªë m·ªói d√≤ng (normalized)
# -0.234,0.567,...,-1.234
# 1.234,-0.456,...,0.789
```

**B∆∞·ªõc 4: Ki·ªÉm tra Spark**

```bash
# Spark c√≥ c√†i ƒë·∫∑t ƒë√∫ng kh√¥ng?
spark-submit --version

# Java version
java -version  # C·∫ßn Java 11 ho·∫∑c 17

# Memory available
free -h
# C·∫ßn √≠t nh·∫•t 16GB RAM free
```

**B∆∞·ªõc 5: Ch·∫°y l·∫°i b∆∞·ªõc 4 ri√™ng l·∫ª**

```bash
# Ch·∫°y manual ƒë·ªÉ xem l·ªói chi ti·∫øt
./02_scripts/spark/run_spark.sh

# Xem output/error messages
# C√°c l·ªói ph·ªï bi·∫øn:
# - OutOfMemoryError ‚Üí TƒÉng memory config
# - FileNotFoundException ‚Üí HDFS kh√¥ng c√≥ file
# - Connection refused ‚Üí HDFS kh√¥ng ch·∫°y
```

**B∆∞·ªõc 6: Ki·ªÉm tra checkpoint**

```bash
# C√°c b∆∞·ªõc tr∆∞·ªõc ƒë√£ ch·∫°y ch∆∞a?
ls .pipeline_checkpoints/

# Output:
# step_1.done
# step_2.done
# step_3.done
# (step_4.done kh√¥ng c√≥ ‚Üí b∆∞·ªõc 4 ch∆∞a ho√†n th√†nh)

# N·∫øu c·∫ßn ch·∫°y l·∫°i t·ª´ ƒë·∫ßu:
./02_scripts/pipeline/reset_pipeline.sh
```

**B∆∞·ªõc 7: Test v·ªõi d·ªØ li·ªáu nh·ªè**

```bash
# T·∫°o file test nh·ªè (1000 d√≤ng)
hdfs dfs -cat /user/spark/hi_large/input/hadoop_input.txt | head -1000 > test.txt

# Upload test file
hdfs dfs -put test.txt /user/spark/test_input.txt

# S·ª≠a script run_spark.sh (t·∫°m th·ªùi)
# INPUT: /user/spark/test_input.txt

# Ch·∫°y K-means v·ªõi test data
./02_scripts/spark/run_spark.sh

# N·∫øu th√†nh c√¥ng ‚Üí V·∫•n ƒë·ªÅ l√† d·ªØ li·ªáu l·ªõn (out of memory)
# N·∫øu v·∫´n l·ªói ‚Üí V·∫•n ƒë·ªÅ l√† config ho·∫∑c code
```

**B∆∞·ªõc 8: Gi·∫£m memory requirements**

```python
# S·ª≠a trong kmeans_spark.py
spark = SparkSession.builder \
    .config("spark.executor.memory", "4g") \  # Gi·∫£m t·ª´ 8g ‚Üí 4g
    .config("spark.executor.instances", "2") \ # Gi·∫£m t·ª´ 4 ‚Üí 2
    .getOrCreate()

# Ho·∫∑c sample data
df = df.sample(0.5)  # Ch·ªâ l·∫•y 50% data
```

---

ƒê√¢y l√† file si√™u chi ti·∫øt v·ªõi 11 c√¢u h·ªèi s√¢u v·ªÅ code v√† workflow, m·ªói c√¢u c√≥:
- ‚úÖ Gi·∫£i th√≠ch code t·ª´ng d√≤ng
- ‚úÖ V√≠ d·ª• c·ª• th·ªÉ v·ªõi s·ªë li·ªáu
- ‚úÖ So s√°nh c√°c ph∆∞∆°ng ph√°p
- ‚úÖ Trade-offs v√† debugging

Gi√°o vi√™n c√≥ th·ªÉ d√πng ƒë·ªÉ:
- Ki·ªÉm tra hi·ªÉu bi·∫øt s√¢u v·ªÅ code
- ƒê√°nh gi√° kh·∫£ nƒÉng debug
- Ki·ªÉm tra hi·ªÉu workflow
- Test kh·∫£ nƒÉng gi·∫£i th√≠ch k·ªπ thu·∫≠t
