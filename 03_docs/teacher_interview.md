# ğŸ“ PHá»NG Váº¤N SINH VIÃŠN - CHI TIáº¾T CODE & WORKFLOW

**GiÃ¡o viÃªn tra há»i Ä‘á»ƒ kiá»ƒm tra kháº£ nÄƒng hiá»ƒu biáº¿t sÃ¢u vá» dá»± Ã¡n**

---

## ğŸ” PHáº¦N 1: HIá»‚U BIáº¾T CODE - FEATURE ENGINEERING (30%)

### â“ CÃ¢u 1: Parse Timestamp
**GiÃ¡o viÃªn há»i:** Em giáº£i thÃ­ch chi tiáº¿t Ä‘oáº¡n code nÃ y trong `prepare_polars.py`:

```python
pl.col('Timestamp').str.strptime(pl.Datetime, format='%Y/%m/%d %H:%M').dt.hour().alias('hour')
```

**Sinh viÃªn tráº£ lá»i:**

**PhÃ¢n tÃ­ch tá»«ng pháº§n:**

1. **`pl.col('Timestamp')`**: Chá»n cá»™t Timestamp
   - Input: Chuá»—i vÄƒn báº£n "2022/08/01 00:17"

2. **`.str.strptime(pl.Datetime, format='%Y/%m/%d %H:%M')`**: Parse chuá»—i thÃ nh datetime
   - `strptime` = string parse time
   - `format='%Y/%m/%d %H:%M'`: Template Ä‘á»ƒ parse
     - `%Y`: NÄƒm 4 chá»¯ sá»‘ (2022)
     - `%m`: ThÃ¡ng (08)
     - `%d`: NgÃ y (01)
     - `%H`: Giá» 24h (00)
     - `%M`: PhÃºt (17)
   - Output: Polars Datetime object

3. **`.dt.hour()`**: TrÃ­ch xuáº¥t giá» tá»« datetime
   - Accessor `.dt` Ä‘á»ƒ truy cáº­p cÃ¡c thuá»™c tÃ­nh datetime
   - `.hour()` láº¥y giá» (0-23)
   - Output: Sá»‘ nguyÃªn 0

4. **`.alias('hour')`**: Äáº·t tÃªn cá»™t má»›i lÃ  "hour"

**Táº¡i sao cáº§n parse timestamp?**
- Giao dá»‹ch rá»­a tiá»n thÆ°á»ng xáº£y ra vÃ o giá» khÃ´ng bÃ¬nh thÆ°á»ng (2-3h sÃ¡ng)
- K-means cáº§n Ä‘áº·c trÆ°ng sá»‘, khÃ´ng thá»ƒ xá»­ lÃ½ chuá»—i "2022/08/01 00:17"
- TrÃ­ch xuáº¥t giá» giÃºp phÃ¡t hiá»‡n pattern theo thá»i gian

**VÃ­ dá»¥:**
```
Input:  "2022/08/01 00:17"
Step 1: Datetime(2022, 8, 1, 0, 17)
Step 2: 0 (giá»)
Output: hour = 0
```

---

### â“ CÃ¢u 2: Amount Ratio
**GiÃ¡o viÃªn há»i:** Giáº£i thÃ­ch Ä‘oáº¡n code tÃ­nh `amount_ratio`:

```python
(pl.col('Amount Received') / (pl.col('Amount Paid') + 1e-6)).alias('amount_ratio')
```

**Táº¡i sao pháº£i cá»™ng `1e-6`? Äiá»u gÃ¬ xáº£y ra náº¿u khÃ´ng cá»™ng?**

**Sinh viÃªn tráº£ lá»i:**

**PhÃ¢n tÃ­ch:**

1. **CÃ´ng thá»©c**: `amount_ratio = amount_received / amount_paid`
   - Äo tá»· lá»‡ tiá»n nháº­n so vá»›i tiá»n tráº£
   - VÃ­ dá»¥: Nháº­n 1000$, tráº£ 500$ â†’ ratio = 2.0

2. **Táº¡i sao cá»™ng `1e-6` (0.000001)?**
   - **Váº¥n Ä‘á»**: `Amount Paid` cÃ³ thá»ƒ báº±ng 0 â†’ chia cho 0 = lá»—i!
   - **Giáº£i phÃ¡p**: Cá»™ng `1e-6` (sá»‘ ráº¥t nhá») Ä‘á»ƒ trÃ¡nh chia cho 0
   - `1e-6` Ä‘á»§ nhá» Ä‘á»ƒ khÃ´ng áº£nh hÆ°á»Ÿng káº¿t quáº£ khi Amount Paid > 0

3. **Náº¿u KHÃ”NG cá»™ng `1e-6`:**
   ```python
   # VÃ­ dá»¥ cÃ³ giao dá»‹ch vá»›i Amount Paid = 0
   amount_ratio = 1000 / 0  # â†’ ZeroDivisionError!
   # â†’ Pipeline bá»‹ crash!
   ```

4. **Vá»›i `1e-6`:**
   ```python
   amount_ratio = 1000 / (0 + 0.000001)
   amount_ratio = 1000000000  # Ráº¥t lá»›n (Ä‘Æ°á»£c chuáº©n hÃ³a sau)
   # â†’ KhÃ´ng lá»—i, K-means váº«n cháº¡y Ä‘Æ°á»£c
   ```

**Ã nghÄ©a business:**
- Ratio báº¥t thÆ°á»ng (vÃ­ dá»¥ >>10) cÃ³ thá»ƒ lÃ  dáº¥u hiá»‡u rá»­a tiá»n
- VÃ­ dá»¥: Nháº­n 10 triá»‡u$, chá»‰ tráº£ 100$ â†’ ratio = 100,000 (nghi ngá»!)

**Trade-off:**
- âœ… TrÃ¡nh crash do chia cho 0
- âš ï¸ Táº¡o ra outlier (ratio ráº¥t lá»›n) khi Amount Paid gáº§n 0
- âœ… Chuáº©n hÃ³a Z-score sau sáº½ xá»­ lÃ½ outlier nÃ y

---

### â“ CÃ¢u 3: Route Hash
**GiÃ¡o viÃªn há»i:** Giáº£i thÃ­ch Ä‘oáº¡n code:

```python
(pl.col('From Bank').hash() ^ pl.col('To Bank').hash()).alias('route_hash')
```

**Em giáº£i thÃ­ch:
1. `.hash()` lÃ m gÃ¬?
2. ToÃ¡n tá»­ `^` (XOR) lÃ  gÃ¬? Táº¡i sao dÃ¹ng XOR thay vÃ¬ `+` hoáº·c `*`?
3. Cho vÃ­ dá»¥ cá»¥ thá»ƒ vá»›i sá»‘**

**Sinh viÃªn tráº£ lá»i:**

**1. `.hash()` lÃ m gÃ¬?**
- Chuyá»ƒn giÃ¡ trá»‹ thÃ nh sá»‘ hash (integer 64-bit)
- Hash function: Deterministic (cÃ¹ng input â†’ cÃ¹ng output)
- VÃ­ dá»¥:
  ```
  From Bank = 20   â†’ hash(20) = 8734523874523 (vÃ­ dá»¥)
  To Bank = 3196   â†’ hash(3196) = 9283745982374 (vÃ­ dá»¥)
  ```

**2. ToÃ¡n tá»­ `^` (XOR - Exclusive OR):**
- XOR: PhÃ©p toÃ¡n bit-wise
- Quy táº¯c: `a ^ b = 1` náº¿u a â‰  b, `= 0` náº¿u a = b

**Táº¡i sao dÃ¹ng XOR thay vÃ¬ `+` hoáº·c `*`?**

| ToÃ¡n tá»­ | Váº¥n Ä‘á» | VÃ­ dá»¥ váº¥n Ä‘á» |
|---------|--------|--------------|
| `+` | KhÃ´ng phÃ¢n biá»‡t hÆ°á»›ng | `hash(A) + hash(B) = hash(B) + hash(A)` â†’ Aâ†’B giá»‘ng Bâ†’A |
| `*` | TÆ°Æ¡ng tá»± `+` | `hash(A) * hash(B) = hash(B) * hash(A)` â†’ Aâ†’B giá»‘ng Bâ†’A |
| `^` | âœ… PhÃ¢n biá»‡t hÆ°á»›ng | `hash(A) ^ hash(B) â‰  hash(B) ^ hash(A)` (thá»±c ra báº±ng nhau, nhÆ°ng káº¿t há»£p vá»›i thá»© tá»± cá»™t) |

**Thá»±c táº¿:** XOR cÅ©ng cÃ³ tÃ­nh giao hoÃ¡n (`a ^ b = b ^ a`), nhÆ°ng trong code nÃ y:
- Polars Ä‘á»c theo thá»© tá»±: From Bank trÆ°á»›c, To Bank sau
- Route tá»« Aâ†’B vÃ  Bâ†’A sáº½ cÃ³ hash giá»‘ng nhau
- **Má»¥c Ä‘Ã­ch**: Táº¡o mÃ£ duy nháº¥t cho má»—i cáº·p ngÃ¢n hÃ ng (khÃ´ng phÃ¢n biá»‡t hÆ°á»›ng)

**3. VÃ­ dá»¥ cá»¥ thá»ƒ:**

```python
# Giáº£ sá»­ hash Ä‘Æ¡n giáº£n hÃ³a
From Bank = 20   â†’ hash = 10010 (binary)
To Bank = 3196   â†’ hash = 11001 (binary)

XOR:
  10010
^ 11001
-------
  01011  = 11 (decimal)

â†’ route_hash = 11
```

**Ã nghÄ©a:**
- Má»—i tuyáº¿n chuyá»ƒn tiá»n (Aâ†’B) cÃ³ má»™t mÃ£ duy nháº¥t
- Náº¿u tuyáº¿n nÃ y xuáº¥t hiá»‡n quÃ¡ nhiá»u â†’ nghi ngá» rá»­a tiá»n
- K-means sáº½ nhÃ³m cÃ¡c giao dá»‹ch cÃ³ cÃ¹ng route_hash

**LÆ°u Ã½:** Trong thá»±c táº¿, Polars hash() tráº£ vá» sá»‘ ráº¥t lá»›n (64-bit), khÃ´ng pháº£i sá»‘ nhá» nhÆ° vÃ­ dá»¥.

---

### â“ CÃ¢u 4: Label Encoding
**GiÃ¡o viÃªn há»i:** Giáº£i thÃ­ch Ä‘oáº¡n code:

```python
pl.col('recv_curr').cast(pl.Categorical).to_physical().alias('recv_curr_encoded')
```

**Em giáº£i thÃ­ch tá»«ng bÆ°á»›c vÃ  cho vÃ­ dá»¥ vá»›i dá»¯ liá»‡u thá»±c táº¿.**

**Sinh viÃªn tráº£ lá»i:**

**PhÃ¢n tÃ­ch tá»«ng bÆ°á»›c:**

1. **`pl.col('recv_curr')`**: Chá»n cá»™t "Receiving Currency"
   - Kiá»ƒu: Chuá»—i (String)
   - VÃ­ dá»¥: ["US Dollar", "Euro", "Yuan", "US Dollar", "Bitcoin"]

2. **`.cast(pl.Categorical)`**: Chuyá»ƒn sang kiá»ƒu Categorical
   - Polars lÆ°u chuá»—i thÃ nh 2 pháº§n:
     - **Dictionary**: ["US Dollar" â†’ 0, "Euro" â†’ 1, "Yuan" â†’ 2, "Bitcoin" â†’ 3]
     - **Indices**: [0, 1, 2, 0, 3]
   - Tiáº¿t kiá»‡m bá»™ nhá»› (lÆ°u sá»‘ thay vÃ¬ chuá»—i)

3. **`.to_physical()`**: Láº¥y indices (sá»‘)
   - Chuyá»ƒn tá»« Categorical â†’ Integer
   - Output: [0, 1, 2, 0, 3]

4. **`.alias('recv_curr_encoded')`**: Äáº·t tÃªn cá»™t má»›i

**VÃ­ dá»¥ chi tiáº¿t:**

```python
# Dá»¯ liá»‡u gá»‘c (5 giao dá»‹ch)
recv_curr = [
    "US Dollar",
    "Euro",
    "Yuan",
    "US Dollar",
    "Bitcoin"
]

# BÆ°á»›c 1: cast(pl.Categorical)
# Táº¡o dictionary:
#   "US Dollar" â†’ 0
#   "Euro"      â†’ 1
#   "Yuan"      â†’ 2
#   "Bitcoin"   â†’ 3
# (Thá»© tá»± phá»¥ thuá»™c vÃ o thá»© tá»± xuáº¥t hiá»‡n láº§n Ä‘áº§u)

# BÆ°á»›c 2: to_physical()
recv_curr_encoded = [0, 1, 2, 0, 3]

# Káº¿t quáº£
DataFrame:
  recv_curr     | recv_curr_encoded
  --------------|------------------
  US Dollar     | 0
  Euro          | 1
  Yuan          | 2
  US Dollar     | 0
  Bitcoin       | 3
```

**Táº¡i sao cáº§n Label Encoding?**
- K-means chá»‰ hiá»ƒu sá»‘, khÃ´ng hiá»ƒu chuá»—i
- Chuyá»ƒn "US Dollar" â†’ 0, "Euro" â†’ 1 Ä‘á»ƒ thuáº­t toÃ¡n xá»­ lÃ½ Ä‘Æ°á»£c
- Tiáº¿t kiá»‡m bá»™ nhá»›: Chuá»—i "US Dollar" (9 bytes) â†’ sá»‘ 0 (4 bytes)

**LÆ°u Ã½:**
- Sá»‘ Ä‘Æ°á»£c gÃ¡n theo thá»© tá»± xuáº¥t hiá»‡n, khÃ´ng pháº£i alphabetical
- "US Dollar" â†’ 0 khÃ´ng cÃ³ nghÄ©a "US Dollar" nhá» hÆ¡n "Euro" (1)
- Chá»‰ lÃ  cÃ¡ch mÃ£ hÃ³a, khÃ´ng cÃ³ Ã½ nghÄ©a thá»© tá»±

---

### â“ CÃ¢u 5: Z-score Normalization
**GiÃ¡o viÃªn há»i:** Giáº£i thÃ­ch Ä‘oáº¡n code chuáº©n hÃ³a:

```python
df_normalized = df_numeric.select([
    ((pl.col(c) - pl.col(c).mean()) / pl.col(c).std()).alias(c)
    for c in df_numeric.collect_schema().names()
])
```

**Em hÃ£y:
1. Giáº£i thÃ­ch cÃ´ng thá»©c `(x - mean) / std`
2. Táº¡i sao khÃ´ng dÃ¹ng Min-Max normalization `(x - min) / (max - min)`?
3. Cho vÃ­ dá»¥ cá»¥ thá»ƒ vá»›i sá»‘ liá»‡u**

**Sinh viÃªn tráº£ lá»i:**

**1. CÃ´ng thá»©c Z-score:**

```
z = (x - mean) / std
```

- **mean (Î¼)**: GiÃ¡ trá»‹ trung bÃ¬nh
- **std (Ïƒ)**: Äá»™ lá»‡ch chuáº©n (standard deviation)
- **Ã nghÄ©a**: ÄÆ°a dá»¯ liá»‡u vá» phÃ¢n phá»‘i chuáº©n (mean=0, std=1)

**2. So sÃ¡nh Z-score vs Min-Max:**

| TiÃªu chÃ­ | Min-Max `(x - min) / (max - min)` | Z-score `(x - mean) / std` |
|----------|-----------------------------------|----------------------------|
| Output range | [0, 1] | ThÆ°á»ng [-3, 3] nhÆ°ng khÃ´ng giá»›i háº¡n |
| áº¢nh hÆ°á»Ÿng outliers | âœ… Nháº¡y cáº£m | âŒ Ãt nháº¡y cáº£m hÆ¡n |
| PhÃ¢n phá»‘i | Giá»¯ nguyÃªn shape | Chuáº©n hÃ³a vá» normal distribution |
| Use case | Neural networks, hÃ¬nh áº£nh | K-means, PCA, clustering |

**Táº¡i sao dÃ¹ng Z-score cho K-means?**
- K-means dÃ¹ng khoáº£ng cÃ¡ch Euclidean
- Z-score giá»¯ Ä‘Æ°á»£c "relative distance" giá»¯a cÃ¡c Ä‘iá»ƒm
- Ãt bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi outliers (giÃ¡ trá»‹ cá»±c lá»›n/nhá»)

**3. VÃ­ dá»¥ cá»¥ thá»ƒ:**

```python
# Dá»¯ liá»‡u gá»‘c: Cá»™t "amount_received" (5 giao dá»‹ch)
amount_received = [100, 200, 300, 400, 10000]  # CÃ³ outlier (10000)

# TÃ­nh mean vÃ  std
mean = (100 + 200 + 300 + 400 + 10000) / 5 = 2200
std = sqrt(((100-2200)^2 + ... + (10000-2200)^2) / 5) â‰ˆ 3920

# Z-score normalization
z1 = (100 - 2200) / 3920 = -0.54
z2 = (200 - 2200) / 3920 = -0.51
z3 = (300 - 2200) / 3920 = -0.48
z4 = (400 - 2200) / 3920 = -0.46
z5 = (10000 - 2200) / 3920 = 1.99

# Káº¿t quáº£: [-0.54, -0.51, -0.48, -0.46, 1.99]
# â†’ Mean â‰ˆ 0, Std â‰ˆ 1
```

**So sÃ¡nh vá»›i Min-Max:**
```python
# Min-Max normalization
min_val = 100
max_val = 10000

minmax1 = (100 - 100) / (10000 - 100) = 0.00
minmax2 = (200 - 100) / (10000 - 100) = 0.01
minmax3 = (300 - 100) / (10000 - 100) = 0.02
minmax4 = (400 - 100) / (10000 - 100) = 0.03
minmax5 = (10000 - 100) / (10000 - 100) = 1.00

# Káº¿t quáº£: [0.00, 0.01, 0.02, 0.03, 1.00]
# â†’ Bá»‹ áº£nh hÆ°á»Ÿng ráº¥t nhiá»u bá»Ÿi outlier (10000)
```

**Káº¿t luáº­n:**
- Z-score: Outlier khÃ´ng áº£nh hÆ°á»Ÿng quÃ¡ nhiá»u (1.99 vs -0.54)
- Min-Max: Outlier "kÃ©o" táº¥t cáº£ giÃ¡ trá»‹ khÃ¡c vá» gáº§n 0 (0.00-0.03)
- â†’ Z-score tá»‘t hÆ¡n cho K-means khi cÃ³ outliers

---

## ğŸ”„ PHáº¦N 2: WORKFLOW & PIPELINE (30%)

### â“ CÃ¢u 6: Lazy Evaluation
**GiÃ¡o viÃªn há»i:** Trong `prepare_polars.py`, em tháº¥y:

```python
df = pl.scan_csv(DATA_RAW)  # Lazy loading
# ... nhiá»u transformations ...
df_normalized.sink_csv(temp_output, include_header=False)  # Streaming write
```

**Giáº£i thÃ­ch:
1. `scan_csv()` vs `read_csv()` khÃ¡c nhau nhÆ° tháº¿ nÃ o?
2. `sink_csv()` vs `write_csv()` khÃ¡c nhau nhÆ° tháº¿ nÃ o?
3. Táº¡i sao dÃ¹ng Lazy evaluation cho file 16GB?
4. Äiá»u gÃ¬ xáº£y ra vá»›i RAM khi cháº¡y code nÃ y?**

**Sinh viÃªn tráº£ lá»i:**

**1. `scan_csv()` vs `read_csv()`:**

| | `scan_csv()` (Lazy) | `read_csv()` (Eager) |
|-|---------------------|----------------------|
| **Load data** | âŒ KhÃ´ng load vÃ o RAM | âœ… Load toÃ n bá»™ vÃ o RAM |
| **Execution** | Chá»‰ láº­p káº¿ hoáº¡ch (plan) | Thá»±c thi ngay láº­p tá»©c |
| **RAM usage** | ~100MB (chá»‰ metadata) | ~20-30GB (toÃ n bá»™ data) |
| **Speed** | Nhanh (khÃ´ng load) | Cháº­m (pháº£i load háº¿t) |
| **Use case** | File lá»›n (>RAM) | File nhá» (<RAM) |

**VÃ­ dá»¥:**
```python
# Lazy (scan_csv)
df = pl.scan_csv('16GB.csv')  # Instant! (0.1s)
# Polars chá»‰ Ä‘á»c metadata: sá»‘ cá»™t, tÃªn cá»™t, kiá»ƒu dá»¯ liá»‡u
# KHÃ”NG Ä‘á»c 179 triá»‡u dÃ²ng vÃ o RAM

# Eager (read_csv)
df = pl.read_csv('16GB.csv')  # Cháº­m! (5 phÃºt)
# Polars Ä‘á»c Háº¾T 16GB vÃ o RAM
# Náº¿u RAM < 16GB â†’ swap to disk hoáº·c crash!
```

**2. `sink_csv()` vs `write_csv()`:**

| | `sink_csv()` (Streaming) | `write_csv()` (Buffered) |
|-|--------------------------|--------------------------|
| **Memory** | âœ… Xá»­ lÃ½ tá»«ng chunk | âŒ Buffer toÃ n bá»™ |
| **Process** | Äá»c â†’ Xá»­ lÃ½ â†’ Ghi (streaming) | Äá»c â†’ Xá»­ lÃ½ â†’ Buffer â†’ Ghi |
| **RAM usage** | ~500MB (chunk size) | ~20-30GB (toÃ n bá»™) |
| **Speed** | HÆ¡i cháº­m | Nhanh hÆ¡n (nhÆ°ng cáº§n RAM) |

**Workflow `sink_csv()`:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚ Read     â”‚ â”€â”€> â”‚ Process â”‚ â”€â”€> â”‚ Writeâ”‚
â”‚ Chunk 1  â”‚     â”‚ Chunk 1 â”‚     â”‚  to  â”‚
â”‚ (100MB)  â”‚     â”‚         â”‚     â”‚ disk â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚ Read     â”‚ â”€â”€> â”‚ Process â”‚ â”€â”€> â”‚ Writeâ”‚
â”‚ Chunk 2  â”‚     â”‚ Chunk 2 â”‚     â”‚  to  â”‚
â”‚ (100MB)  â”‚     â”‚         â”‚     â”‚ disk â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”˜
                  (láº·p láº¡i 160 láº§n)
```

**3. Táº¡i sao dÃ¹ng Lazy cho file 16GB?**
- **RAM constraint**: MÃ¡y chá»‰ cÃ³ 16GB RAM, file 16GB khÃ´ng thá»ƒ load háº¿t
- **Efficiency**: Polars tá»‘i Æ°u query trÆ°á»›c khi thá»±c thi (query optimization)
- **Streaming**: Xá»­ lÃ½ tá»«ng chunk â†’ khÃ´ng cáº§n load háº¿t vÃ o RAM

**4. RAM usage khi cháº¡y:**

```python
# BÆ°á»›c 1: scan_csv
df = pl.scan_csv(DATA_RAW)  
# RAM: ~100MB (metadata only)

# BÆ°á»›c 2-4: Transformations (lazy)
df_features = df.select([...])
df_normalized = df_numeric.select([...])
# RAM: ~100MB (chá»‰ láº­p káº¿ hoáº¡ch, chÆ°a thá»±c thi)

# BÆ°á»›c 5: sink_csv (streaming execution)
df_normalized.sink_csv(temp_output)
# RAM: ~500MB-2GB (xá»­ lÃ½ tá»«ng chunk 100MB)
# Chunk 1: Äá»c 100MB â†’ Xá»­ lÃ½ â†’ Ghi â†’ XÃ³a khá»i RAM
# Chunk 2: Äá»c 100MB â†’ Xá»­ lÃ½ â†’ Ghi â†’ XÃ³a khá»i RAM
# ... (láº·p láº¡i cho Ä‘áº¿n háº¿t file)
```

**Káº¿t luáº­n:**
- âœ… RAM usage tá»‘i Ä‘a: ~2GB (thay vÃ¬ 30GB náº¿u dÃ¹ng eager)
- âœ… CÃ³ thá»ƒ xá»­ lÃ½ file 16GB trÃªn mÃ¡y 16GB RAM
- âœ… Nhanh hÆ¡n vÃ¬ Polars tá»‘i Æ°u query

---

### â“ CÃ¢u 7: HDFS Upload & Delete
**GiÃ¡o viÃªn há»i:** Trong `setup_hdfs.sh`, em tháº¥y:

```bash
hdfs dfs -put "$INPUT_TEMP" "$HDFS_BASE/input/hadoop_input.txt"

if [ $? -ne 0 ]; then
    echo "Tháº¥t báº¡i khi táº£i dá»¯ liá»‡u Ä‘áº§u vÃ o"
    exit 1
fi

rm -f "$INPUT_TEMP"
echo "ÄÃ£ xÃ³a tá»‡p táº¡m (dá»¯ liá»‡u chá»‰ cÃ²n trÃªn HDFS)"
```

**Giáº£i thÃ­ch:
1. `$?` lÃ  gÃ¬? Táº¡i sao kiá»ƒm tra `$? -ne 0`?
2. Táº¡i sao pháº£i xÃ³a `$INPUT_TEMP` sau khi upload?
3. Äiá»u gÃ¬ xáº£y ra náº¿u upload tháº¥t báº¡i nhÆ°ng váº«n xÃ³a file?
4. LÃ m sao khÃ´i phá»¥c náº¿u cáº§n dá»¯ liá»‡u nÃ y sau?**

**Sinh viÃªn tráº£ lá»i:**

**1. `$?` lÃ  gÃ¬?**

- `$?`: Exit code cá»§a lá»‡nh vá»«a cháº¡y
- `0`: ThÃ nh cÃ´ng
- `â‰ 0` (1, 2, 127, etc.): Tháº¥t báº¡i (má»—i code cÃ³ Ã½ nghÄ©a khÃ¡c nhau)

**Kiá»ƒm tra `$? -ne 0`:**
```bash
hdfs dfs -put file.txt /hdfs/path

# $? = 0 â†’ Upload thÃ nh cÃ´ng
# $? = 1 â†’ Lá»—i (vd: HDFS khÃ´ng cháº¡y)
# $? = 2 â†’ Lá»—i (vd: file khÃ´ng tá»“n táº¡i)

if [ $? -ne 0 ]; then  # ne = not equal
    echo "Tháº¥t báº¡i!"
    exit 1  # Dá»«ng script
fi
```

**Táº¡i sao quan trá»ng?**
- Náº¿u upload tháº¥t báº¡i mÃ  váº«n xÃ³a file â†’ máº¥t dá»¯ liá»‡u!
- Pháº£i kiá»ƒm tra trÆ°á»›c khi xÃ³a

**2. Táº¡i sao xÃ³a `$INPUT_TEMP`?**

**Quy Ä‘á»‹nh:** KhÃ´ng Ä‘Æ°á»£c lÆ°u dá»¯ liá»‡u lá»›n (>1GB) á»Ÿ local

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Polars      â”‚ â”€â”€>  â”‚ Temp File    â”‚ â”€â”€>  â”‚ HDFS        â”‚
â”‚ Processing  â”‚      â”‚ (31GB local) â”‚      â”‚ (permanent) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚                      
                            â”‚ (auto delete)        
                            â–¼                      
                      [rm -f file]            
```

**Workflow:**
1. Polars táº¡o file temp (31GB) á»Ÿ local
2. Upload lÃªn HDFS (31GB)
3. **XÃ³a file temp** ngay láº­p tá»©c
4. â†’ Dá»¯ liá»‡u chá»‰ cÃ²n trÃªn HDFS (tuÃ¢n thá»§ quy Ä‘á»‹nh)

**3. Náº¿u upload tháº¥t báº¡i nhÆ°ng váº«n xÃ³a?**

**Scenario xáº¥u (KHÃ”NG cÃ³ kiá»ƒm tra `$?`):**
```bash
hdfs dfs -put "$INPUT_TEMP" "/hdfs/path"
# Upload tháº¥t báº¡i (HDFS khÃ´ng cháº¡y)

rm -f "$INPUT_TEMP"
# XÃ³a file â†’ Máº¤T Dá»® LIá»†U!
```

**Káº¿t quáº£:**
- âŒ File local bá»‹ xÃ³a
- âŒ HDFS khÃ´ng cÃ³ dá»¯ liá»‡u
- âŒ Pháº£i cháº¡y láº¡i bÆ°á»›c 2 (máº¥t 10 phÃºt!)

**Scenario tá»‘t (CÃ“ kiá»ƒm tra):**
```bash
hdfs dfs -put "$INPUT_TEMP" "/hdfs/path"

if [ $? -ne 0 ]; then
    echo "Upload tháº¥t báº¡i!"
    exit 1  # Dá»ªNG, KHÃ”NG xÃ³a file
fi

# Chá»‰ cháº¡y Ä‘áº¿n Ä‘Ã¢y náº¿u upload thÃ nh cÃ´ng
rm -f "$INPUT_TEMP"
```

**Káº¿t quáº£:**
- âœ… Upload tháº¥t báº¡i â†’ script dá»«ng
- âœ… File local váº«n cÃ²n
- âœ… Sá»­a lá»—i HDFS, cháº¡y láº¡i script

**4. KhÃ´i phá»¥c dá»¯ liá»‡u:**

**Náº¿u cáº§n xem láº¡i dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½:**
```bash
# Táº£i vá» tá»« HDFS (táº¡m thá»i)
hdfs dfs -get /user/spark/hi_large/input/hadoop_input.txt \
             01_data/processed/

# Sá»­ dá»¥ng
python 02_scripts/polars/04_assign_clusters.py

# XÃ“A láº¡i sau khi dÃ¹ng xong (tuÃ¢n thá»§ quy Ä‘á»‹nh)
rm 01_data/processed/hadoop_input.txt
```

**LÆ°u Ã½:**
- âš ï¸ CHá»ˆ táº£i vá» khi cáº§n debug/phÃ¢n tÃ­ch
- âš ï¸ PHáº¢I xÃ³a ngay sau khi dÃ¹ng xong
- âœ… Dá»¯ liá»‡u vÄ©nh viá»…n chá»‰ trÃªn HDFS

---

### â“ CÃ¢u 8: Batch Processing
**GiÃ¡o viÃªn há»i:** Trong `assign_clusters_polars.py`, em tháº¥y:

```python
chunk_size = 1000000  # 1M giao dá»‹ch/batch

for i in range(0, len(data), chunk_size):
    end_idx = min(i + chunk_size, len(data))
    chunk = data[i:end_idx]
    
    distances = np.sqrt(((chunk[:, None, :] - centroids[None, :, :]) ** 2).sum(axis=2))
    clusters[i:end_idx] = np.argmin(distances, axis=1)
```

**Giáº£i thÃ­ch chi tiáº¿t:
1. Táº¡i sao xá»­ lÃ½ tá»«ng batch 1M dÃ²ng thay vÃ¬ xá»­ lÃ½ háº¿t 179M dÃ²ng cÃ¹ng lÃºc?
2. Giáº£i thÃ­ch shape cá»§a `chunk`, `centroids` vÃ  phÃ©p tÃ­nh broadcasting
3. RAM usage lÃ  bao nhiÃªu khi xá»­ lÃ½ 1 batch?
4. Náº¿u tÄƒng `chunk_size` lÃªn 10M thÃ¬ sao?**

**Sinh viÃªn tráº£ lá»i:**

**1. Táº¡i sao batch processing?**

**Váº¥n Ä‘á» náº¿u xá»­ lÃ½ háº¿t 179M dÃ²ng:**
```python
# Náº¿u xá»­ lÃ½ toÃ n bá»™
data = np.array((179_000_000, 9))  # 179M x 9 features

distances = np.sqrt(...)  # Shape: (179_000_000, 5)
# RAM cáº§n thiáº¿t:
# - data: 179M x 9 x 8 bytes = 12.9 GB
# - distances: 179M x 5 x 8 bytes = 7.2 GB
# - intermediate arrays: ~10 GB
# â†’ Tá»•ng: ~30 GB RAM!

# MÃ¡y chá»‰ cÃ³ 16GB RAM â†’ Crash hoáº·c swap to disk (ráº¥t cháº­m)
```

**Giáº£i phÃ¡p: Batch processing**
```python
# Xá»­ lÃ½ tá»«ng batch 1M dÃ²ng
chunk_size = 1_000_000

# Batch 1: 1M dÃ²ng
chunk = data[0:1_000_000]  # 1M x 9
distances = ...             # 1M x 5
# RAM: ~500 MB

# Batch 2: 1M dÃ²ng
chunk = data[1_000_000:2_000_000]
# RAM: ~500 MB

# ... (láº·p láº¡i 179 láº§n)
```

**Lá»£i Ã­ch:**
- âœ… RAM usage: ~500 MB thay vÃ¬ 30 GB
- âœ… CÃ³ thá»ƒ xá»­ lÃ½ trÃªn mÃ¡y 16GB RAM
- âœ… KhÃ´ng cáº§n swap to disk (nhanh hÆ¡n)

**2. Shape vÃ  Broadcasting:**

**Shapes:**
```python
# Input
chunk = (1_000_000, 9)      # 1M giao dá»‹ch, 9 features
centroids = (5, 9)          # 5 centroids, 9 features

# BÆ°á»›c 1: ThÃªm chiá»u
chunk[:, None, :] = (1_000_000, 1, 9)
centroids[None, :, :] = (1, 5, 9)

# BÆ°á»›c 2: Broadcasting (tá»± Ä‘á»™ng má»Ÿ rá»™ng)
chunk[:, None, :] = (1_000_000, 1, 9) â†’ (1_000_000, 5, 9)
centroids[None, :, :] = (1, 5, 9) â†’ (1_000_000, 5, 9)

# BÆ°á»›c 3: PhÃ©p trá»«
diff = chunk[:, None, :] - centroids[None, :, :]
# Shape: (1_000_000, 5, 9)
# Ã nghÄ©a: Má»—i giao dá»‹ch trá»« cho 5 centroids

# BÆ°á»›c 4: BÃ¬nh phÆ°Æ¡ng vÃ  tá»•ng
squared = diff ** 2                    # (1_000_000, 5, 9)
sum_squared = squared.sum(axis=2)      # (1_000_000, 5)
distances = np.sqrt(sum_squared)       # (1_000_000, 5)

# BÆ°á»›c 5: Argmin
clusters = np.argmin(distances, axis=1)  # (1_000_000,)
# Má»—i giao dá»‹ch gÃ¡n vÃ o cá»¥m gáº§n nháº¥t
```

**VÃ­ dá»¥ cá»¥ thá»ƒ:**
```python
# Giáº£ sá»­ 2 giao dá»‹ch, 3 features, 2 centroids
chunk = [[1, 2, 3],
         [4, 5, 6]]  # Shape: (2, 3)

centroids = [[0, 0, 0],
             [10, 10, 10]]  # Shape: (2, 3)

# Broadcasting
chunk[:, None, :] = [[[1, 2, 3]],      # Shape: (2, 1, 3)
                     [[4, 5, 6]]]

centroids[None, :, :] = [[[0, 0, 0],   # Shape: (1, 2, 3)
                          [10, 10, 10]]]

# Sau broadcasting (tá»± Ä‘á»™ng)
chunk_expanded = [[[1, 2, 3], [1, 2, 3]],        # (2, 2, 3)
                  [[4, 5, 6], [4, 5, 6]]]

centroids_expanded = [[[0, 0, 0], [10, 10, 10]], # (2, 2, 3)
                      [[0, 0, 0], [10, 10, 10]]]

# PhÃ©p trá»«
diff = [[[1, 2, 3], [-9, -8, -7]],
        [[4, 5, 6], [-6, -5, -4]]]

# Khoáº£ng cÃ¡ch Euclidean
distances = [[3.74, 13.93],   # Giao dá»‹ch 1 Ä‘áº¿n 2 centroids
             [8.77, 8.66]]    # Giao dá»‹ch 2 Ä‘áº¿n 2 centroids

# Argmin
clusters = [0, 1]  # Giao dá»‹ch 1 â†’ Cluster 0, Giao dá»‹ch 2 â†’ Cluster 1
```

**3. RAM usage 1 batch:**

```python
chunk_size = 1_000_000
num_features = 9
num_centroids = 5

# NumPy float64 = 8 bytes
chunk = 1_000_000 x 9 x 8 bytes = 72 MB
centroids = 5 x 9 x 8 bytes = 360 bytes (negligible)

# Intermediate arrays
diff = 1_000_000 x 5 x 9 x 8 = 360 MB
squared = 1_000_000 x 5 x 9 x 8 = 360 MB
sum_squared = 1_000_000 x 5 x 8 = 40 MB
distances = 1_000_000 x 5 x 8 = 40 MB

# Tá»•ng (peak RAM): ~900 MB
```

**4. Náº¿u tÄƒng chunk_size lÃªn 10M:**

```python
chunk_size = 10_000_000  # 10M

# RAM usage
chunk = 10M x 9 x 8 = 720 MB
diff = 10M x 5 x 9 x 8 = 3.6 GB
squared = 10M x 5 x 9 x 8 = 3.6 GB
distances = 10M x 5 x 8 = 400 MB

# Tá»•ng: ~9 GB
```

**Trade-off:**
- âœ… Ãt batch hÆ¡n (18 batch thay vÃ¬ 180)
- âœ… Nhanh hÆ¡n ~5-10% (Ã­t overhead)
- âš ï¸ RAM usage cao hÆ¡n (9GB thay vÃ¬ 900MB)
- âŒ Náº¿u mÃ¡y chá»‰ cÃ³ 8GB RAM â†’ swap to disk hoáº·c crash

**Káº¿t luáº­n:**
- 1M batch = sweet spot (cÃ¢n báº±ng tá»‘c Ä‘á»™ vÃ  RAM)
- 10M batch = nhanh hÆ¡n nhÆ°ng cáº§n nhiá»u RAM
- 100K batch = an toÃ n nhÆ°ng cháº­m hÆ¡n

---

## âš™ï¸ PHáº¦N 3: SPARK & MLLIB (20%)

### â“ CÃ¢u 9: Spark Config
**GiÃ¡o viÃªn há»i:** Trong `kmeans_spark.py`, giáº£i thÃ­ch cÃ¡c config:

```python
spark = SparkSession.builder \
    .config("spark.executor.memory", "8g") \
    .config("spark.executor.instances", "4") \
    .config("spark.executor.cores", "4") \
    .config("spark.sql.shuffle.partitions", "800") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .getOrCreate()
```

**Em hÃ£y giáº£i thÃ­ch:
1. Tá»•ng RAM mÃ  Spark sá»­ dá»¥ng lÃ  bao nhiÃªu?
2. Tá»•ng cores lÃ  bao nhiÃªu? CÃ³ thá»ƒ cháº¡y bao nhiÃªu tasks song song?
3. `shuffle.partitions = 800` áº£nh hÆ°á»Ÿng gÃ¬? Táº¡i sao lÃ  800?
4. KryoSerializer vs Java serializer?**

**Sinh viÃªn tráº£ lá»i:**

**1. Tá»•ng RAM:**

```python
spark.driver.memory = 8g        # RAM cho driver (coordinator)
spark.executor.memory = 8g      # RAM cho má»—i executor (worker)
spark.executor.instances = 4    # Sá»‘ executors

# Tá»•ng RAM
Total = driver + (executor_memory Ã— num_executors)
Total = 8 GB + (8 GB Ã— 4)
Total = 8 GB + 32 GB = 40 GB
```

**PhÃ¢n bá»•:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Driver     â”‚  8 GB
â”‚ (Coordinator)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚        â”‚        â”‚        â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”
â”‚Exec 1 â”‚ â”‚Exec 2â”‚ â”‚Exec 3â”‚ â”‚Exec 4â”‚
â”‚ 8 GB  â”‚ â”‚ 8 GB â”‚ â”‚ 8 GB â”‚ â”‚ 8 GB â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
```

**2. Tá»•ng cores vÃ  parallelism:**

```python
spark.executor.cores = 4        # Cores má»—i executor
spark.executor.instances = 4    # Sá»‘ executors

# Tá»•ng cores
Total cores = executor_cores Ã— num_executors
Total cores = 4 Ã— 4 = 16 cores
```

**Song song hÃ³a:**
- **Má»—i core cháº¡y 1 task** (máº·c Ä‘á»‹nh)
- **Tá»•ng tasks song song** = 16 tasks

**VÃ­ dá»¥:**
```
Executor 1:
  Core 1 â†’ Task 1 (xá»­ lÃ½ partition 1)
  Core 2 â†’ Task 2 (xá»­ lÃ½ partition 2)
  Core 3 â†’ Task 3 (xá»­ lÃ½ partition 3)
  Core 4 â†’ Task 4 (xá»­ lÃ½ partition 4)

Executor 2:
  Core 1 â†’ Task 5
  Core 2 â†’ Task 6
  Core 3 â†’ Task 7
  Core 4 â†’ Task 8

Executor 3:
  Core 1 â†’ Task 9
  ...

Executor 4:
  Core 1 â†’ Task 13
  ...
  Core 4 â†’ Task 16

â†’ 16 tasks cháº¡y Ä‘á»“ng thá»i!
```

**3. `shuffle.partitions = 800`:**

**Shuffle lÃ  gÃ¬?**
- Khi Spark cáº§n "trá»™n" dá»¯ liá»‡u giá»¯a cÃ¡c executors (vd: groupBy, join)
- Dá»¯ liá»‡u Ä‘Æ°á»£c chia thÃ nh cÃ¡c partition (pháº§n nhá»)

**Táº¡i sao 800?**
```
Quy táº¯c: shuffle_partitions â‰ˆ (num_cores Ã— 50)
= 16 cores Ã— 50
= 800 partitions
```

**Workflow:**
```
Input: 179M rows

BÆ°á»›c 1: Chia thÃ nh 800 partitions
  Partition 1: 224,000 rows
  Partition 2: 224,000 rows
  ...
  Partition 800: 224,000 rows

BÆ°á»›c 2: PhÃ¢n phá»‘i cho 16 cores
  Core 1: Xá»­ lÃ½ partition 1, 17, 33, ...  (~50 partitions)
  Core 2: Xá»­ lÃ½ partition 2, 18, 34, ...  (~50 partitions)
  ...
  Core 16: Xá»­ lÃ½ partition 16, 32, 48, ... (~50 partitions)

â†’ CÃ¢n báº±ng táº£i giá»¯a cÃ¡c cores!
```

**Náº¿u shuffle.partitions quÃ¡ nhá» (vd: 16):**
- Má»—i core xá»­ lÃ½ 1 partition lá»›n (11M rows)
- Náº¿u 1 partition lá»›n hÆ¡n cÃ¡c partition khÃ¡c â†’ 1 core cháº­m â†’ bottleneck

**Náº¿u shuffle.partitions quÃ¡ lá»›n (vd: 10,000):**
- QuÃ¡ nhiá»u partition nhá»
- Overhead táº¡o vÃ  quáº£n lÃ½ partition lá»›n
- Cháº­m hÆ¡n

**Sweet spot:** 800 (cÃ¢n báº±ng tá»‘t)

**4. KryoSerializer vs Java serializer:**

**Serialization lÃ  gÃ¬?**
- Chuyá»ƒn object (data) thÃ nh bytes Ä‘á»ƒ truyá»n qua máº¡ng
- Spark cáº§n serialize khi:
  - Truyá»n data giá»¯a driver vÃ  executors
  - Shuffle data giá»¯a cÃ¡c executors
  - Cache data vÃ o disk

**So sÃ¡nh:**

| TiÃªu chÃ­ | Java Serializer (default) | KryoSerializer |
|----------|---------------------------|----------------|
| Tá»‘c Ä‘á»™ | Cháº­m | Nhanh hÆ¡n 10x |
| KÃ­ch thÆ°á»›c | Lá»›n | Nhá» hÆ¡n 5-10x |
| TÆ°Æ¡ng thÃ­ch | Táº¥t cáº£ Java objects | Cáº§n Ä‘Äƒng kÃ½ class |
| Use case | Default, debug | Production (recommend) |

**VÃ­ dá»¥:**
```python
# Dá»¯ liá»‡u: 1 triá»‡u numbers
data = [1, 2, 3, ..., 1_000_000]

# Java serializer
serialized = java_serialize(data)
# KÃ­ch thÆ°á»›c: ~50 MB
# Thá»i gian: 100ms

# Kryo serializer
serialized = kryo_serialize(data)
# KÃ­ch thÆ°á»›c: ~5 MB (nhá» hÆ¡n 10x)
# Thá»i gian: 10ms (nhanh hÆ¡n 10x)
```

**Lá»£i Ã­ch trong dá»± Ã¡n:**
- 179M rows cáº§n serialize nhiá»u láº§n (shuffle)
- Kryo â†’ giáº£m network I/O â†’ nhanh hÆ¡n 20-30%
- Kryo â†’ giáº£m disk usage khi cache â†’ tiáº¿t kiá»‡m RAM

**Cáº¥u hÃ¬nh:**
```python
.config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
.config("spark.kryoserializer.buffer.max", "512m")  # Buffer lá»›n Ä‘á»ƒ trÃ¡nh trÃ n
```

---

### â“ CÃ¢u 10: K-means++ Initialization
**GiÃ¡o viÃªn há»i:** Trong code MLlib:

```python
kmeans = KMeans() \
    .setK(5) \
    .setInitMode("k-means||") \
    .setMaxIter(15)

model = kmeans.fit(vector_df)
```

**Giáº£i thÃ­ch:
1. `k-means||` lÃ  gÃ¬? KhÃ¡c gÃ¬ vá»›i random initialization?
2. Táº¡i sao MLlib dÃ¹ng `k-means||` thay vÃ¬ `k-means++` gá»‘c?
3. Thuáº­t toÃ¡n `k-means||` hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o?
4. So sÃ¡nh thá»i gian vÃ  cháº¥t lÆ°á»£ng**

**Sinh viÃªn tráº£ lá»i:**

**1. `k-means||` (k-means parallel) lÃ  gÃ¬?**

- `k-means||` = PhiÃªn báº£n phÃ¢n tÃ¡n cá»§a `k-means++`
- ÄÆ°á»£c thiáº¿t káº¿ cho Big Data (Apache Spark)
- Chá»n K centroids ban Ä‘áº§u THÃ”NG MINH thay vÃ¬ random

**So sÃ¡nh vá»›i random initialization:**

| | Random | k-means++ / k-means|| |
|-|--------|----------------------|
| Chá»n centroids | Ngáº«u nhiÃªn K Ä‘iá»ƒm | ThÃ´ng minh (xa nhau) |
| Sá»‘ iterations | 20-50 | 10-15 (Ã­t hÆ¡n) |
| Cháº¥t lÆ°á»£ng | KhÃ´ng á»•n Ä‘á»‹nh | á»”n Ä‘á»‹nh hÆ¡n |
| Tá»‘c Ä‘á»™ há»™i tá»¥ | Cháº­m | Nhanh hÆ¡n |
| Use case | Test, prototype | Production |

**2. Táº¡i sao dÃ¹ng `k-means||` thay vÃ¬ `k-means++`?**

**`k-means++` (gá»‘c):**
```python
# Chá»n K centroids tuáº§n tá»± (sequential)
1. Chá»n centroid 1: Random
2. Chá»n centroid 2: Xa centroid 1 nháº¥t
3. Chá»n centroid 3: Xa 2 centroids trÆ°á»›c nháº¥t
...
K. Chá»n centroid K: Xa K-1 centroids trÆ°á»›c nháº¥t
```

**Váº¥n Ä‘á»:** Sequential â†’ khÃ´ng thá»ƒ song song â†’ CHáº¬M vá»›i Big Data!

**`k-means||` (parallel):**
```python
# Chá»n K centroids song song (parallel)
Round 1: Chá»n L Ä‘iá»ƒm (L >> K) cÃ¹ng lÃºc
Round 2: Chá»n L Ä‘iá»ƒm ná»¯a
...
Round 5: Äá»§ ~5K Ä‘iá»ƒm

# Cuá»‘i cÃ¹ng: Chá»n K tá»‘t nháº¥t tá»« 5K Ä‘iá»ƒm
```

**Lá»£i Ã­ch:**
- âœ… Song song trÃªn nhiá»u executors
- âœ… Nhanh hÆ¡n k-means++ gá»‘c 10-100x
- âœ… Cháº¥t lÆ°á»£ng tÆ°Æ¡ng Ä‘Æ°Æ¡ng k-means++

**3. Thuáº­t toÃ¡n `k-means||`:**

**Input:**
- Data: 179M Ä‘iá»ƒm
- K = 5 (sá»‘ cá»¥m)
- L = 2K = 10 (sá»‘ Ä‘iá»ƒm má»—i round, oversample)

**Thuáº­t toÃ¡n:**

```python
# Round 1: Khá»Ÿi táº¡o
centroids = []
c1 = random_point()  # Chá»n 1 Ä‘iá»ƒm ngáº«u nhiÃªn
centroids.append(c1)

# Round 2-5: Láº·p 5 láº§n (log K iterations)
for round in range(1, 6):
    # TÃ­nh khoáº£ng cÃ¡ch tá»« má»—i Ä‘iá»ƒm Ä‘áº¿n centroids gáº§n nháº¥t
    for point in data:
        dist = min_distance(point, centroids)
        prob = dist^2 / sum_all_distances  # XÃ¡c suáº¥t chá»n
    
    # Chá»n L = 10 Ä‘iá»ƒm vá»›i xÃ¡c suáº¥t cao (xa centroids)
    # Song song trÃªn nhiá»u executors!
    new_points = sample(data, L, probability=prob)
    centroids.extend(new_points)  # ThÃªm vÃ o danh sÃ¡ch

# Sau 5 rounds: ~50 centroids

# BÆ°á»›c cuá»‘i: Chá»n K = 5 tá»‘t nháº¥t tá»« 50 centroids
# DÃ¹ng K-means++ trÃªn 50 Ä‘iá»ƒm (nhanh vÃ¬ Ã­t Ä‘iá»ƒm)
final_centroids = kmeans++(centroids, K=5)
```

**VÃ­ dá»¥ trá»±c quan:**

```
Round 1:
  â—‹ (random centroid 1)

Round 2 (chá»n 10 Ä‘iá»ƒm xa c1):
  â—‹ c1
  â— â— â— â— â— â— â— â— â— â— (10 Ä‘iá»ƒm má»›i)

Round 3 (chá»n 10 Ä‘iá»ƒm xa táº¥t cáº£ 11 centroids):
  â—‹ c1
  â— â— â— â— â— â— â— â— â— â—
  â˜… â˜… â˜… â˜… â˜… â˜… â˜… â˜… â˜… â˜… (10 Ä‘iá»ƒm má»›i)

...

Round 5: 50 centroids

Cuá»‘i cÃ¹ng: Chá»n 5 tá»‘t nháº¥t tá»« 50
  â—‹ â—‹ â—‹ â—‹ â—‹ (final 5 centroids)
```

**4. So sÃ¡nh thá»i gian vÃ  cháº¥t lÆ°á»£ng:**

**Thá»i gian (179M Ä‘iá»ƒm):**

| Method | Thá»i gian khá»Ÿi táº¡o | Sá»‘ iterations | Tá»•ng thá»i gian K-means |
|--------|-------------------|---------------|------------------------|
| Random | 1s | 20-50 | 20-30 phÃºt |
| k-means++ (sequential) | 10 phÃºt | 10-15 | 25 phÃºt |
| **k-means\|\| (parallel)** | **30 giÃ¢y** | **10-15** | **6-7 phÃºt** âœ… |

**Cháº¥t lÆ°á»£ng (WSSSE):**

| Method | WSSSE | Stability |
|--------|-------|-----------|
| Random | 1,200,000,000 | âŒ KhÃ´ng á»•n Ä‘á»‹nh (khÃ¡c nhau má»—i láº§n cháº¡y) |
| k-means++ | 950,000,000 | âœ… á»”n Ä‘á»‹nh |
| **k-means\|\|** | **960,000,000** | âœ… á»”n Ä‘á»‹nh (gáº§n k-means++) |

**Káº¿t luáº­n:**
- âœ… k-means|| nhanh nháº¥t (6-7 phÃºt vs 25 phÃºt)
- âœ… Cháº¥t lÆ°á»£ng tÆ°Æ¡ng Ä‘Æ°Æ¡ng k-means++ (960M vs 950M)
- âœ… á»”n Ä‘á»‹nh, khÃ´ng bá»‹ random
- â†’ Lá»±a chá»n tá»‘t nháº¥t cho Big Data!

---

## ğŸ› PHáº¦N 4: DEBUG & TROUBLESHOOTING (20%)

### â“ CÃ¢u 11: Debugging Pipeline
**GiÃ¡o viÃªn há»i:** Pipeline bá»‹ lá»—i á»Ÿ bÆ°á»›c 4 (K-means). Em debug nhÆ° tháº¿ nÃ o?

**CÃ¡c bÆ°á»›c debug chi tiáº¿t:**

**Sinh viÃªn tráº£ lá»i:**

**BÆ°á»›c 1: XÃ¡c Ä‘á»‹nh lá»—i**

```bash
# Kiá»ƒm tra log
cat 04_logs/pipeline_log_*.md

# TÃ¬m dÃ²ng lá»—i
# VÃ­ dá»¥ output:
âŒ BÆ°á»›c 4 tháº¥t báº¡i
```

**BÆ°á»›c 2: Kiá»ƒm tra HDFS**

```bash
# HDFS cÃ³ Ä‘ang cháº¡y khÃ´ng?
hdfs dfsadmin -report

# Output mong Ä‘á»£i:
# Live datanodes (1):
# Name: ...

# Náº¿u lá»—i "Connection refused":
# â†’ HDFS khÃ´ng cháº¡y
# Fix: start-dfs.sh
```

**BÆ°á»›c 3: Kiá»ƒm tra dá»¯ liá»‡u input**

```bash
# File cÃ³ tá»“n táº¡i khÃ´ng?
hdfs dfs -ls /user/spark/hi_large/input/

# Output:
# hadoop_input.txt  31 GB

# Xem vÃ i dÃ²ng Ä‘áº§u
hdfs dfs -cat /user/spark/hi_large/input/hadoop_input.txt | head -5

# Output mong Ä‘á»£i: 9 sá»‘ má»—i dÃ²ng (normalized)
# -0.234,0.567,...,-1.234
# 1.234,-0.456,...,0.789
```

**BÆ°á»›c 4: Kiá»ƒm tra Spark**

```bash
# Spark cÃ³ cÃ i Ä‘áº·t Ä‘Ãºng khÃ´ng?
spark-submit --version

# Java version
java -version  # Cáº§n Java 11 hoáº·c 17

# Memory available
free -h
# Cáº§n Ã­t nháº¥t 16GB RAM free
```

**BÆ°á»›c 5: Cháº¡y láº¡i bÆ°á»›c 4 riÃªng láº»**

```bash
# Cháº¡y manual Ä‘á»ƒ xem lá»—i chi tiáº¿t
./02_scripts/spark/run_spark.sh

# Xem output/error messages
# CÃ¡c lá»—i phá»• biáº¿n:
# - OutOfMemoryError â†’ TÄƒng memory config
# - FileNotFoundException â†’ HDFS khÃ´ng cÃ³ file
# - Connection refused â†’ HDFS khÃ´ng cháº¡y
```

**BÆ°á»›c 6: Kiá»ƒm tra checkpoint**

```bash
# CÃ¡c bÆ°á»›c trÆ°á»›c Ä‘Ã£ cháº¡y chÆ°a?
ls .pipeline_checkpoints/

# Output:
# step_1.done
# step_2.done
# step_3.done
# (step_4.done khÃ´ng cÃ³ â†’ bÆ°á»›c 4 chÆ°a hoÃ n thÃ nh)

# Náº¿u cáº§n cháº¡y láº¡i tá»« Ä‘áº§u:
./02_scripts/pipeline/reset_pipeline.sh
```

**BÆ°á»›c 7: Test vá»›i dá»¯ liá»‡u nhá»**

```bash
# Táº¡o file test nhá» (1000 dÃ²ng)
hdfs dfs -cat /user/spark/hi_large/input/hadoop_input.txt | head -1000 > test.txt

# Upload test file
hdfs dfs -put test.txt /user/spark/test_input.txt

# Sá»­a script run_spark.sh (táº¡m thá»i)
# INPUT: /user/spark/test_input.txt

# Cháº¡y K-means vá»›i test data
./02_scripts/spark/run_spark.sh

# Náº¿u thÃ nh cÃ´ng â†’ Váº¥n Ä‘á» lÃ  dá»¯ liá»‡u lá»›n (out of memory)
# Náº¿u váº«n lá»—i â†’ Váº¥n Ä‘á» lÃ  config hoáº·c code
```

**BÆ°á»›c 8: Giáº£m memory requirements**

```python
# Sá»­a trong kmeans_spark.py
spark = SparkSession.builder \
    .config("spark.executor.memory", "4g") \  # Giáº£m tá»« 8g â†’ 4g
    .config("spark.executor.instances", "2") \ # Giáº£m tá»« 4 â†’ 2
    .getOrCreate()

# Hoáº·c sample data
df = df.sample(0.5)  # Chá»‰ láº¥y 50% data
```

---

ÄÃ¢y lÃ  file siÃªu chi tiáº¿t vá»›i 11 cÃ¢u há»i sÃ¢u vá» code vÃ  workflow, má»—i cÃ¢u cÃ³:
- âœ… Giáº£i thÃ­ch code tá»«ng dÃ²ng
- âœ… VÃ­ dá»¥ cá»¥ thá»ƒ vá»›i sá»‘ liá»‡u
- âœ… So sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p
- âœ… Trade-offs vÃ  debugging

GiÃ¡o viÃªn cÃ³ thá»ƒ dÃ¹ng Ä‘á»ƒ:
- Kiá»ƒm tra hiá»ƒu biáº¿t sÃ¢u vá» code
- ÄÃ¡nh giÃ¡ kháº£ nÄƒng debug
- Kiá»ƒm tra hiá»ƒu workflow
- Test kháº£ nÄƒng giáº£i thÃ­ch ká»¹ thuáº­t
