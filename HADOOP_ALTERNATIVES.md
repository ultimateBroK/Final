# ğŸš€ Giáº£i PhÃ¡p Thay Tháº¿ Hadoop Cho BÆ°á»›c 4 (K-means Clustering)

## ğŸ“Š Tá»•ng Quan

BÆ°á»›c 4 hiá»‡n táº¡i sá»­ dá»¥ng Hadoop MapReduce Ä‘á»ƒ cháº¡y K-means clustering trÃªn 180M rows (~33GB hadoop_input.txt). ÄÃ¢y lÃ  bottleneck lá»›n nháº¥t trong pipeline.

**Váº¥n Ä‘á» vá»›i Hadoop:**
- Overhead cao tá»« HDFS I/O (upload/download qua network)
- Serialize/deserialize dá»¯ liá»‡u má»—i iteration
- Shuffle phase cháº­m giá»¯a mapper vÃ  reducer
- KhÃ´ng táº­n dá»¥ng tá»‘i Ä‘a RAM cá»§a single machine
- Thá»i gian xá»­ lÃ½: **1-2 giá»** cho 15 iterations

---

## ğŸ”¥ Giáº£i PhÃ¡p 1: MiniBatch K-Means (Scikit-learn)

**Nhanh nháº¥t vÃ  Ä‘Æ¡n giáº£n nháº¥t - KHUYáº¾N NGHá»Š cho háº§u háº¿t trÆ°á»ng há»£p**

### Táº¡i sao nhanh hÆ¡n?
- Chá»‰ load má»™t pháº§n dá»¯ liá»‡u vÃ o RAM má»—i láº§n (streaming)
- Táº­n dá»¥ng optimized C/Cython code
- KhÃ´ng cÃ³ network overhead
- Converge nhanh hÆ¡n (~5-8 iterations thay vÃ¬ 15)

### Implementation

```python path=null start=null
# step4_minibatch_kmeans.py
import polars as pl
import numpy as np
from sklearn.cluster import MiniBatchKMeans
from time import time

print("Step 4: MiniBatch K-Means (Scikit-learn)")
start_time = time()

# Load dá»¯ liá»‡u Ä‘Ã£ chuáº©n bá»‹
print("Loading data...")
df = pl.read_csv('hadoop_input.txt', has_header=False, 
                 new_columns=['Amount_Received', 'Amount_Paid', 'amount_ratio', 'hour'])

# Chuyá»ƒn sang numpy array
X = df.to_numpy()
print(f"Data shape: {X.shape}")

# MiniBatch K-Means
print("\nTraining MiniBatch K-Means...")
kmeans = MiniBatchKMeans(
    n_clusters=5,
    batch_size=100000,      # Process 100K rows at a time
    max_iter=100,           # Max iterations
    random_state=42,
    verbose=1,
    compute_labels=False,   # Chá»‰ train centroids, khÃ´ng assign labels
    init='k-means++',
    n_init=3                # Sá»‘ láº§n khá»Ÿi táº¡o
)

# CÃ³ thá»ƒ train trÃªn chunks náº¿u RAM khÃ´ng Ä‘á»§
# Uncomment block nÃ y náº¿u RAM < 16GB:
# chunk_size = 10_000_000
# for i in range(0, len(X), chunk_size):
#     print(f"Processing chunk {i//chunk_size + 1}...")
#     kmeans.partial_fit(X[i:i+chunk_size])

# Train toÃ n bá»™ (náº¿u RAM Ä‘á»§)
kmeans.fit(X)

# LÆ°u centroids
centroids = kmeans.cluster_centers_
np.savetxt('final_centroids.txt', centroids, delimiter='\t', fmt='%.6f')

elapsed_time = time() - start_time
print(f"\nâœ… Completed in {elapsed_time/60:.2f} minutes")
print(f"Centroids saved to final_centroids.txt")
print(f"Inertia: {kmeans.inertia_:.2f}")
```

### Káº¿t quáº£ dá»± kiáº¿n
- **Thá»i gian**: 8-15 phÃºt (thay vÃ¬ 1-2 giá»)
- **Tá»‘c Ä‘á»™ tÄƒng**: 8-12x nhanh hÆ¡n
- **RAM cáº§n**: 8-16GB
- **Accuracy**: ~95-98% so vá»›i standard K-Means

### Cháº¡y pipeline má»›i

```bash
# Sá»­a file full_pipeline.sh, thay dÃ²ng 132:
# bash run_hadoop_optimized.sh 2>&1 | tee -a "$LOG_FILE"
# ThÃ nh:
python step4_minibatch_kmeans.py 2>&1 | tee -a "$LOG_FILE"
```

---

## âš¡ Giáº£i PhÃ¡p 2: Dask-ML K-Means (Parallel + Out-of-Core)

**Tá»‘t nháº¥t khi cÃ³ nhiá»u CPU cores vÃ  muá»‘n 100% accuracy**

### Táº¡i sao nhanh hÆ¡n?
- Parallel processing trÃªn táº¥t cáº£ CPU cores
- Out-of-core: xá»­ lÃ½ dá»¯ liá»‡u lá»›n hÆ¡n RAM
- Thuáº­t toÃ¡n giá»‘ng sklearn nhÆ°ng distributed

### Implementation

```python path=null start=null
# step4_dask_kmeans.py
import dask.dataframe as dd
import dask.array as da
from dask_ml.cluster import KMeans
from time import time
import numpy as np

print("Step 4: Dask-ML K-Means (Parallel)")
start_time = time()

# Load vá»›i Dask (lazy)
print("Loading data with Dask...")
df = dd.read_csv('hadoop_input.txt', header=None, 
                 names=['Amount_Received', 'Amount_Paid', 'amount_ratio', 'hour'],
                 blocksize='256MB')  # Process in 256MB chunks

# Convert to dask array
X = df.to_dask_array(lengths=True)
print(f"Data shape: {X.shape}")

# Dask K-Means
print("\nTraining Dask K-Means...")
kmeans = KMeans(
    n_clusters=5,
    init='k-means++',
    max_iter=100,
    random_state=42
)

kmeans.fit(X)

# LÆ°u centroids
centroids = kmeans.cluster_centers_
np.savetxt('final_centroids.txt', centroids, delimiter='\t', fmt='%.6f')

elapsed_time = time() - start_time
print(f"\nâœ… Completed in {elapsed_time/60:.2f} minutes")
print(f"Centroids saved to final_centroids.txt")
```

### CÃ i Ä‘áº·t

```bash
pip install dask[complete] dask-ml
```

### Káº¿t quáº£ dá»± kiáº¿n
- **Thá»i gian**: 15-25 phÃºt
- **Tá»‘c Ä‘á»™ tÄƒng**: 4-6x nhanh hÆ¡n
- **RAM cáº§n**: CÃ³ thá»ƒ xá»­ lÃ½ data > RAM
- **Accuracy**: 100% giá»‘ng standard K-Means

---

## ğŸ”¬ Giáº£i PhÃ¡p 3: FAISS K-Means (GPU-accelerated)

**Nhanh nháº¥t náº¿u cÃ³ GPU**

### Táº¡i sao nhanh hÆ¡n?
- Cháº¡y trÃªn GPU (hÃ ng nghÃ¬n cores)
- Optimized cho high-dimensional data
- Vector operations siÃªu nhanh

### Implementation

```python path=null start=null
# step4_faiss_kmeans.py
import polars as pl
import numpy as np
import faiss
from time import time

print("Step 4: FAISS K-Means (GPU-accelerated)")
start_time = time()

# Load data
print("Loading data...")
df = pl.read_csv('hadoop_input.txt', has_header=False,
                 new_columns=['Amount_Received', 'Amount_Paid', 'amount_ratio', 'hour'])
X = df.to_numpy().astype('float32')  # FAISS requires float32

print(f"Data shape: {X.shape}")
n_samples, n_features = X.shape

# FAISS K-Means
print("\nTraining FAISS K-Means...")
n_clusters = 5
kmeans = faiss.Kmeans(
    d=n_features,           # dimensionality
    k=n_clusters,           # number of clusters
    niter=100,              # max iterations
    verbose=True,
    gpu=True,               # Use GPU (set False if no GPU)
    nredo=3,                # sá»‘ láº§n khá»Ÿi táº¡o
    seed=42
)

# Train
kmeans.train(X)

# LÆ°u centroids
centroids = kmeans.centroids
np.savetxt('final_centroids.txt', centroids, delimiter='\t', fmt='%.6f')

elapsed_time = time() - start_time
print(f"\nâœ… Completed in {elapsed_time/60:.2f} minutes")
print(f"Centroids saved to final_centroids.txt")
print(f"Final objective: {kmeans.obj[-1]:.2f}")
```

### CÃ i Ä‘áº·t

```bash
# CPU version
pip install faiss-cpu

# GPU version (náº¿u cÃ³ CUDA)
pip install faiss-gpu
```

### Káº¿t quáº£ dá»± kiáº¿n
- **Thá»i gian GPU**: 3-5 phÃºt (30-40x nhanh hÆ¡n!)
- **Thá»i gian CPU**: 10-15 phÃºt (8-12x nhanh hÆ¡n)
- **RAM/VRAM cáº§n**: 8-16GB
- **Accuracy**: 100%

---

## ğŸ”¥ Giáº£i PhÃ¡p 4: Apache Spark MLlib

**Tá»‘t nháº¥t cho distributed computing vá»›i API dá»… dÃ¹ng hÆ¡n Hadoop**

### Táº¡i sao tá»‘t hÆ¡n Hadoop?
- API high-level dá»… dÃ¹ng hÆ¡n MapReduce
- In-memory processing (nhanh hÆ¡n 10-100x so vá»›i Hadoop)
- KhÃ´ng cáº§n viáº¿t mapper/reducer thá»§ cÃ´ng
- MLlib cÃ³ sáºµn K-Means optimized
- Váº«n distributed nhÆ° Hadoop nhÆ°ng hiá»‡n Ä‘áº¡i hÆ¡n

### Implementation

```python path=null start=null
# step4_spark_kmeans.py
from pyspark.sql import SparkSession
from pyspark.ml.clustering import KMeans
from pyspark.ml.feature import VectorAssembler
from time import time
import numpy as np

print("Step 4: Apache Spark K-Means")
start_time = time()

# Khá»Ÿi táº¡o Spark
spark = SparkSession.builder \
    .appName("HI-Large K-Means") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .config("spark.sql.shuffle.partitions", "200") \
    .config("spark.default.parallelism", "200") \
    .getOrCreate()

print("Loading data...")
# Load dá»¯ liá»‡u
df = spark.read.csv('hadoop_input.txt', inferSchema=True, header=False)
df = df.toDF('Amount_Received', 'Amount_Paid', 'amount_ratio', 'hour')

print(f"Data loaded: {df.count()} rows")

# Vector assembly (Spark MLlib requires vector format)
assembler = VectorAssembler(
    inputCols=['Amount_Received', 'Amount_Paid', 'amount_ratio', 'hour'],
    outputCol='features'
)
df_features = assembler.transform(df)

# K-Means
print("\nTraining Spark K-Means...")
kmeans = KMeans(
    k=5,
    maxIter=100,
    seed=42,
    featuresCol='features',
    predictionCol='cluster'
)

# Train
model = kmeans.fit(df_features)

# Láº¥y centroids
centroids = model.clusterCenters()
centroids_array = np.array(centroids)

# LÆ°u centroids
np.savetxt('final_centroids.txt', centroids_array, delimiter='\t', fmt='%.6f')

# Metrics
wssse = model.summary.trainingCost
print(f"\nWithin Set Sum of Squared Errors: {wssse:.2f}")
print(f"Number of iterations: {model.summary.numIter}")

elapsed_time = time() - start_time
print(f"\nâœ… Completed in {elapsed_time/60:.2f} minutes")
print(f"Centroids saved to final_centroids.txt")

# Cleanup
spark.stop()
```

### CÃ i Ä‘áº·t

```bash
# CÃ i PySpark
pip install pyspark

# Hoáº·c vá»›i conda
conda install pyspark
```

### Káº¿t quáº£ dá»± kiáº¿n
- **Thá»i gian (local mode)**: 20-30 phÃºt
- **Thá»i gian (cluster mode)**: 10-15 phÃºt
- **Tá»‘c Ä‘á»™ tÄƒng**: 3-6x nhanh hÆ¡n Hadoop
- **RAM cáº§n**: 8-16GB (local), scalable (cluster)
- **Accuracy**: 100%

### Spark vs Hadoop

| TiÃªu chÃ­ | Hadoop MapReduce | Apache Spark |
|----------|------------------|-------------|
| Xá»­ lÃ½ | Disk-based | In-memory |
| API | Low-level (mapper/reducer) | High-level (DataFrame, ML) |
| Tá»‘c Ä‘á»™ | Baseline | 10-100x nhanh hÆ¡n |
| Code | Nhiá»u, phá»©c táº¡p | Ãt, dá»… hiá»ƒu |
| Iterative algorithms | Cháº­m (write to disk) | Nhanh (keep in memory) |
| Setup | ÄÆ¡n giáº£n | Medium |

### Spark Standalone (Local Mode)

```python path=null start=null
# step4_spark_kmeans_local.py - Simplified cho single machine
from pyspark.sql import SparkSession
from pyspark.ml.clustering import KMeans
from pyspark.ml.feature import VectorAssembler
import numpy as np

# Spark local mode - dÃ¹ng táº¥t cáº£ cores
spark = SparkSession.builder \
    .appName("HI-Large K-Means Local") \
    .master("local[*]") \
    .config("spark.driver.memory", "8g") \
    .config("spark.driver.maxResultSize", "4g") \
    .getOrCreate()

print("Loading data with Spark (local mode)...")
df = spark.read.csv('hadoop_input.txt', inferSchema=True, header=False) \
    .toDF('f0', 'f1', 'f2', 'f3')

# Cache Ä‘á»ƒ tÄƒng tá»‘c
df.cache()
print(f"Total rows: {df.count():,}")

# Vector assembly
assembler = VectorAssembler(inputCols=['f0', 'f1', 'f2', 'f3'], outputCol='features')
df_vec = assembler.transform(df).select('features')

# K-Means vá»›i tuning
kmeans = KMeans(k=5, seed=42, maxIter=100, initMode='k-means||')
model = kmeans.fit(df_vec)

# LÆ°u káº¿t quáº£
centroids = np.array(model.clusterCenters())
np.savetxt('final_centroids.txt', centroids, delimiter='\t', fmt='%.6f')

print(f"âœ… WSSSE: {model.summary.trainingCost:.2f}")
print(f"Iterations: {model.summary.numIter}")

spark.stop()
```

### Cháº¡y vá»›i Spark Submit (Production)

```bash
# submit_spark_job.sh
spark-submit \
  --master local[*] \
  --driver-memory 8g \
  --executor-memory 8g \
  --conf spark.sql.shuffle.partitions=200 \
  --conf spark.default.parallelism=200 \
  step4_spark_kmeans.py
```

### Æ¯u Ä‘iá»ƒm Spark so vá»›i cÃ¡c giáº£i phÃ¡p khÃ¡c

âœ… **vs Hadoop MapReduce**:
- Nhanh hÆ¡n 10-30x (in-memory)
- Code ngáº¯n gá»n hÆ¡n ráº¥t nhiá»u
- API hiá»‡n Ä‘áº¡i, dá»… maintain

âœ… **vs Scikit-learn/FAISS**:
- Scalable ra nhiá»u mÃ¡y (horizontal scaling)
- Xá»­ lÃ½ Ä‘Æ°á»£c data > RAM cá»§a 1 mÃ¡y
- Fault tolerant (production-ready)

âŒ **vs Scikit-learn MiniBatch**:
- Cháº­m hÆ¡n trÃªn single machine
- Setup phá»©c táº¡p hÆ¡n
- Overhead tá»« Spark framework

### Khi nÃ o nÃªn dÃ¹ng Spark?

âœ… **NÃŠN dÃ¹ng khi**:
- CÃ³ cluster nhiá»u mÃ¡y
- Dá»¯ liá»‡u > 100GB
- Cáº§n fault tolerance cao
- Pipeline cÃ³ nhiá»u bÆ°á»›c ML khÃ¡c
- Team Ä‘Ã£ quen vá»›i Spark ecosystem

âŒ **KHÃ”NG NÃŠN dÃ¹ng khi**:
- Single machine Ä‘á»§ RAM (dÃ¹ng MiniBatch/FAISS nhanh hÆ¡n)
- Chá»‰ cáº§n POC/prototype
- Dá»¯ liá»‡u < 50GB (overhead khÃ´ng Ä‘Ã¡ng)

---

## ğŸ» Giáº£i PhÃ¡p 5: Polars + Custom K-Means

**Best balance giá»¯a tá»‘c Ä‘á»™ vÃ  control**

### Implementation

```python path=null start=null
# step4_polars_kmeans.py
import polars as pl
import numpy as np
from time import time

def kmeans_iteration(df, centroids):
    """Má»™t iteration cá»§a K-means using Polars"""
    # TÃ­nh khoáº£ng cÃ¡ch Ä‘áº¿n táº¥t cáº£ centroids
    distances = []
    for i, centroid in enumerate(centroids):
        dist_expr = pl.lit(0.0)
        for j, col in enumerate(df.columns):
            dist_expr = dist_expr + (pl.col(col) - centroid[j]) ** 2
        distances.append(dist_expr.sqrt().alias(f'dist_{i}'))
    
    # Assign clusters
    df_with_dist = df.with_columns(distances)
    dist_cols = [f'dist_{i}' for i in range(len(centroids))]
    df_clustered = df_with_dist.with_columns(
        pl.concat_list(dist_cols).list.arg_min().alias('cluster')
    )
    
    # TÃ­nh centroids má»›i
    new_centroids = []
    for i in range(len(centroids)):
        cluster_data = df_clustered.filter(pl.col('cluster') == i)
        if len(cluster_data) > 0:
            centroid = cluster_data.select(df.columns).mean().to_numpy()[0]
            new_centroids.append(centroid)
        else:
            new_centroids.append(centroids[i])  # Keep old centroid
    
    return np.array(new_centroids), df_clustered['cluster']

print("Step 4: Custom K-Means with Polars")
start_time = time()

# Load data
print("Loading data...")
df = pl.read_csv('hadoop_input.txt', has_header=False,
                 new_columns=['f0', 'f1', 'f2', 'f3'])

# Load initial centroids
centroids = np.loadtxt('centroids.txt', delimiter='\t')
print(f"Data shape: {df.shape}")
print(f"Initial centroids shape: {centroids.shape}")

# K-Means iterations
max_iter = 100
tolerance = 1e-4

for iteration in range(max_iter):
    print(f"\nIteration {iteration + 1}/{max_iter}")
    
    # Má»™t iteration
    new_centroids, labels = kmeans_iteration(df, centroids)
    
    # Check convergence
    centroid_shift = np.linalg.norm(new_centroids - centroids)
    print(f"Centroid shift: {centroid_shift:.6f}")
    
    if centroid_shift < tolerance:
        print(f"âœ… Converged at iteration {iteration + 1}")
        break
    
    centroids = new_centroids

# LÆ°u centroids
np.savetxt('final_centroids.txt', centroids, delimiter='\t', fmt='%.6f')

elapsed_time = time() - start_time
print(f"\nâœ… Completed in {elapsed_time/60:.2f} minutes")
```

### Káº¿t quáº£ dá»± kiáº¿n
- **Thá»i gian**: 20-30 phÃºt
- **Tá»‘c Ä‘á»™ tÄƒng**: 3-5x nhanh hÆ¡n
- **RAM cáº§n**: 16-32GB
- **Accuracy**: 100%

---

## ğŸ“Š So SÃ¡nh CÃ¡c Giáº£i PhÃ¡p

| PhÆ°Æ¡ng PhÃ¡p | Thá»i Gian | Tá»‘c Äá»™ TÄƒng | RAM Cáº§n | Accuracy | Äá»™ Phá»©c Táº¡p | Khuyáº¿n Nghá»‹ |
|-------------|-----------|-------------|---------|----------|-------------|-------------|
| **Hadoop MapReduce** (hiá»‡n táº¡i) | 1-2 giá» | 1x | 4-8GB | 100% | Medium | âŒ Cháº­m nháº¥t |
| **MiniBatch K-Means** | 8-15 phÃºt | 8-12x | 8-16GB | 95-98% | Easy | âœ… **BEST** |
| **Dask-ML K-Means** | 15-25 phÃºt | 4-6x | >RAM OK | 100% | Easy | âœ… RAM tháº¥p |
| **FAISS (GPU)** | 3-5 phÃºt | 30-40x | 8GB VRAM | 100% | Medium | âœ… CÃ³ GPU |
| **FAISS (CPU)** | 10-15 phÃºt | 8-12x | 8-16GB | 100% | Medium | âœ… Nhanh |
| **Spark (local)** | 20-30 phÃºt | 3-6x | 8-16GB | 100% | Medium | âœ… Modern |
| **Spark (cluster)** | 10-15 phÃºt | 6-12x | Scalable | 100% | Medium | âœ… Production |
| **Polars Custom** | 20-30 phÃºt | 3-5x | 16-32GB | 100% | Hard | âš ï¸ Advanced |

---

## ğŸ¯ Khuyáº¿n Nghá»‹ Cá»¥ Thá»ƒ

### TrÆ°á»ng há»£p 1: Development/Testing (Æ°u tiÃªn tá»‘c Ä‘á»™)
```bash
# DÃ¹ng MiniBatch K-Means
pip install scikit-learn
python step4_minibatch_kmeans.py
```
**Káº¿t quáº£**: 8-15 phÃºt, accuracy 95-98%

### TrÆ°á»ng há»£p 2: Production (Æ°u tiÃªn accuracy)
```bash
# DÃ¹ng FAISS CPU
pip install faiss-cpu
python step4_faiss_kmeans.py
```
**Káº¿t quáº£**: 10-15 phÃºt, accuracy 100%

### TrÆ°á»ng há»£p 3: CÃ³ GPU
```bash
# DÃ¹ng FAISS GPU
pip install faiss-gpu
python step4_faiss_kmeans.py
```
**Káº¿t quáº£**: 3-5 phÃºt, accuracy 100%

### TrÆ°á»ng há»£p 4: RAM tháº¥p (<8GB)
```bash
# DÃ¹ng Dask-ML (out-of-core)
pip install dask[complete] dask-ml
python step4_dask_kmeans.py
```
**Káº¿t quáº£**: 15-25 phÃºt, accuracy 100%

### TrÆ°á»ng há»£p 5: Muá»‘n thay Hadoop báº±ng framework hiá»‡n Ä‘áº¡i hÆ¡n
```bash
# DÃ¹ng Apache Spark (API dá»… hÆ¡n, nhanh hÆ¡n Hadoop)
pip install pyspark
python step4_spark_kmeans.py
```
**Káº¿t quáº£**: 20-30 phÃºt (local), 10-15 phÃºt (cluster), accuracy 100%

---

## ğŸ”§ Migration Guide

### BÆ°á»›c 1: Chá»n giáº£i phÃ¡p
Dá»±a vÃ o báº£ng so sÃ¡nh vÃ  khuyáº¿n nghá»‹ trÃªn

### BÆ°á»›c 2: Táº¡o file má»›i
Táº¡o file script tÆ°Æ¡ng á»©ng (vÃ­ dá»¥: `step4_minibatch_kmeans.py`)

### BÆ°á»›c 3: Sá»­a pipeline
Sá»­a file `full_pipeline.sh`:

```bash
# TÃ¬m dÃ²ng 132:
bash run_hadoop_optimized.sh 2>&1 | tee -a "$LOG_FILE"

# Thay thÃ nh (vÃ­ dá»¥ vá»›i MiniBatch):
python step4_minibatch_kmeans.py 2>&1 | tee -a "$LOG_FILE"
```

### BÆ°á»›c 4: Test
```bash
# Reset pipeline
./reset_pipeline.sh

# Cháº¡y láº¡i
./full_pipeline.sh
```

### BÆ°á»›c 5: Verify káº¿t quáº£
```bash
# So sÃ¡nh centroids
cat final_centroids.txt

# Cháº¡y bÆ°á»›c 5-6 nhÆ° bÃ¬nh thÆ°á»ng
python assign_clusters_polars.py
python analyze_polars.py
```

---

## âš ï¸ LÆ°u Ã Quan Trá»ng

### 1. Compatibility vá»›i pipeline hiá»‡n táº¡i
- Táº¥t cáº£ cÃ¡c giáº£i phÃ¡p Ä‘á»u output `final_centroids.txt` cÃ¹ng format
- BÆ°á»›c 5 vÃ  6 khÃ´ng cáº§n thay Ä‘á»•i
- Chá»‰ cáº§n thay tháº¿ bÆ°á»›c 4

### 2. Trade-offs
- **MiniBatch**: Máº¥t má»™t chÃºt accuracy (~2-5%) nhÆ°ng cá»±c nhanh
- **Dask/FAISS**: 100% accuracy nhÆ°ng cáº§n cÃ i thÃªm dependencies
- **GPU**: Cáº§n CUDA/NVIDIA GPU

### 3. Khi nÃ o váº«n nÃªn dÃ¹ng Hadoop?
- Dá»¯ liá»‡u > 100GB vÃ  khÃ´ng fit vÃ o RAM
- CÃ³ cluster nhiá»u mÃ¡y
- Cáº§n fault tolerance cao (production critical)

### 4. Optimization tips
- Giáº£m sá»‘ features náº¿u cÃ³ thá»ƒ (step 2)
- Sample data cho testing
- Tune `batch_size` vÃ  `max_iter`

---

## ğŸ“ˆ Æ¯á»›c TÃ­nh Hiá»‡u Suáº¥t

### Pipeline hiá»‡n táº¡i
```
Step 1: 1 phÃºt
Step 2: 10 phÃºt  
Step 3: <1 phÃºt
Step 4: 90 phÃºt (HADOOP) â°
Step 5: 15 phÃºt
Step 6: 5 phÃºt
--------------------
TOTAL: ~122 phÃºt (~2 giá»)
```

### Pipeline má»›i (vá»›i MiniBatch K-Means)
```
Step 1: 1 phÃºt
Step 2: 10 phÃºt  
Step 3: <1 phÃºt
Step 4: 10 phÃºt (MiniBatch) âš¡
Step 5: 15 phÃºt
Step 6: 5 phÃºt
--------------------
TOTAL: ~42 phÃºt
```

**TÄƒng tá»‘c tá»•ng thá»ƒ**: **~3x nhanh hÆ¡n** (2 giá» â†’ 42 phÃºt)

---

## ğŸš€ Quick Start Commands

```bash
# Giáº£i phÃ¡p khuyáº¿n nghá»‹ (MiniBatch K-Means)
pip install scikit-learn

# Táº¡o file step4_minibatch_kmeans.py (copy code tá»« Giáº£i phÃ¡p 1)

# Sá»­a full_pipeline.sh dÃ²ng 132
# Tá»«: bash run_hadoop_optimized.sh 2>&1 | tee -a "$LOG_FILE"
# ThÃ nh: python step4_minibatch_kmeans.py 2>&1 | tee -a "$LOG_FILE"

# Reset vÃ  cháº¡y
./reset_pipeline.sh
./full_pipeline.sh
```

**Expected result**: Pipeline hoÃ n thÃ nh trong 40-50 phÃºt thay vÃ¬ 2 giá»! ğŸ‰

---

## ğŸ“ Troubleshooting

### RAM khÃ´ng Ä‘á»§
```python
# Trong step4_minibatch_kmeans.py, uncomment pháº§n train theo chunks:
chunk_size = 10_000_000
for i in range(0, len(X), chunk_size):
    kmeans.partial_fit(X[i:i+chunk_size])
```

### Accuracy khÃ´ng Ä‘á»§ cao
```python
# TÄƒng batch_size vÃ  sá»‘ iterations
kmeans = MiniBatchKMeans(
    n_clusters=5,
    batch_size=200000,    # TÄƒng tá»« 100K
    max_iter=200,         # TÄƒng tá»« 100
    n_init=5              # TÄƒng sá»‘ láº§n khá»Ÿi táº¡o
)
```

### Muá»‘n giá»¯ 100% accuracy
â†’ DÃ¹ng FAISS hoáº·c Dask-ML thay vÃ¬ MiniBatch

---

## ğŸ“ Káº¿t Luáº­n

**TL;DR**: Thay Hadoop báº±ng **MiniBatch K-Means** (scikit-learn) Ä‘á»ƒ:
- âš¡ Giáº£m thá»i gian tá»« 90 phÃºt â†’ 10 phÃºt (9x nhanh hÆ¡n)
- ğŸ¯ Pipeline tá»•ng thá»ƒ: 2 giá» â†’ 40 phÃºt (3x nhanh hÆ¡n)
- ğŸ’» ÄÆ¡n giáº£n hÆ¡n, khÃ´ng cáº§n HDFS
- ğŸ“Š Accuracy váº«n ráº¥t tá»‘t (95-98%)

**Khi nÃ o dÃ¹ng cÃ¡c giáº£i phÃ¡p khÃ¡c**:
- CÃ³ GPU â†’ **FAISS GPU** (3-5 phÃºt)
- Cáº§n 100% accuracy â†’ **FAISS CPU** hoáº·c **Dask-ML**
- RAM tháº¥p â†’ **Dask-ML** (out-of-core)
- Muá»‘n upgrade tá»« Hadoop â†’ **Apache Spark** (API hiá»‡n Ä‘áº¡i, nhanh hÆ¡n 10-30x)
- CÃ³ cluster â†’ **Spark cluster mode** (scalable, production-ready)

### ğŸ’¡ Spark vs Other Solutions

**Apache Spark lÃ  lá»±a chá»n tá»‘t náº¿u**:
- âœ… Báº¡n Ä‘Ã£ cÃ³ Hadoop cluster (dá»… migrate)
- âœ… Team quen vá»›i distributed systems
- âœ… Dá»¯ liá»‡u sáº½ tiáº¿p tá»¥c tÄƒng lÃªn
- âœ… Cáº§n pipeline ML phá»©c táº¡p hÆ¡n trong tÆ°Æ¡ng lai

**MiniBatch/FAISS tá»‘t hÆ¡n Spark náº¿u**:
- âœ… Single machine Ä‘á»§ máº¡nh (RAM â‰¥ 16GB)
- âœ… Æ¯u tiÃªn tá»‘c Ä‘á»™ tá»‘i Ä‘a
- âœ… KhÃ´ng cáº§n distributed
- âœ… Setup Ä‘Æ¡n giáº£n
