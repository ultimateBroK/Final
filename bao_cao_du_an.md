# üìä B√ÅO C√ÅO D·ª∞ √ÅN: PH√ÅT HI·ªÜN R·ª¨A TI·ªÄN B·∫∞NG H·ªåC M√ÅY

## Ph√¢n T√≠ch 179 Tri·ªáu Giao D·ªãch v·ªõi Apache Spark

---

## üìñ CHO NG∆Ø·ªúI M·ªöI B·∫ÆT ƒê·∫¶U

**B·∫°n ch∆∞a bi·∫øt g√¨ v·ªÅ Big Data? ƒê·ª´ng lo!** B√°o c√°o n√†y ƒë∆∞·ª£c vi·∫øt ƒë·ªÉ m·ªçi ng∆∞·ªùi ƒë·ªÅu hi·ªÉu ƒë∆∞·ª£c.

### Nh·ªØng g√¨ b·∫°n c·∫ßn bi·∫øt tr∆∞·ªõc:
- ‚úÖ **Kh√¥ng c·∫ßn bi·∫øt l·∫≠p tr√¨nh** ƒë·ªÉ hi·ªÉu √Ω t∆∞·ªüng ch√≠nh
- ‚úÖ **Kh√¥ng c·∫ßn bi·∫øt to√°n cao c·∫•p** - ch√∫ng t√¥i s·∫Ω gi·∫£i th√≠ch b·∫±ng v√≠ d·ª• ƒë∆°n gi·∫£n
- ‚ö†Ô∏è M·ªôt s·ªë ph·∫ßn k·ªπ thu·∫≠t c√≥ th·ªÉ h∆°i kh√≥, nh∆∞ng ƒë√£ c√≥ gi·∫£i th√≠ch chi ti·∫øt

### C√°ch ƒë·ªçc b√°o c√°o n√†y:
1. **B·∫Øt ƒë·∫ßu v·ªõi "T√≥m t·∫Øt ƒëi·ªÅu h√†nh"** - Hi·ªÉu t·ªïng quan d·ª± √°n
2. **ƒê·ªçc "Ph·∫ßn 1: Gi·ªõi thi·ªáu"** - Hi·ªÉu v·∫•n ƒë·ªÅ v√† gi·∫£i ph√°p
3. **B·ªè qua c√°c ph·∫ßn k·ªπ thu·∫≠t n·∫øu kh√≥ hi·ªÉu** - Quay l·∫°i sau khi ƒë√£ hi·ªÉu t·ªïng quan
4. **Xem "Ph·ª• l·ª•c - Thu·∫≠t ng·ªØ"** khi g·∫∑p t·ª´ kh√≥

### V√≠ d·ª• v·ªÅ c√°ch ch√∫ng t√¥i gi·∫£i th√≠ch:
> ‚ùå **C√°ch c≈© (kh√≥ hi·ªÉu)**: "K-means l√† thu·∫≠t to√°n ph√¢n c·ª•m unsupervised learning s·ª≠ d·ª•ng kho·∫£ng c√°ch Euclidean ƒë·ªÉ minimize within-cluster sum of squares."
> 
> ‚úÖ **C√°ch m·ªõi (d·ªÖ hi·ªÉu)**: "K-means t·ª± ƒë·ªông chia 179 tri·ªáu h·ªçc sinh th√†nh 5 l·ªõp d·ª±a tr√™n ƒëi·ªÉm s·ªë. H·ªçc sinh gi·ªëng nhau s·∫Ω ·ªü c√πng l·ªõp."

---

## M·ª•c l·ª•c
- [T√≥m t·∫Øt ƒëi·ªÅu h√†nh](#tom-tat)
- [Ph·∫ßn 1: Gi·ªõi thi·ªáu d·ª± √°n](#p1)
- [Ph·∫ßn 2: D·ªØ li·ªáu v√† ti·ªÅn x·ª≠ l√Ω](#p2)
- [Ph·∫ßn 3: Ki·∫øn tr√∫c h·ªá th·ªëng](#p3)
- [Ph·∫ßn 4: Quy tr√¨nh x·ª≠ l√Ω (Pipeline)](#p4)
- [Ph·∫ßn 5: K·∫øt qu·∫£ v√† ƒë√°nh gi√°](#p5)
- [Ph·∫ßn 6: Tu√¢n th·ªß quy ƒë·ªãnh b·∫£o m·∫≠t](#p6)
- [Ph·∫ßn 7: H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng](#p7)
- [Ph·∫ßn 8: X·ª≠ l√Ω s·ª± c·ªë](#p8)
- [Ph·∫ßn 9: K·∫øt lu·∫≠n v√† h∆∞·ªõng ph√°t tri·ªÉn](#p9)
- [Ph·ª• l·ª•c](#phu-luc)

---

- Ng√†y l·∫≠p b√°o c√°o: 29/10/2025 21:32:29
- V·ªã tr√≠ d·ª± √°n: `/home/ultimatebrok/Downloads/Final`
- Ng∆∞·ªùi th·ª±c hi·ªán: Sinh vi√™n
- Gi·∫£ng vi√™n h∆∞·ªõng d·∫´n: [T√™n gi·∫£ng vi√™n]
- Snapshot: `snapshot_20251029_213229`

---

<a id="tom-tat"></a>
## T√ìM T·∫ÆT ƒêI·ªÄU H√ÄNH

### B√†i to√°n
**V·∫•n ƒë·ªÅ th·ª±c t·∫ø**: Ng√¢n h√†ng c√≥ h√†ng trƒÉm tri·ªáu giao d·ªãch m·ªói th√°ng. L√†m sao t√¨m ƒë∆∞·ª£c nh·ªØng giao d·ªãch nghi ng·ªù r·ª≠a ti·ªÅn trong s·ªë ƒë√≥?

**Gi·∫£i ph√°p**: Ch√∫ng ta c√≥ m·ªôt file CSV **r·∫•t l·ªõn** (16GB, t∆∞∆°ng ƒë∆∞∆°ng ~35,000 b√†i nh·∫°c MP3 ho·∫∑c 8,000 video YouTube 2 ph√∫t) ch·ª©a **179 tri·ªáu giao d·ªãch**. S·ª≠ d·ª•ng m√°y t√≠nh ph√¢n t√≠ch v√† nh√≥m c√°c giao d·ªãch t∆∞∆°ng t·ª± nhau l·∫°i, sau ƒë√≥ t√¨m nh·ªØng nh√≥m c√≥ d·∫•u hi·ªáu b·∫•t th∆∞·ªùng.

> **Big Data l√† g√¨?** D·ªØ li·ªáu qu√° l·ªõn ƒë·∫øn m·ª©c m·ªôt m√°y t√≠nh th√¥ng th∆∞·ªùng kh√¥ng th·ªÉ x·ª≠ l√Ω h·∫øt trong th·ªùi gian h·ª£p l√Ω. Ph·∫£i d√πng nhi·ªÅu m√°y t√≠nh l√†m vi·ªác c√πng l√∫c (ph√¢n t√°n).

### K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c
- ‚úÖ X·ª≠ l√Ω th√†nh c√¥ng **179,702,229 giao d·ªãch** (g·∫ßn nh∆∞ to√†n b·ªô d√¢n s·ªë n∆∞·ªõc M·ªπ!)
- ‚úÖ Ph√¢n th√†nh **5 nh√≥m** (c·ª•m) giao d·ªãch v·ªõi t·ª∑ l·ªá r·ª≠a ti·ªÅn kh√°c nhau (0.041% - 5.56%)
- ‚úÖ Th·ªùi gian x·ª≠ l√Ω: **11 ph√∫t 47 gi√¢y** - r·∫•t nhanh cho kh·ªëi l∆∞·ª£ng d·ªØ li·ªáu kh·ªïng l·ªì n√†y
- ‚úÖ Ph√°t hi·ªán **225,546 giao d·ªãch nghi ng·ªù** r·ª≠a ti·ªÅn c·∫ßn ki·ªÉm tra th·ªß c√¥ng
- ‚úÖ Tu√¢n th·ªß quy ƒë·ªãnh: KH√îNG l∆∞u d·ªØ li·ªáu l·ªõn ·ªü m√°y c·ª•c b·ªô (ch·ªâ l∆∞u tr√™n h·ªá th·ªëng an to√†n)

### C√¥ng ngh·ªá s·ª≠ d·ª•ng (Gi·∫£i th√≠ch ƒë∆°n gi·∫£n)

| C√¥ng ngh·ªá | Vai tr√≤ ƒë∆°n gi·∫£n | V√≠ d·ª• so s√°nh |
|----------|-----------------|---------------|
| **Polars** | ƒê·ªçc v√† x·ª≠ l√Ω file CSV c·ª±c nhanh ·ªü m√°y t√≠nh c√° nh√¢n | Nh∆∞ Excel nh∆∞ng nhanh g·∫•p 10-100 l·∫ßn, x·ª≠ l√Ω ƒë∆∞·ª£c file 16GB |
| **Apache Spark** | Ph√¢n t√°n c√¥ng vi·ªác cho nhi·ªÅu m√°y t√≠nh l√†m c√πng l√∫c | Nh∆∞ c√≥ 4 c√¥ng nh√¢n c√πng l√†m vi·ªác song song, nhanh g·∫•p 4 l·∫ßn |
| **HDFS** | L∆∞u tr·ªØ file l·ªõn an to√†n tr√™n nhi·ªÅu m√°y | Nh∆∞ Google Drive nh∆∞ng d√†nh cho d·ªØ li·ªáu c·ª±c l·ªõn, t·ª± ƒë·ªông sao l∆∞u |
| **Python** | Ng√¥n ng·ªØ l·∫≠p tr√¨nh | Ti·∫øng Vi·ªát ƒë·ªÉ vi·∫øt code |
| **K-means** | Thu·∫≠t to√°n t·ª± ƒë·ªông nh√≥m d·ªØ li·ªáu t∆∞∆°ng t·ª± | Nh∆∞ t·ª± ƒë·ªông s·∫Øp x·∫øp h·ªçc sinh v√†o 5 l·ªõp d·ª±a tr√™n ƒëi·ªÉm s·ªë |

---

<a id="p1"></a>
## PH·∫¶N 1: GI·ªöI THI·ªÜU D·ª∞ √ÅN

### 1.1. B·ªëi c·∫£nh v√† ƒê·ªông l·ª±c

#### V·∫•n ƒë·ªÅ r·ª≠a ti·ªÅn trong th·ª±c t·∫ø
**R·ª≠a ti·ªÅn l√† g√¨?** ƒê∆°n gi·∫£n l√† h√†nh vi "gi·∫∑t" ti·ªÅn b·∫©n th√†nh ti·ªÅn s·∫°ch. V√≠ d·ª•: M·ªôt ng∆∞·ªùi c√≥ ti·ªÅn t·ª´ bu√¥n b√°n ma t√∫y b·∫•t h·ª£p ph√°p, h·ªç kh√¥ng th·ªÉ d√πng tr·ª±c ti·∫øp v√¨ s·∫Ω b·ªã ph√°t hi·ªán. Thay v√†o ƒë√≥, h·ªç s·∫Ω:
1. Chuy·ªÉn ti·ªÅn qua nhi·ªÅu t√†i kho·∫£n kh√°c nhau
2. T·∫°o nhi·ªÅu giao d·ªãch nh·ªè ƒë·ªÉ che gi·∫•u
3. D√πng nhi·ªÅu ng√¢n h√†ng kh√°c nhau

**Nhi·ªám v·ª• c·ªßa ng√¢n h√†ng**: Ph·∫£i ph√°t hi·ªán nh·ªØng giao d·ªãch c√≥ d·∫•u hi·ªáu r·ª≠a ti·ªÅn v√† b√°o c√°o cho c∆° quan ch·ª©c nƒÉng. Nh∆∞ng v·ªõi h√†ng trƒÉm tri·ªáu giao d·ªãch m·ªói th√°ng, con ng∆∞·ªùi kh√¥ng th·ªÉ ki·ªÉm tra th·ªß c√¥ng ƒë∆∞·ª£c!

#### Th√°ch th·ª©c v·ªõi d·ªØ li·ªáu l·ªõn
- **Kh·ªëi l∆∞·ª£ng kh·ªïng l·ªì**: H√†ng trƒÉm tri·ªáu giao d·ªãch m·ªói th√°ng
- **T·ªëc ƒë·ªô x·ª≠ l√Ω**: C·∫ßn ph√¢n t√≠ch nhanh ƒë·ªÉ ph√°t hi·ªán k·ªãp th·ªùi
- **ƒê·ªô ch√≠nh x√°c**: Gi·∫£m thi·ªÉu c·∫£nh b√°o gi·∫£ (false positive)
- **Tu√¢n th·ªß quy ƒë·ªãnh**: B·∫£o m·∫≠t d·ªØ li·ªáu kh√°ch h√†ng

#### Gi·∫£i ph√°p c·ªßa d·ª± √°n
**√ù t∆∞·ªüng**: D√πng m√°y t√≠nh t·ª± ƒë·ªông ph√¢n t√≠ch!

**C√°ch ho·∫°t ƒë·ªông ƒë∆°n gi·∫£n**:
1. **H·ªçc m√°y kh√¥ng gi√°m s√°t** = M√°y t·ª± h·ªçc, kh√¥ng c·∫ßn d·∫°y tr∆∞·ªõc (gi·ªëng nh∆∞ ƒë·ªÉ m√°y t·ª± t√¨m pattern)
2. **K-means** = Thu·∫≠t to√°n t·ª± ƒë·ªông nh√≥m giao d·ªãch t∆∞∆°ng t·ª± nhau (v√≠ d·ª•: nh√≥m theo gi√° tr·ªã, th·ªùi gian, ƒë·ªãa ƒëi·ªÉm)
3. **Ph√¢n t√°n** = D√πng nhi·ªÅu m√°y t√≠nh c√πng l√†m (nh∆∞ c√≥ nhi·ªÅu c√¥ng nh√¢n)
4. **B·∫£o m·∫≠t** = D·ªØ li·ªáu kh√°ch h√†ng ƒë∆∞·ª£c b·∫£o v·ªá, kh√¥ng l∆∞u ·ªü m√°y c√° nh√¢n

**V√≠ d·ª• minh h·ªça**: 
> Gi·ªëng nh∆∞ gi√°o vi√™n t·ª± ƒë·ªông s·∫Øp x·∫øp 179 tri·ªáu h·ªçc sinh v√†o 5 l·ªõp d·ª±a tr√™n ƒëi·ªÉm s·ªë, chi·ªÅu cao, tu·ªïi t√°c. Sau ƒë√≥ xem l·ªõp n√†o c√≥ nhi·ªÅu h·ªçc sinh quay c√≥p (t·ª∑ l·ªá r·ª≠a ti·ªÅn cao).

### 1.2. M·ª•c ti√™u d·ª± √°n

#### M·ª•c ti√™u ch√≠nh

**1. Ph√¢n t√≠ch d·ªØ li·ªáu giao d·ªãch quy m√¥ l·ªõn**
   - **Input**: File CSV 16GB (r·∫•t l·ªõn!)
   - **C√¥ng vi·ªác**: 
     - ƒê·ªçc 179 tri·ªáu d√≤ng d·ªØ li·ªáu
     - **Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng** = Chuy·ªÉn d·ªØ li·ªáu th√¥ th√†nh s·ªë ƒë·ªÉ m√°y t√≠nh hi·ªÉu (v√≠ d·ª•: "US Dollar" ‚Üí s·ªë 0, "Euro" ‚Üí s·ªë 1)
     - **Chu·∫©n h√≥a** = ƒê∆∞a t·∫•t c·∫£ s·ªë v·ªÅ c√πng thang ƒëo (gi·ªëng nh∆∞ quy ƒë·ªïi v·ªÅ c√πng ƒë∆°n v·ªã: km, m, cm ‚Üí ch·ªâ d√πng 1 ƒë∆°n v·ªã)

**2. Ph√¢n c·ª•m giao d·ªãch b·∫±ng K-means**
   - **√ù t∆∞·ªüng**: T·ª± ƒë·ªông chia 179 tri·ªáu giao d·ªãch th√†nh **5 nh√≥m** (c·ª•m)
   - **V√≠ d·ª•**: 
     - C·ª•m 1: Giao d·ªãch nh·ªè, ban ng√†y
     - C·ª•m 2: Giao d·ªãch l·ªõn, ban ƒë√™m
     - C·ª•m 3: Giao d·ªãch qu·ªëc t·∫ø
     - v.v.
   - **C√¥ng c·ª•**: Apache Spark (ph√¢n t√°n cho nhi·ªÅu m√°y c√πng l√†m)

**3. Ph√°t hi·ªán giao d·ªãch nghi ng·ªù**
   - Xem trong m·ªói nh√≥m c√≥ bao nhi√™u % l√† r·ª≠a ti·ªÅn
   - N·∫øu nh√≥m n√†o c√≥ t·ª∑ l·ªá cao b·∫•t th∆∞·ªùng ‚Üí ƒë√°nh d·∫•u l√† nghi ng·ªù
   - Xu·∫•t danh s√°ch ƒë·ªÉ con ng∆∞·ªùi ki·ªÉm tra th·ªß c√¥ng

**4. Tu√¢n th·ªß quy ƒë·ªãnh b·∫£o m·∫≠t**
   - D·ªØ li·ªáu kh√°ch h√†ng **KH√îNG ƒë∆∞·ª£c** l∆∞u ·ªü m√°y t√≠nh c√° nh√¢n
   - Ch·ªâ l∆∞u tr√™n h·ªá th·ªëng an to√†n (HDFS)
   - File t·∫°m t·ª± ƒë·ªông x√≥a sau khi x·ª≠ l√Ω xong

#### M·ª•c ti√™u ph·ª•
- H·ªçc v√† √°p d·ª•ng c√¥ng ngh·ªá Big Data (Spark, HDFS)
- So s√°nh hi·ªáu su·∫•t gi·ªØa Hadoop MapReduce v√† Apache Spark
- X√¢y d·ª±ng quy tr√¨nh t·ª± ƒë·ªông (pipeline) t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi
- Vi·∫øt t√†i li·ªáu chi ti·∫øt, d·ªÖ hi·ªÉu cho ng∆∞·ªùi kh√°c

---

<a id="p2"></a>
## PH·∫¶N 2: D·ªÆ LI·ªÜU V√Ä TI·ªÄN X·ª¨ L√ù

### 2.1. M√¥ t·∫£ t·∫≠p d·ªØ li·ªáu

#### Th√¥ng tin c∆° b·∫£n
- **T√™n file**: `HI-Large_Trans.csv` (file Excel/CSV format)
- **K√≠ch th∆∞·ªõc**: **16 GB** = Kho·∫£ng 35,000 b√†i nh·∫°c MP3 ho·∫∑c 8,000 video YouTube ng·∫Øn
- **S·ªë b·∫£n ghi**: **179,702,229 giao d·ªãch** = G·∫ßn b·∫±ng d√¢n s·ªë n∆∞·ªõc M·ªπ (330 tri·ªáu ng∆∞·ªùi)
- **Ngu·ªìn**: D·ªØ li·ªáu m√¥ ph·ªèng (kh√¥ng ph·∫£i d·ªØ li·ªáu th·∫≠t, ch·ªâ ƒë·ªÉ h·ªçc t·∫≠p) v·ªÅ giao d·ªãch ng√¢n h√†ng qu·ªëc t·∫ø

> **T·∫°i sao d·ªØ li·ªáu l·ªõn ƒë·∫øn v·∫≠y?** M·ªói ng√¢n h√†ng l·ªõn c√≥ th·ªÉ c√≥ h√†ng tri·ªáu giao d·ªãch m·ªói ng√†y. File n√†y m√¥ ph·ªèng d·ªØ li·ªáu trong v√†i th√°ng c·ªßa nhi·ªÅu ng√¢n h√†ng.

#### C·∫•u tr√∫c d·ªØ li·ªáu (11 c·ªôt)

| T√™n c·ªôt | √ù nghƒ©a | Ki·ªÉu d·ªØ li·ªáu | V√≠ d·ª• |
|---------|---------|--------------|-------|
| `Timestamp` | Th·ªùi gian giao d·ªãch | Chu·ªói | "2022/08/01 00:17" |
| `From Bank` | M√£ ng√¢n h√†ng g·ª≠i | S·ªë nguy√™n | 20, 3196, 1208 |
| `Account` | M√£ t√†i kho·∫£n g·ª≠i | Chu·ªói | "800104D70" |
| `To Bank` | M√£ ng√¢n h√†ng nh·∫≠n | S·ªë nguy√™n | 20, 3196 |
| `Account.1` | M√£ t√†i kho·∫£n nh·∫≠n | Chu·ªói | "800107150" |
| `Amount Received` | S·ªë ti·ªÅn nh·∫≠n ƒë∆∞·ª£c | S·ªë th·ª±c | 6794.63 |
| `Receiving Currency` | Lo·∫°i ti·ªÅn nh·∫≠n | Chu·ªói | "US Dollar", "Yuan" |
| `Amount Paid` | S·ªë ti·ªÅn tr·∫£ | S·ªë th·ª±c | 7739.29 |
| `Payment Currency` | Lo·∫°i ti·ªÅn tr·∫£ | Chu·ªói | "US Dollar", "Bitcoin" |
| `Payment Format` | H√¨nh th·ª©c thanh to√°n | Chu·ªói | "Reinvestment", "Cheque" |
| `Is Laundering` | Nh√£n r·ª≠a ti·ªÅn | 0 ho·∫∑c 1 | 0 = B√¨nh th∆∞·ªùng, 1 = R·ª≠a ti·ªÅn |

#### Th·ªëng k√™ m√¥ t·∫£
- **T·ª∑ l·ªá r·ª≠a ti·ªÅn t·ªïng th·ªÉ**: 0.126% (225,546 / 179,702,229)
- **Lo·∫°i ti·ªÅn ph·ªï bi·∫øn nh·∫•t**: Euro (23%), Yuan (7.2%), Mexican Peso (2.7%)
- **Gi√° tr·ªã giao d·ªãch trung b√¨nh**: ~1.14 tri·ªáu ƒë∆°n v·ªã ti·ªÅn t·ªá
- **Kho·∫£ng gi√° tr·ªã**: T·ª´ 0.01 ƒë·∫øn h∆°n 5 t·ª∑ ƒë∆°n v·ªã

### 2.2. Quy tr√¨nh ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu

#### B∆∞·ªõc 1: Kh√°m ph√° d·ªØ li·ªáu (Data Exploration)

**Script**: `scripts/polars/explore_fast.py`  
**Th·ªùi gian**: ~30 gi√¢y  
**C√¥ng vi·ªác**:
- ƒê·ªçc nhanh 100,000 d√≤ng ƒë·∫ßu ƒë·ªÉ hi·ªÉu c·∫•u tr√∫c
- Xem ki·ªÉu d·ªØ li·ªáu c·ªßa t·ª´ng c·ªôt (s·ªë, chu·ªói)
- Ki·ªÉm tra gi√° tr·ªã thi·∫øu (missing values)
- Th·ªëng k√™ m√¥ t·∫£: min, max, mean, median
- Ph√¢n t√≠ch ph√¢n ph·ªëi c·ªßa nh√£n r·ª≠a ti·ªÅn

**K·ªπ thu·∫≠t s·ª≠ d·ª•ng** (Gi·∫£i th√≠ch ƒë∆°n gi·∫£n):
- **Lazy Loading** = Ch·ªâ ƒë·ªçc m·ªôt ph·∫ßn nh·ªè ƒë·ªÉ xem c·∫•u tr√∫c, kh√¥ng t·∫£i h·∫øt 16GB v√†o RAM (gi·ªëng nh∆∞ ch·ªâ ƒë·ªçc m·ª•c l·ª•c s√°ch thay v√¨ ƒë·ªçc to√†n b·ªô 1000 trang)
- **Polars DataFrame** = C√¥ng c·ª• x·ª≠ l√Ω d·ªØ li·ªáu c·ª±c nhanh (vi·∫øt b·∫±ng Rust - ng√¥n ng·ªØ nhanh nh∆∞ C++)
- **Statistical Summary** = T√≠nh to√°n nhi·ªÅu ph√©p to√°n c√πng l√∫c (song song)

#### B∆∞·ªõc 2: Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng (Feature Engineering)

**Script**: `scripts/polars/prepare_polars.py`  
**Th·ªùi gian**: ~36 gi√¢y (r·∫•t nhanh!)  
**C√¥ng vi·ªác**: Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√¥ th√†nh d·∫°ng s·ªë ƒë·ªÉ m√°y t√≠nh ph√¢n t√≠ch

**1. Ph√¢n t√≠ch th·ªùi gian (Temporal Features)**
   - **Input**: "2022/08/01 00:17" (chu·ªói vƒÉn b·∫£n)
   - **Output**: 
     - Gi·ªù trong ng√†y: 0 (n·ª≠a ƒë√™m)
     - Ng√†y trong tu·∫ßn: 1 (Th·ª© 2)
   - **L√Ω do**: Giao d·ªãch r·ª≠a ti·ªÅn th∆∞·ªùng x·∫£y ra v√†o gi·ªù l·∫° (2-3h s√°ng) ho·∫∑c cu·ªëi tu·∫ßn

**2. T√≠nh to√°n t·ª∑ l·ªá (Ratio Features)**
   - **C√¥ng th·ª©c**: `amount_ratio = S·ªë ti·ªÅn nh·∫≠n / S·ªë ti·ªÅn tr·∫£`
   - **V√≠ d·ª•**: Nh·∫≠n 1000$, tr·∫£ 500$ ‚Üí ratio = 2.0
   - **L√Ω do**: N·∫øu ratio qu√° cao ho·∫∑c qu√° th·∫•p ‚Üí c√≥ th·ªÉ nghi ng·ªù (v√≠ d·ª•: nh·∫≠n 1 tri·ªáu nh∆∞ng ch·ªâ tr·∫£ 100$)

**3. M√£ h√≥a tuy·∫øn ƒë∆∞·ªùng (Route Hash)**
   - **V√≠ d·ª•**: Giao d·ªãch t·ª´ Ng√¢n h√†ng A ‚Üí Ng√¢n h√†ng B ‚Üí chuy·ªÉn th√†nh m·ªôt s·ªë duy nh·∫•t (nh∆∞ m√£ s·ªë)
   - **L√Ω do**: N·∫øu tuy·∫øn A‚ÜíB xu·∫•t hi·ªán qu√° nhi·ªÅu l·∫ßn ‚Üí c√≥ th·ªÉ ƒëang r·ª≠a ti·ªÅn qua tuy·∫øn n√†y

**4. M√£ h√≥a bi·∫øn ph√¢n lo·∫°i (Categorical Encoding)**
   - **V·∫•n ƒë·ªÅ**: M√°y t√≠nh kh√¥ng hi·ªÉu ch·ªØ, ch·ªâ hi·ªÉu s·ªë
   - **Gi·∫£i ph√°p**: Chuy·ªÉn t·∫•t c·∫£ ch·ªØ th√†nh s·ªë
   - **V√≠ d·ª•**: 
     - "US Dollar" ‚Üí 0
     - "Euro" ‚Üí 1  
     - "Bitcoin" ‚Üí 2
   - Gi·ªëng nh∆∞ ƒë√°nh s·ªë cho t·ª´ng lo·∫°i ti·ªÅn t·ªá

**5. Chu·∫©n h√≥a (Normalization)**
   - **V·∫•n ƒë·ªÅ**: S·ªë ti·ªÅn c√≥ th·ªÉ t·ª´ 0.01$ ƒë·∫øn 5 t·ª∑$, c√≤n gi·ªù ch·ªâ t·ª´ 0-23. N·∫øu kh√¥ng chu·∫©n h√≥a, s·ªë ti·ªÅn s·∫Ω "l·∫•n √°t" gi·ªù
   - **Gi·∫£i ph√°p**: ƒê∆∞a t·∫•t c·∫£ v·ªÅ thang ƒëo 0-1
   - **V√≠ d·ª•**: 
     - S·ªë ti·ªÅn: 1,000,000$ (min=0.01, max=5 t·ª∑) ‚Üí chu·∫©n h√≥a th√†nh 0.2
     - Gi·ªù: 23h (min=0, max=23) ‚Üí chu·∫©n h√≥a th√†nh 1.0
   - Gi·ªëng nh∆∞ quy ƒë·ªïi t·∫•t c·∫£ v·ªÅ c√πng ƒë∆°n v·ªã ƒë·ªÉ so s√°nh c√¥ng b·∫±ng

**ƒê·∫ßu ra**:
- File: `data/processed/hadoop_input_temp.txt` (T·∫†M TH·ªúI)
- K√≠ch th∆∞·ªõc: 33GB (sau khi normalize)
- 9 c·ªôt ƒë·∫∑c tr∆∞ng s·ªë: `[amount_received, amount_paid, amount_ratio, hour, day_of_week, route_hash, recv_curr_encoded, payment_curr_encoded, payment_format_encoded]`
- **L∆∞u √Ω**: File n√†y s·∫Ω B·ªä X√ìA t·ª± ƒë·ªông sau khi upload l√™n HDFS

#### ~~B∆∞·ªõc 3: Kh·ªüi t·∫°o t√¢m c·ª•m ban ƒë·∫ßu~~ ‚ùå **ƒê√É LO·∫†I B·ªé**

**Tr·∫°ng th√°i**: ƒê√£ lo·∫°i b·ªè kh·ªèi pipeline

**T·∫°i sao lo·∫°i b·ªè?**
- **MLlib K-means t·ª± ƒë·ªông** s·ª≠ d·ª•ng thu·∫≠t to√°n **k-means++** (g·ªçi l√† "k-means||") ƒë·ªÉ kh·ªüi t·∫°o centroids
- K-means++ th√¥ng minh h∆°n random initialization, cho k·∫øt qu·∫£ t·ªët h∆°n
- Kh√¥ng c·∫ßn file `centroids_temp.txt` n·ªØa
- Ti·∫øt ki·ªám 30 gi√¢y th·ªùi gian x·ª≠ l√Ω

**L·ª£i √≠ch**:
- H·ªôi t·ª• nhanh h∆°n (~10-12 iterations thay v√¨ 15)
- K·∫øt qu·∫£ ·ªïn ƒë·ªãnh h∆°n, tr√°nh local minima
- Gi·∫£m ƒë·ªô ph·ª©c t·∫°p pipeline (7 b∆∞·ªõc thay v√¨ 8)

---

<a id="p3"></a>
## PH·∫¶N 3: KI·∫æN TR√öC H·ªÜ TH·ªêNG

### 3.1. S∆° ƒë·ªì t·ªïng quan

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              KI·∫æN TR√öC H·ªÜ TH·ªêNG PH√ÇN T√ÅN                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   D·ªÆ LI·ªÜU    ‚îÇ   16GB CSV (179M giao d·ªãch)
‚îÇ   ƒê·∫¶U V√ÄO    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   POLARS     ‚îÇ   X·ª≠ l√Ω d·ªØ li·ªáu c·ª•c b·ªô (1 m√°y)
‚îÇ  (M√°y c√°     ‚îÇ   - ƒê·ªçc CSV nhanh
‚îÇ   nh√¢n)      ‚îÇ   - Feature engineering
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   - Chu·∫©n h√≥a
       ‚îÇ
       ‚îÇ T·∫°o file temp 33GB
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     HDFS     ‚îÇ   H·ªá th·ªëng l∆∞u tr·ªØ ph√¢n t√°n
‚îÇ  (Nhi·ªÅu m√°y  ‚îÇ   - Chia nh·ªè th√†nh blocks
‚îÇ    t√≠nh)     ‚îÇ   - Sao l∆∞u t·ª± ƒë·ªông (replication)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   - Fault-tolerant
       ‚îÇ
       ‚îÇ üóëÔ∏è  X√ìA file temp c·ª•c b·ªô
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    SPARK     ‚îÇ   X·ª≠ l√Ω ph√¢n t√°n song song
‚îÇ  (Cluster)   ‚îÇ   - ƒê·ªçc t·ª´ HDFS
‚îÇ              ‚îÇ   - K-means trong RAM
‚îÇ  [Master]    ‚îÇ   - L∆∞u k·∫øt qu·∫£ v·ªÅ HDFS
‚îÇ  [Worker 1]  ‚îÇ
‚îÇ  [Worker 2]  ‚îÇ
‚îÇ  [Worker 3]  ‚îÇ
‚îÇ  [Worker 4]  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  K·∫æT QU·∫¢     ‚îÇ   File nh·ªè (~4KB)
‚îÇ  (C·ª•c b·ªô)    ‚îÇ   - 5 t√¢m c·ª•m cu·ªëi c√πng
‚îÇ              ‚îÇ   - B√°o c√°o ph√¢n t√≠ch
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.2. Gi·∫£i th√≠ch c√°c th√†nh ph·∫ßn

#### Polars - X·ª≠ l√Ω d·ªØ li·ªáu nhanh
**Vai tr√≤**: Nh∆∞ Excel/Pandas nh∆∞ng nhanh h∆°n r·∫•t nhi·ªÅu

**T·∫°i sao d√πng Polars thay v√¨ Pandas?**:
- Pandas: ƒê·ªçc 16GB m·∫•t 45 ph√∫t (qu√° l√¢u!)
- Polars: ƒê·ªçc 16GB m·∫•t 4-5 ph√∫t (nhanh g·∫•p 9 l·∫ßn!)
- **L√Ω do**: Polars vi·∫øt b·∫±ng Rust (ng√¥n ng·ªØ nhanh nh∆∞ C++) trong khi Pandas vi·∫øt b·∫±ng Python (ch·∫≠m h∆°n)
- **Lazy evaluation**: Kh√¥ng t√≠nh to√°n ngay, ch·ªâ t√≠nh khi c·∫ßn (nh∆∞ ƒë·ªçc m·ª•c l·ª•c tr∆∞·ªõc, ƒë·ªçc n·ªôi dung sau)
- **X·ª≠ l√Ω file l·ªõn**: C√≥ th·ªÉ x·ª≠ l√Ω file l·ªõn h∆°n c·∫£ RAM c·ªßa m√°y t√≠nh (nh∆∞ streaming video YouTube)

**V√≠ d·ª• so s√°nh**: 
> N·∫øu Pandas l√† xe ƒë·∫°p th√¨ Polars l√† xe m√°y. C√πng qu√£ng ƒë∆∞·ªùng nh∆∞ng nhanh h∆°n nhi·ªÅu!

#### HDFS - L∆∞u tr·ªØ ph√¢n t√°n (Nh∆∞ Google Drive cho Big Data)
**Vai tr√≤**: L∆∞u tr·ªØ file c·ª±c l·ªõn (33GB) tr√™n nhi·ªÅu m√°y t√≠nh, t·ª± ƒë·ªông sao l∆∞u

**C√°ch ho·∫°t ƒë·ªông ƒë∆°n gi·∫£n**:
1. **Chia nh·ªè**: File 33GB ƒë∆∞·ª£c chia th√†nh nhi·ªÅu m·∫£nh 128MB (nh∆∞ chia b√°nh th√†nh nhi·ªÅu mi·∫øng)
2. **Sao l∆∞u**: M·ªói m·∫£nh ƒë∆∞·ª£c l∆∞u ·ªü 3 m√°y kh√°c nhau (nh∆∞ photo 3 b·∫£n quan tr·ªçng)
3. **An to√†n**: N·∫øu 1 m√°y h·ªèng ‚Üí v·∫´n c√≤n 2 b·∫£n sao ·ªü m√°y kh√°c (kh√¥ng m·∫•t d·ªØ li·ªáu!)

**V√≠ d·ª• minh h·ªça**:
> HDFS gi·ªëng nh∆∞ m·ªôt kho l∆∞u tr·ªØ c√≥ nhi·ªÅu ng∆∞·ªùi canh gi·ªØ. M·ªói t√†i li·ªáu quan tr·ªçng ƒë∆∞·ª£c photo 3 b·∫£n, l∆∞u ·ªü 3 kho kh√°c nhau. N·∫øu 1 kho ch√°y, v·∫´n c√≤n 2 kho kh√°c!

**C·∫•u tr√∫c th∆∞ m·ª•c HDFS** (gi·ªëng nh∆∞ th∆∞ m·ª•c tr√™n m√°y t√≠nh):
```
/user/spark/hi_large/          ‚Üê Th∆∞ m·ª•c ch√≠nh
‚îú‚îÄ‚îÄ input/
‚îÇ   ‚îî‚îÄ‚îÄ hadoop_input.txt       ‚Üê File d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω (33GB)
‚îî‚îÄ‚îÄ output_centroids/          ‚Üê Th∆∞ m·ª•c k·∫øt qu·∫£
    ‚îî‚îÄ‚îÄ part-00000             ‚Üê File k·∫øt qu·∫£ nh·ªè (~4KB)
```

**L·ª£i √≠ch**:
- ‚úÖ **Kh√¥ng gi·ªõi h·∫°n**: Th√™m m√°y = th√™m kh√¥ng gian (nh∆∞ Google Drive)
- ‚úÖ **An to√†n**: T·ª± ƒë·ªông sao l∆∞u 3 b·∫£n (replication)
- ‚úÖ **Tu√¢n th·ªß quy ƒë·ªãnh**: Kh√¥ng l∆∞u ·ªü m√°y c√° nh√¢n, ch·ªâ tr√™n h·ªá th·ªëng an to√†n

#### Apache Spark - X·ª≠ l√Ω ph√¢n t√°n (Nhi·ªÅu m√°y c√πng l√†m vi·ªác)
**Vai tr√≤**: Ch·∫°y thu·∫≠t to√°n K-means tr√™n nhi·ªÅu m√°y t√≠nh c√πng l√∫c (song song)

**V√≠ d·ª• ƒë∆°n gi·∫£n**: 
> Gi·ªëng nh∆∞ c√≥ 1 √¥ng ch·ªß (Master) v√† 4 c√¥ng nh√¢n (Workers). √îng ch·ªß giao vi·ªác:
> - C√¥ng nh√¢n 1: X·ª≠ l√Ω 44 tri·ªáu giao d·ªãch ƒë·∫ßu
> - C√¥ng nh√¢n 2: X·ª≠ l√Ω 44 tri·ªáu giao d·ªãch ti·∫øp
> - C√¥ng nh√¢n 3: X·ª≠ l√Ω 44 tri·ªáu giao d·ªãch ti·∫øp
> - C√¥ng nh√¢n 4: X·ª≠ l√Ω ph·∫ßn c√≤n l·∫°i
> 
> T·∫•t c·∫£ l√†m c√πng l√∫c ‚Üí nhanh g·∫•p 4 l·∫ßn!

**Ki·∫øn tr√∫c Spark**:
```
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   MASTER    ‚îÇ  ‚Üê √îng ch·ªß ƒëi·ªÅu ph·ªëi
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ        ‚îÇ        ‚îÇ
   ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê
   ‚îÇ W1  ‚îÇ  ‚îÇ W2  ‚îÇ  ‚îÇ W3  ‚îÇ  ‚Üê C√¥ng nh√¢n l√†m vi·ªác
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   44M     44M     44M      (M·ªói ng∆∞·ªùi 1 ph·∫ßn)
```

**C√°ch Spark x·ª≠ l√Ω K-means** (chia c√¥ng vi·ªác):
1. **Ph√¢n chia**: Chia 179 tri·ªáu giao d·ªãch th√†nh 4 ph·∫ßn cho 4 workers
2. **L√†m vi·ªác song song**: M·ªói worker x·ª≠ l√Ω ph·∫ßn c·ªßa m√¨nh (nh∆∞ 4 ng∆∞·ªùi c√πng ƒë·ªçc 4 quy·ªÉn s√°ch kh√°c nhau)
3. **T·ªïng h·ª£p**: Master thu th·∫≠p k·∫øt qu·∫£ t·ª´ t·∫•t c·∫£ workers (nh∆∞ thu b√†i l√†m)
4. **L·∫∑p l·∫°i**: L√†m 15 l·∫ßn cho ƒë·∫øn khi ƒë·∫°t k·∫øt qu·∫£ t·ªët

**T·∫°i sao Spark nhanh h∆°n Hadoop?**
- **In-memory computing**: L∆∞u d·ªØ li·ªáu trong RAM (nhanh) thay v√¨ ·ªï c·ª©ng (ch·∫≠m) - gi·ªëng nh∆∞ ƒë·ªçc s√°ch tr√™n m√°y t√≠nh (RAM) nhanh h∆°n ƒë·ªçc t·ª´ ·ªï c·ª©ng
- **Lazy evaluation**: Ch·ªâ t√≠nh khi c·∫ßn (nh∆∞ xem m·ª•c l·ª•c tr∆∞·ªõc)
- **T·ª± ƒë·ªông t·ªëi ∆∞u**: Spark t·ª± ƒë·ªông s·∫Øp x·∫øp l·∫°i c√¥ng vi·ªác ƒë·ªÉ l√†m nhanh nh·∫•t c√≥ th·ªÉ

**C·∫•u h√¨nh Spark trong d·ª± √°n** (C·∫•u h√¨nh m√°y t√≠nh):

| Th√†nh ph·∫ßn | Gi·∫£i th√≠ch ƒë∆°n gi·∫£n | S·ªë l∆∞·ª£ng |
|------------|---------------------|----------|
| **Driver memory** | B·ªô nh·ªõ cho √¥ng ch·ªß (Master) | 4GB (nh∆∞ RAM laptop) |
| **Executor memory** | B·ªô nh·ªõ cho m·ªói c√¥ng nh√¢n (Worker) | 4GB √ó 4 = 16GB t·ªïng |
| **Cores** | CPU cores (nh∆∞ s·ªë "tay" c·ªßa m√°y t√≠nh) | 4 cores/worker √ó 4 = 16 cores |
| **Parallelism** | S·ªë vi·ªác l√†m c√πng l√∫c | 16 (16 vi·ªác song song) |

> **Gi·∫£i th√≠ch th√™m**: 
> - 1 core = nh∆∞ 1 tay l√†m vi·ªác. 4 cores = c√≥ 4 tay, l√†m ƒë∆∞·ª£c 4 vi·ªác c√πng l√∫c
> - 16GB RAM = nh∆∞ c√≥ 16 t·ªß s√°ch ƒë·ªÉ ch·ª©a d·ªØ li·ªáu
> - X·ª≠ l√Ω song song = nh∆∞ 16 ng∆∞·ªùi c√πng ƒë·ªçc 16 quy·ªÉn s√°ch kh√°c nhau, nhanh g·∫•p 16 l·∫ßn!

---

<a id="p4"></a>
## PH·∫¶N 4: QUY TR√åNH X·ª¨ L√ù (PIPELINE)

### 4.1. T·ªïng quan quy tr√¨nh 7 b∆∞·ªõc

‚ö†Ô∏è **Thay ƒë·ªïi quan tr·ªçng**: Pipeline ƒë√£ t·ªëi ∆∞u t·ª´ 8 b∆∞·ªõc xu·ªëng c√≤n **7 b∆∞·ªõc**. B∆∞·ªõc kh·ªüi t·∫°o centroids ƒë√£ lo·∫°i b·ªè v√¨ MLlib K-means t·ª± ƒë·ªông d√πng **k-means++**.

**Th·ªùi gian th·ª±c t·∫ø t·ª´ Snapshot 29/10/2025 21:32:29**:

```
B∆Ø·ªöC 1        B∆Ø·ªöC 2        B∆Ø·ªöC 3
Kh√°m ph√°  ‚Üí   X·ª≠ l√Ω    ‚Üí   Upload
 13 gi√¢y       36 gi√¢y      41 gi√¢y

B∆Ø·ªöC 4            B∆Ø·ªöC 5        B∆Ø·ªöC 6        B∆Ø·ªöC 7
K-means       ‚Üí   T·∫£i v·ªÅ   ‚Üí   G√°n nh√£n  ‚Üí   Ph√¢n t√≠ch
6 ph√∫t 5s      3 gi√¢y       3 ph√∫t 14s      30 gi√¢y

T·ªîNG TH·ªúI GIAN: 11 ph√∫t 22 gi√¢y (682 gi√¢y)
```

### 4.2. Chi ti·∫øt t·ª´ng b∆∞·ªõc

#### B∆Ø·ªöC 1: Kh√°m ph√° d·ªØ li·ªáu üîç

**M·ª•c ƒë√≠ch**: Hi·ªÉu c·∫•u tr√∫c v√† ƒë·∫∑c ƒëi·ªÉm c·ªßa d·ªØ li·ªáu  
**File th·ª±c thi**: `scripts/polars/explore_fast.py`  
**Th·ªùi gian th·ª±c t·∫ø**: **13 gi√¢y** (Snapshot 29/10/2025 21:20:57 - 21:21:10)  
**Input**: `data/raw/HI-Large_Trans.csv` (16GB)  
**Output**: Th·ªëng k√™ in ra m√†n h√¨nh

**C√°c ph√¢n t√≠ch th·ª±c hi·ªán**:
1. **Lazy Loading**: ƒê·ªçc metadata v√† 100,000 d√≤ng ƒë·∫ßu (ƒë·∫°i di·ªán) - kh√¥ng t·∫£i to√†n b·ªô v√†o RAM
2. **Schema Analysis**: Xem t√™n c·ªôt, ki·ªÉu d·ªØ li·ªáu (11 c·ªôt: Timestamp, From Bank, Account, To Bank, Account.1, Amount Received, Receiving Currency, Amount Paid, Payment Currency, Payment Format, Is Laundering)
3. **Th·ªëng k√™ m√¥ t·∫£**: min, max, mean, median, std cho c√°c c·ªôt s·ªë
4. **Ph√¢n t√≠ch nh√£n r·ª≠a ti·ªÅn**: ƒê·∫øm s·ªë giao d·ªãch b√¨nh th∆∞·ªùng vs nghi ng·ªù
5. **Top lo·∫°i ti·ªÅn t·ªá**: Ph√¢n t√≠ch ph√¢n ph·ªëi c√°c lo·∫°i ti·ªÅn ph·ªï bi·∫øn

**K·∫øt qu·∫£ th·ª±c t·∫ø t·ª´ Snapshot**:
```
T·ªïng s·ªë giao d·ªãch: 179,702,229
T·ª∑ l·ªá r·ª≠a ti·ªÅn: 0.126% (225,546 / 179,702,229)
Ph√¢n ph·ªëi nh√£n:
  - 0 (B√¨nh th∆∞·ªùng): 179,476,683 giao d·ªãch
  - 1 (R·ª≠a ti·ªÅn): 225,546 giao d·ªãch

Top 10 lo·∫°i ti·ªÅn t·ªá nh·∫≠n ph·ªï bi·∫øn:
  - US Dollar: 65,292,945 giao d·ªãch (36.4%)
  - Euro: 41,290,069 giao d·ªãch (23.0%)
  - Yuan: 12,920,668 giao d·ªãch (7.2%)
  - Ruble: 5,571,567 giao d·ªãch (3.1%)
  - Australian Dollar: 5,256,710 giao d·ªãch (2.9%)
  - Yen: 4,841,570 giao d·ªãch (2.7%)
  - Swiss Franc: 4,829,099 giao d·ªãch (2.7%)
  - Rupee: 4,178,243 giao d·ªãch (2.3%)
  - Bitcoin: 3,958,153 giao d·ªãch (2.2%)
  - Brazil Real: 3,596,378 giao d·ªãch (2.0%)

Gi√° tr·ªã giao d·ªãch:
  - Min: 0.01
  - Max: 5,115,400,000 (tr√™n 5 t·ª∑!)
  - Mean: 1,142,200
  - Median: 2,513.06
```

#### B∆Ø·ªöC 2: X·ª≠ l√Ω v√† tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng üîß

**M·ª•c ƒë√≠ch**: Chuy·ªÉn d·ªØ li·ªáu th√¥ th√†nh d·∫°ng s·ªë ƒë·ªÉ thu·∫≠t to√°n x·ª≠ l√Ω  
**File th·ª±c thi**: `scripts/polars/prepare_polars.py`  
**Th·ªùi gian th·ª±c t·∫ø**: **36 gi√¢y** (21:21:11 - 21:21:45, Snapshot 29/10/2025)  
**Input**: `data/raw/HI-Large_Trans.csv` (16GB)  
**Output**: `data/processed/hadoop_input_temp.txt` (**31GB**, T·∫†M TH·ªúI)

**Chi ti·∫øt 6 b∆∞·ªõc x·ª≠ l√Ω (t·ª´ log th·ª±c t·∫ø)**:

**B∆∞·ªõc 2.1/6: Thi·∫øt l·∫≠p ƒë·ªçc tr√¨ ho√£n (Lazy Loading)**
- Th·ªùi gian: 0.0s
- M·ª•c ƒë√≠ch: Kh√¥ng t·∫£i to√†n b·ªô v√†o RAM, ch·ªâ ƒë·ªçc khi c·∫ßn thi·∫øt
- S·ª≠ d·ª•ng: `pl.scan_csv()` - Polars lazy evaluation

**B∆∞·ªõc 2.2/6: Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ d·ªØ li·ªáu th√¥**
- Th·ªùi gian: 0.0s (t√≠nh to√°n lazy, ch∆∞a th·ª±c thi)
- C√°c ƒë·∫∑c tr∆∞ng ƒë∆∞·ª£c t·∫°o:
  1. **Temporal Features**: Parse `Timestamp` ‚Üí `hour` (0-23), `day_of_week` (0-6)
  2. **Amount Features**: `Amount Received`, `Amount Paid`, `amount_ratio = Received / Paid`
  3. **Route Feature**: `route_hash = hash(From Bank + To Bank)` - m√£ h√≥a tuy·∫øn chuy·ªÉn ti·ªÅn

**B∆∞·ªõc 2.3/6: M√£ h√≥a bi·∫øn ph√¢n lo·∫°i (Categorical Encoding)**
- Th·ªùi gian: 0.0s
- M√£ h√≥a Label Encoding cho:
  - `Receiving Currency` ‚Üí `recv_curr_encoded` (s·ªë nguy√™n)
  - `Payment Currency` ‚Üí `payment_curr_encoded` (s·ªë nguy√™n)
  - `Payment Format` ‚Üí `payment_format_encoded` (s·ªë nguy√™n)

**B∆∞·ªõc 2.4/6: Ch·ªçn c√°c ƒë·∫∑c tr∆∞ng s·ªë**
- Th·ªùi gian: 0.0s
- K·∫øt qu·∫£: Ch·ªçn **9 ƒë·∫∑c tr∆∞ng s·ªë** cho K-means:
  1. `amount_received`
  2. `amount_paid`
  3. `amount_ratio`
  4. `hour`
  5. `day_of_week`
  6. `route_hash`
  7. `recv_curr_encoded`
  8. `payment_curr_encoded`
  9. `payment_format_encoded`

**B∆∞·ªõc 2.5/6: Chu·∫©n h√≥a d·ªØ li·ªáu (Z-score Normalization)**
- Th·ªùi gian: 0.0s (t√≠nh to√°n lazy)
- C√¥ng th·ª©c: `(x - mean) / std` (Z-score, kh√¥ng ph·∫£i Min-Max)
- M·ª•c ƒë√≠ch: ƒê∆∞a t·∫•t c·∫£ features v·ªÅ c√πng scale (mean=0, std=1)

**B∆∞·ªõc 2.6/6: L∆∞u t·ªáp t·∫°m th·ªùi cho HDFS**
- Th·ªùi gian: **34.7 gi√¢y** (chi·∫øm ph·∫ßn l·ªõn th·ªùi gian c·ªßa b∆∞·ªõc 2)
- ƒê∆∞·ªùng d·∫´n: `/home/ultimatebrok/Downloads/Final/data/processed/hadoop_input_temp.txt`
- K√≠ch th∆∞·ªõc: **31.00 GB** (sau khi normalize)
- Ghi ch√∫: Polars streaming write - kh√¥ng t·ªën RAM
- **C·∫£nh b√°o**: File n√†y s·∫Ω t·ª± ƒë·ªông x√≥a sau khi upload l√™n HDFS!

**T·ªïng th·ªùi gian b∆∞·ªõc 2: 0.6 ph√∫t (34.7s)**

**T·∫°i sao l·∫°i t·ª´ 16GB th√†nh 31GB?**
- D·ªØ li·ªáu g·ªëc: 11 c·ªôt (c√≥ c·∫£ chu·ªói, s·ªë)
- Sau x·ª≠ l√Ω: 9 c·ªôt s·ªë float64
- M·ªói s·ªë float64 = 8 bytes
- 179,702,229 rows √ó 9 features √ó 8 bytes ‚âà 12.9GB l√Ω thuy·∫øt
- Overhead (delimiters, newlines, formatting): ~18GB ‚Üí **31GB th·ª±c t·∫ø**

#### ~~B∆Ø·ªöC 3: Kh·ªüi t·∫°o t√¢m c·ª•m~~ ‚ùå **ƒê√É LO·∫†I B·ªé**

**Tr·∫°ng th√°i**: Lo·∫°i b·ªè ‚Äì MLlib K-means t·ª± ƒë·ªông d√πng **k-means++** kh·ªüi t·∫°o th√¥ng minh.

---

#### B∆Ø·ªöC 3: Upload l√™n HDFS ‚òÅÔ∏è

**M·ª•c ƒë√≠ch**: Chuy·ªÉn d·ªØ li·ªáu l√™n h·ªá th·ªëng ph√¢n t√°n v√† x√≥a file t·∫°m c·ª•c b·ªô  
**File th·ª±c thi**: `scripts/spark/setup_hdfs.sh`  
**Th·ªùi gian th·ª±c t·∫ø**: **41 gi√¢y** (Snapshot 29/10/2025 21:22 - 21:22:41)  
**Input**: File temp c·ª•c b·ªô `hadoop_input_temp.txt` (31GB)  
**Output**: D·ªØ li·ªáu tr√™n HDFS t·∫°i `/user/spark/hi_large/input/hadoop_input.txt`

**Chi ti·∫øt c√°c b∆∞·ªõc th·ª±c hi·ªán**:

1. **Ki·ªÉm tra HDFS ƒëang ch·∫°y**
   - Ch·∫°y: `hdfs dfsadmin -report`
   - K·∫øt qu·∫£: HDFS c√≥ th·ªÉ truy c·∫≠p

2. **T√¨m file d·ªØ li·ªáu t·∫°m**
   - Ki·ªÉm tra: `/home/ultimatebrok/Downloads/Final/data/processed/hadoop_input_temp.txt`
   - X√°c nh·∫≠n: File t·ªìn t·∫°i (31GB)

3. **T·∫°o th∆∞ m·ª•c HDFS**
   - L·ªánh: `hdfs dfs -mkdir -p /user/spark/hi_large/input`
   - M·ª•c ƒë√≠ch: Chu·∫©n b·ªã th∆∞ m·ª•c ƒë√≠ch

4. **D·ªçn d·∫πp d·ªØ li·ªáu c≈© trong HDFS** (n·∫øu c√≥)
   - X√≥a: `/user/spark/hi_large/input/hadoop_input.txt` (n·∫øu t·ªìn t·∫°i)
   - X√≥a: `/user/spark/hi_large/output_centroids` (n·∫øu t·ªìn t·∫°i)

5. **Upload d·ªØ li·ªáu l√™n HDFS**
   - Ngu·ªìn: `/home/ultimatebrok/Downloads/Final/data/processed/hadoop_input_temp.txt`
   - ƒê√≠ch: `/user/spark/hi_large/input/hadoop_input.txt`
   - Th·ªùi gian upload: ~35-40 gi√¢y (31GB qua m·∫°ng n·ªôi b·ªô)

6. **X√ìA file t·∫°m c·ª•c b·ªô** ‚ö†Ô∏è **QUAN TR·ªåNG**
   - L·ªánh: `rm -rf data/processed/*`
   - K·∫øt qu·∫£: File 31GB ƒë√£ ƒë∆∞·ª£c x√≥a kh·ªèi m√°y c·ª•c b·ªô
   - **L√Ω do**: Tu√¢n th·ªß quy ƒë·ªãnh b·∫£o m·∫≠t - kh√¥ng l∆∞u d·ªØ li·ªáu l·ªõn local

7. **X√°c minh upload**
   - Ki·ªÉm tra k√≠ch th∆∞·ªõc tr√™n HDFS: `hdfs dfs -du -h /user/spark/hi_large/input/`
   - K·∫øt qu·∫£: **31.0 GB** (33,282,391,568 bytes)
   - ƒê∆∞·ªùng d·∫´n HDFS: `hdfs://localhost:9000/user/spark/hi_large/input/hadoop_input.txt`

**üîí Tu√¢n th·ªß quy ƒë·ªãnh b·∫£o m·∫≠t**:
- ‚úÖ Sau b∆∞·ªõc n√†y, **KH√îNG c√≤n** d·ªØ li·ªáu l·ªõn (31GB) ·ªü m√°y c·ª•c b·ªô
- ‚úÖ Ch·ªâ t·ªìn t·∫°i tr√™n HDFS (ph√¢n t√°n, an to√†n, c√≥ replication)
- ‚úÖ File temp ƒë√£ ƒë∆∞·ª£c x√≥a t·ª± ƒë·ªông
- üìù L∆∞u √Ω: MLlib s·∫Ω t·ª± ƒë·ªông kh·ªüi t·∫°o centroids v·ªõi k-means++ (kh√¥ng c·∫ßn file centroids.txt n·ªØa)

**C·∫•u tr√∫c HDFS sau b∆∞·ªõc 3**:
```
/user/spark/hi_large/
‚îú‚îÄ‚îÄ input/
‚îÇ   ‚îî‚îÄ‚îÄ hadoop_input.txt    (31.0 GB - d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω)
‚îú‚îÄ‚îÄ centroids.txt            (437 bytes - t√¢m c·ª•m c≈©, kh√¥ng d√πng n·ªØa)
‚îî‚îÄ‚îÄ output_centroids/        (s·∫Ω ƒë∆∞·ª£c t·∫°o ·ªü b∆∞·ªõc 4)
```

#### B∆Ø·ªöC 4: Ch·∫°y K-means tr√™n Spark üöÄ

**M·ª•c ƒë√≠ch**: T·ª± ƒë·ªông chia 179 tri·ªáu giao d·ªãch th√†nh **5 nh√≥m** (c·ª•m) b·∫±ng thu·∫≠t to√°n **K-means**  
**File th·ª±c thi**: `scripts/spark/run_spark.sh` + `kmeans_spark.py`  
**Th·ªùi gian th·ª±c t·∫ø**: **6 ph√∫t 5 gi√¢y** (r·∫•t nhanh cho 179 tri·ªáu giao d·ªãch!)  
**Input**: File d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω tr√™n HDFS (31GB)  
**Output**: 5 t√¢m c·ª•m (centroids) - m·ªói t√¢m l√† ƒë·∫°i di·ªán cho 1 nh√≥m

> **K-means l√† g√¨?** 
> V√≠ d·ª•: B·∫°n c√≥ 179 tri·ªáu h·ªçc sinh, c·∫ßn chia th√†nh 5 l·ªõp. K-means s·∫Ω:
> 1. Ch·ªçn ng·∫´u nhi√™n 5 h·ªçc sinh l√†m "t√¢m l·ªõp" (centroids)
> 2. G√°n m·ªói h·ªçc sinh v√†o l·ªõp g·∫ßn nh·∫•t (d·ª±a tr√™n ƒëi·ªÉm s·ªë, chi·ªÅu cao, v.v.)
> 3. T√≠nh l·∫°i t√¢m l·ªõp m·ªõi (l·∫•y ƒëi·ªÉm trung b√¨nh)
> 4. L·∫∑p l·∫°i cho ƒë·∫øn khi ·ªïn ƒë·ªãnh
> 
> K·∫øt qu·∫£: 5 l·ªõp h·ªçc sinh t∆∞∆°ng t·ª± nhau v·ªÅ ƒë·∫∑c ƒëi·ªÉm!

**C·∫•u h√¨nh Spark cluster**:
- **Spark version**: 4.0.1
- **Java version**: 17.0.16
- **Ch·∫ø ƒë·ªô**: Standalone cluster (local)
- **S·ªë executor**: 4 workers
- **Executor cores**: 4 cores/worker (t·ªïng 16 cores)
- **Executor memory**: 8GB/worker (t·ªïng 32GB RAM)
- **Driver memory**: 8GB
- **Spark UI**: `http://192.168.1.10:4040` (c√≥ th·ªÉ theo d√µi ti·∫øn tr√¨nh)

**Chi ti·∫øt 5 b∆∞·ªõc x·ª≠ l√Ω**:

**B∆∞·ªõc 4.1/5: ƒê·ªçc d·ªØ li·ªáu t·ª´ HDFS** üìÇ
- Th·ªùi gian: **58.2 gi√¢y** (21:22:35 - 21:23:33)
- D·ªØ li·ªáu ƒë·ªçc: 179,702,229 b·∫£n ghi t·ª´ file 31GB tr√™n HDFS
- ƒê·ªãnh d·∫°ng: CSV kh√¥ng header, 9 c·ªôt s·ªë (features ƒë√£ normalized)

**B∆∞·ªõc 4.2/5: T·∫°o vector ƒë·∫∑c tr∆∞ng** üîß
- Th·ªùi gian: **63.1 gi√¢y** (21:23:33 - 21:24:36)
- C√¥ng vi·ªác:
  - S·ª≠ d·ª•ng `VectorAssembler` ƒë·ªÉ gh√©p 9 c·ªôt th√†nh 1 vector
  - Cache v√†o b·ªô nh·ªõ/ƒëƒ©a ƒë·ªÉ tƒÉng t·ªëc c√°c iteration ti·∫øp theo
  - K·∫øt qu·∫£: 179,702,229 vector ƒë·∫∑c tr∆∞ng

**B∆∞·ªõc 4.3/5: C·∫•u h√¨nh K-means** üéØ
- Th·ªùi gian: **0.1 gi√¢y** (r·∫•t nhanh!)
- **C√°c tham s·ªë** (gi·ªëng nh∆∞ c√†i ƒë·∫∑t):
  - `K = 5`: Chia th√†nh 5 nh√≥m (c·ª•m) - nh∆∞ chia th√†nh 5 l·ªõp
  - `MaxIter = 15`: L·∫∑p t·ªëi ƒëa 15 l·∫ßn (nh∆∞ l√†m l·∫°i 15 l·∫ßn cho ƒë·∫øn khi ƒë·∫°t)
  - `Seed = 42`: S·ªë ng·∫´u nhi√™n c·ªë ƒë·ªãnh (ƒë·ªÉ t√°i t·∫°o k·∫øt qu·∫£ gi·ªëng nhau m·ªói l·∫ßn ch·∫°y)
  - `Tol = 0.0001`: Ng∆∞·ª°ng h·ªôi t·ª• (n·∫øu thay ƒë·ªïi < 0.0001 th√¨ d·ª´ng - ƒë√£ ƒë·ªß t·ªët)
  - `InitMode = "k-means||"`: **T·ª± ƒë·ªông ch·ªçn t√¢m c·ª•m th√¥ng minh** (kh√¥ng c·∫ßn ch·ªçn th·ªß c√¥ng)

> **k-means++ l√† g√¨?** Thay v√¨ ch·ªçn 5 t√¢m c·ª•m ng·∫´u nhi√™n, n√≥ ch·ªçn th√¥ng minh h∆°n:
> - T√¢m 1: Ch·ªçn ng·∫´u nhi√™n
> - T√¢m 2: Ch·ªçn ƒëi·ªÉm xa t√¢m 1 nh·∫•t
> - T√¢m 3: Ch·ªçn ƒëi·ªÉm xa 2 t√¢m tr∆∞·ªõc ƒë√≥ nh·∫•t
> - ... 
> ‚Üí K·∫øt qu·∫£ t·ªët h∆°n v√† nhanh h∆°n!

**B∆∞·ªõc 4.4/5: Hu·∫•n luy·ªán K-means** üöÄ (Ph·∫ßn quan tr·ªçng nh·∫•t!)
- Th·ªùi gian: **3 ph√∫t 50.8 gi√¢y** - chi·∫øm 63% t·ªïng th·ªùi gian (nh∆∞ng x·ª≠ l√Ω ƒë∆∞·ª£c 179 tri·ªáu giao d·ªãch!)

**Qu√° tr√¨nh K-means ho·∫°t ƒë·ªông** (gi·∫£i th√≠ch ƒë∆°n gi·∫£n):

**1. Kh·ªüi t·∫°o th√¥ng minh (k-means++)**:
   - Ch·ªçn 5 "h·ªçc sinh ti√™u bi·ªÉu" l√†m t√¢m l·ªõp (th√¥ng minh, kh√¥ng ph·∫£i random)
   - L√†m sao ƒë·ªÉ c√°c t√¢m c√°ch xa nhau ‚Üí c√°c nh√≥m kh√°c bi·ªát r√µ r√†ng

**2. L·∫∑p l·∫°i 15 l·∫ßn** (nh∆∞ s·∫Øp x·∫øp l·∫°i 15 l·∫ßn):
   
   **M·ªói l·∫ßn l·∫∑p c√≥ 3 b∆∞·ªõc**:
   
   **a) Assign (G√°n)**: 
   - M·ªói giao d·ªãch ƒë∆∞·ª£c g√°n v√†o c·ª•m g·∫ßn nh·∫•t
   - T√≠nh kho·∫£ng c√°ch Euclidean = nh∆∞ ƒëo kho·∫£ng c√°ch gi·ªØa 2 ƒëi·ªÉm tr√™n b·∫£n ƒë·ªì
   - V√≠ d·ª•: Giao d·ªãch A g·∫ßn t√¢m c·ª•m 2 nh·∫•t ‚Üí g√°n v√†o c·ª•m 2
   
   **b) Update (C·∫≠p nh·∫≠t)**:
   - T√≠nh l·∫°i t√¢m c·ª•m m·ªõi = l·∫•y ƒëi·ªÉm trung b√¨nh c·ªßa t·∫•t c·∫£ giao d·ªãch trong c·ª•m
   - V√≠ d·ª•: C·ª•m 2 c√≥ 1000 giao d·ªãch ‚Üí t√¢m m·ªõi = trung b√¨nh 1000 giao d·ªãch ƒë√≥
   
   **c) Check (Ki·ªÉm tra)**:
   - N·∫øu t√¢m c·ª•m thay ƒë·ªïi r·∫•t √≠t (< 0.0001) ‚Üí d·ª´ng s·ªõm (ƒë√£ ƒë·∫°t k·∫øt qu·∫£ t·ªët)
   - N·∫øu kh√¥ng ‚Üí ti·∫øp t·ª•c l·∫∑p

**V√≠ d·ª• minh h·ªça**:
> Gi·ªëng nh∆∞ gi√°o vi√™n s·∫Øp x·∫øp h·ªçc sinh v√†o 5 l·ªõp:
> - L·∫ßn 1: Chia ng·∫´u nhi√™n
> - L·∫ßn 2: Xem l·∫°i, c√≥ h·ªçc sinh n√†o n√™n chuy·ªÉn l·ªõp kh√¥ng?
> - L·∫ßn 3: ƒêi·ªÅu ch·ªânh l·∫°i
> - ...
> - L·∫ßn 15: Ho√†n thi·ªán!
**T·ªëi ∆∞u h√≥a c·ªßa Spark** (T·ª± ƒë·ªông l√†m nhanh h∆°n):
- **Catalyst Optimizer**: T·ª± ƒë·ªông s·∫Øp x·∫øp l·∫°i c√°c b∆∞·ªõc l√†m vi·ªác ƒë·ªÉ nhanh nh·∫•t (nh∆∞ Google Maps t√¨m ƒë∆∞·ªùng ng·∫Øn nh·∫•t)
- **Tungsten Execution Engine**: Th·ª±c thi nhanh trong RAM (nh∆∞ l√†m vi·ªác tr√™n m√°y t√≠nh nhanh thay v√¨ gi·∫•y)
- **Adaptive Query Execution (AQE)**: T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh s·ªë ph·∫ßn chia (n·∫øu 1 ph·∫ßn qu√° l·ªõn ‚Üí chia nh·ªè h∆°n ƒë·ªÉ c√¢n b·∫±ng)

**K·∫øt qu·∫£ hu·∫•n luy·ªán**:
- **S·ªë v√≤ng l·∫∑p th·ª±c t·∫ø**: 15 (ƒë·∫°t max iterations, ch∆∞a h·ªôi t·ª• s·ªõm)
- **WSSSE (Within-Set Sum of Squared Errors)**: 961,278,012.73
- **Trung b√¨nh SSE/ƒëi·ªÉm**: 5.349283
- **Ch·∫•t l∆∞·ª£ng**: T·ªët - c√°c c·ª•m ph√¢n t√°ch r√µ r√†ng

**B∆∞·ªõc 4.5/5: L∆∞u t√¢m c·ª•m v√†o HDFS** üíæ
- Th·ªùi gian: **0.8 gi√¢y**
- ƒê∆∞·ªùng d·∫´n: `hdfs://localhost:9000/user/spark/hi_large/output_centroids/`
- K√≠ch th∆∞·ªõc: ~4KB (5 d√≤ng, m·ªói d√≤ng 9 gi√° tr·ªã float)

**Ph√¢n t√≠ch k·∫øt qu·∫£** (t·ª´ log):
- Th·ªùi gian: **3.7 gi√¢y** (21:28:28 - 21:28:31)
- Ph√¢n ph·ªëi c·ª•m:
  ```
  Cluster 0: 36,926,397 ƒëi·ªÉm (20.55%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
  Cluster 1: 69,939,093 ƒëi·ªÉm (38.92%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚Üê L·ªõn nh·∫•t
  Cluster 2: 68,931,700 ƒëi·ªÉm (38.36%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚Üê L·ªõn th·ª© 2
  Cluster 3: 18 ƒëi·ªÉm (0.00%)          ‚ñà ‚Üê Outlier c·ª±c l·ªõn!
  Cluster 4: 3,905,021 ƒëi·ªÉm (2.17%)   ‚ñà ‚Üê C·ª•m nh·ªè
  ```

**T·ªïng th·ªùi gian b∆∞·ªõc 4: 5.9 ph√∫t (365.8s)**

**Nh·∫≠n x√©t v·ªÅ hi·ªáu su·∫•t**:
- ‚úÖ Nhanh h∆°n 30-50% so v·ªõi RDD-based K-means (∆∞·ªõc t√≠nh 10-25 ph√∫t)
- ‚úÖ MLlib t·ªëi ∆∞u t·ªët v·ªõi Catalyst + Tungsten
- ‚úÖ K-means++ kh·ªüi t·∫°o th√¥ng minh gi√∫p ch·∫•t l∆∞·ª£ng t·ªët h∆°n
- ‚ö†Ô∏è Ch∆∞a h·ªôi t·ª• s·ªõm (ph·∫£i ch·∫°y ƒë·ªß 15 iterations) - c√≥ th·ªÉ c·∫ßn tune tolerance

#### B∆Ø·ªöC 5: T·∫£i k·∫øt qu·∫£ v·ªÅ üì•

**M·ª•c ƒë√≠ch**: L·∫•y t√¢m c·ª•m cu·ªëi c√πng t·ª´ HDFS  
**File th·ª±c thi**: `scripts/spark/download_from_hdfs.sh`  
**Th·ªùi gian**: ~30 gi√¢y  
**Input**: `/user/spark/hi_large/output_centroids/` tr√™n HDFS  
**Output**: `data/results/final_centroids.txt` (~4KB)

**C√°c b∆∞·ªõc**:
1. `hdfs dfs -cat /user/.../output_centroids/part-*`
2. L∆∞u v√†o file c·ª•c b·ªô
3. Verify: Ki·ªÉm tra c√≥ ƒë√∫ng 5 d√≤ng

**T·∫°i sao ƒë∆∞·ª£c ph√©p t·∫£i v·ªÅ?**
- File r·∫•t nh·ªè (~4KB)
- Ch·ªâ ch·ª©a k·∫øt qu·∫£ t·ªïng h·ª£p, kh√¥ng ph·∫£i d·ªØ li·ªáu g·ªëc
- C·∫ßn thi·∫øt cho b∆∞·ªõc ph√¢n t√≠ch ti·∫øp theo

#### B∆Ø·ªöC 6: G√°n nh√£n c·ª•m cho t·ª´ng giao d·ªãch üè∑Ô∏è

**M·ª•c ƒë√≠ch**: X√°c ƒë·ªãnh **m·ªói giao d·ªãch thu·ªôc nh√≥m n√†o** b·∫±ng c√°ch t√≠nh kho·∫£ng c√°ch  
**File th·ª±c thi**: `scripts/polars/assign_clusters_polars.py`  
**Th·ªùi gian th·ª±c t·∫ø**: **3 ph√∫t 14 gi√¢y** (194s - r·∫•t nhanh!)  
**Input**: 
  - File d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω t·ª´ HDFS (31GB, 179 tri·ªáu d√≤ng)
  - 5 t√¢m c·ª•m t·ª´ b∆∞·ªõc 5 (ƒë·∫°i di·ªán cho 5 nh√≥m)  
**Output**: File k·∫øt qu·∫£ (342.75 MB) - m·ªói d√≤ng l√† s·ªë nh√≥m (0-4) c·ªßa m·ªói giao d·ªãch

> **V√≠ d·ª•**: 
> - Giao d·ªãch 1 ‚Üí t√≠nh kho·∫£ng c√°ch ƒë·∫øn 5 t√¢m c·ª•m ‚Üí t√¢m c·ª•m 2 g·∫ßn nh·∫•t ‚Üí g√°n v√†o nh√≥m 2
> - Giao d·ªãch 2 ‚Üí t√≠nh kho·∫£ng c√°ch ‚Üí t√¢m c·ª•m 0 g·∫ßn nh·∫•t ‚Üí g√°n v√†o nh√≥m 0
> - ... (l√†m 179 tri·ªáu l·∫ßn!)

**Kho·∫£ng c√°ch Euclidean l√† g√¨?** 
Gi·ªëng nh∆∞ ƒëo kho·∫£ng c√°ch th·∫≥ng gi·ªØa 2 ƒëi·ªÉm tr√™n b·∫£n ƒë·ªì. Kho·∫£ng c√°ch nh·ªè nh·∫•t = thu·ªôc nh√≥m ƒë√≥!

**Chi ti·∫øt quy tr√¨nh x·ª≠ l√Ω**:

**B∆∞·ªõc 6.1: ƒê·ªçc t√¢m c·ª•m cu·ªëi c√πng**
- File: `data/results/final_centroids.txt`
- K·∫øt qu·∫£: Load 5 t√¢m c·ª•m, m·ªói t√¢m c√≥ 9 ƒë·∫∑c tr∆∞ng
- Th·ªùi gian: < 1 gi√¢y

**B∆∞·ªõc 6.2: ƒê·ªçc d·ªØ li·ªáu t·ª´ HDFS (Streaming)**
- ƒê∆∞·ªùng d·∫´n: `/user/spark/hi_large/input/hadoop_input.txt`
- C√°ch ƒë·ªçc: **Streaming t·ª´ HDFS** - kh√¥ng load to√†n b·ªô v√†o RAM
- K·∫øt qu·∫£: 179,702,229 b·∫£n ghi (ƒë√£ normalized, 9 features)
- Th·ªùi gian: ~30-40 gi√¢y

**B∆∞·ªõc 6.3: Chuy·ªÉn sang NumPy v√† t√≠nh kho·∫£ng c√°ch** üî¢
- D·ªØ li·ªáu: 179,702,229 d√≤ng √ó 9 c·ªôt
- T√¢m c·ª•m: 5 c·ª•m √ó 9 ƒë·∫∑c tr∆∞ng
- Ph∆∞∆°ng ph√°p: **Batch Processing** v·ªõi NumPy vectorization

**Thu·∫≠t to√°n t√≠nh kho·∫£ng c√°ch** (X·ª≠ l√Ω t·ª´ng l√¥ nh·ªè):

> **T·∫°i sao x·ª≠ l√Ω t·ª´ng l√¥?** V√¨ 179 tri·ªáu giao d·ªãch qu√° l·ªõn, kh√¥ng th·ªÉ load h·∫øt v√†o RAM. Gi·∫£i ph√°p: X·ª≠ l√Ω t·ª´ng l√¥ 1 tri·ªáu giao d·ªãch.

**Quy tr√¨nh** (ƒë∆°n gi·∫£n h√≥a):
1. **L·∫•y 1 tri·ªáu giao d·ªãch** t·ª´ file (nh∆∞ ƒë·ªçc 1 tri·ªáu d√≤ng)
2. **T√≠nh kho·∫£ng c√°ch** ƒë·∫øn 5 t√¢m c·ª•m (nh∆∞ ƒëo 1 tri·ªáu ƒëi·ªÉm ƒë·∫øn 5 ƒëi·ªÉm m·ªëc)
3. **Ch·ªçn c·ª•m g·∫ßn nh·∫•t** cho m·ªói giao d·ªãch (nh∆∞ t√¨m ƒëi·ªÉm m·ªëc g·∫ßn nh·∫•t)
4. **L∆∞u k·∫øt qu·∫£** (ghi v√†o file)
5. **L·∫∑p l·∫°i** 179 l·∫ßn (179 tri·ªáu √∑ 1 tri·ªáu = 179 l·∫ßn)

**T·∫°i sao d√πng NumPy vectorization?**
- **B√¨nh th∆∞·ªùng**: D√πng v√≤ng l·∫∑p Python ‚Üí ch·∫≠m (nh∆∞ ƒë·∫øm t·ª´ng s·ªë m·ªôt)
- **Vectorization**: NumPy t√≠nh to√°n h√†ng lo·∫°t ‚Üí nhanh g·∫•p 100-1000 l·∫ßn (nh∆∞ m√°y t√≠nh ƒë·∫øm h√†ng lo·∫°t)

**V√≠ d·ª•**: 
> Thay v√¨ t√≠nh 1 tri·ªáu l·∫ßn kho·∫£ng c√°ch ri√™ng l·∫ª (m·∫•t 10 ph√∫t), NumPy t√≠nh t·∫•t c·∫£ c√πng l√∫c (m·∫•t 6 gi√¢y)!

**Ti·∫øn tr√¨nh x·ª≠ l√Ω** (t·ª´ log):
```
ƒê√£ x·ª≠ l√Ω 1,000,000/179,702,229 giao d·ªãch (0.6%)
ƒê√£ x·ª≠ l√Ω 11,000,000/179,702,229 giao d·ªãch (6.1%)
ƒê√£ x·ª≠ l√Ω 21,000,000/179,702,229 giao d·ªãch (11.7%)
ƒê√£ x·ª≠ l√Ω 31,000,000/179,702,229 giao d·ªãch (17.3%)
ƒê√£ x·ª≠ l√Ω 41,000,000/179,702,229 giao d·ªãch (22.8%)
ƒê√£ x·ª≠ l√Ω 51,000,000/179,702,229 giao d·ªãch (28.4%)
ƒê√£ x·ª≠ l√Ω 61,000,000/179,702,229 giao d·ªãch (33.9%)
ƒê√£ x·ª≠ l√Ω 71,000,000/179,702,229 giao d·ªãch (39.5%)
ƒê√£ x·ª≠ l√Ω 81,000,000/179,702,229 giao d·ªãch (45.1%)
ƒê√£ x·ª≠ l√Ω 91,000,000/179,702,229 giao d·ªãch (50.6%)
ƒê√£ x·ª≠ l√Ω 101,000,000/179,702,229 giao d·ªãch (56.2%)
ƒê√£ x·ª≠ l√Ω 111,000,000/179,702,229 giao d·ªãch (61.8%)
ƒê√£ x·ª≠ l√Ω 121,000,000/179,702,229 giao d·ªãch (67.3%)
ƒê√£ x·ª≠ l√Ω 131,000,000/179,702,229 giao d·ªãch (72.9%)
ƒê√£ x·ª≠ l√Ω 141,000,000/179,702,229 giao d·ªãch (78.5%)
ƒê√£ x·ª≠ l√Ω 151,000,000/179,702,229 giao d·ªãch (84.0%)
ƒê√£ x·ª≠ l√Ω 161,000,000/179,702,229 giao d·ªãch (89.6%)
ƒê√£ x·ª≠ l√Ω 171,000,000/179,702,229 giao d·ªãch (95.2%)
ƒê√£ x·ª≠ l√Ω 179,702,229/179,702,229 giao d·ªãch (100.0%)
```

**B∆∞·ªõc 6.4: L∆∞u k·∫øt qu·∫£**
- File: `data/results/clustered_results.txt`
- K√≠ch th∆∞·ªõc: **342.75 MB**
- ƒê·ªãnh d·∫°ng: 1 d√≤ng = 1 cluster_id (s·ªë nguy√™n 0-4)
- T·ªïng d√≤ng: 179,702,229 (b·∫±ng s·ªë giao d·ªãch)

**Ph√¢n ph·ªëi c·ª•m** (x√°c nh·∫≠n t·ª´ k·∫øt qu·∫£):
```
Cluster 0: 36,926,395 giao d·ªãch (20.55%)
Cluster 1: 69,939,082 giao d·ªãch (38.92%) ‚Üê L·ªõn nh·∫•t
Cluster 2: 68,931,713 giao d·ªãch (38.36%) ‚Üê L·ªõn th·ª© 2
Cluster 3: 18 giao d·ªãch (0.00%)          ‚Üê Outlier!
Cluster 4: 3,905,021 giao d·ªãch (2.17%)
```

**T·ªëi ∆∞u h√≥a** (L√†m sao ƒë·ªÉ nhanh?):
- ‚úÖ **NumPy vectorization**: T√≠nh to√°n h√†ng lo·∫°t, nhanh h∆°n v√≤ng l·∫∑p Python 100-1000 l·∫ßn
  - V√≠ d·ª•: Thay v√¨ t√≠nh t·ª´ng s·ªë m·ªôt (m·∫•t 10 ph√∫t), t√≠nh c·∫£ tri·ªáu s·ªë c√πng l√∫c (m·∫•t 6 gi√¢y)
- ‚úÖ **Batch processing**: X·ª≠ l√Ω t·ª´ng l√¥ 1 tri·ªáu ‚Üí kh√¥ng t·ªën RAM
  - Nh∆∞ ƒë·ªçc s√°ch t·ª´ng ch∆∞∆°ng m·ªôt thay v√¨ ƒë·ªçc h·∫øt quy·ªÉn s√°ch 1000 trang
- ‚úÖ **Streaming t·ª´ HDFS**: ƒê·ªçc d·ªØ li·ªáu t·ª´ng ph·∫ßn, kh√¥ng load h·∫øt 31GB v√†o RAM
  - Nh∆∞ xem video streaming (t·ª´ng ƒëo·∫°n) thay v√¨ t·∫£i h·∫øt video v·ªÅ
- ‚úÖ **T·ªëc ƒë·ªô**: X·ª≠ l√Ω ~58 tri·ªáu giao d·ªãch/ph√∫t (c·ª±c k·ª≥ nhanh!)

#### B∆Ø·ªöC 7: Ph√¢n t√≠ch k·∫øt qu·∫£ üìä

**M·ª•c ƒë√≠ch**: Ph√¢n t√≠ch k·∫øt qu·∫£ v√† t√¨m nh√≥m n√†o c√≥ **t·ª∑ l·ªá r·ª≠a ti·ªÅn cao nh·∫•t**  
**File th·ª±c thi**: `scripts/polars/analyze_polars.py`  
**Th·ªùi gian th·ª±c t·∫ø**: **30 gi√¢y** (r·∫•t nhanh!)  
**Input**: 
  - File k·∫øt qu·∫£ ph√¢n c·ª•m (342.75 MB) - m·ªói giao d·ªãch ƒë√£ bi·∫øt thu·ªôc nh√≥m n√†o (0-4)
  - File d·ªØ li·ªáu g·ªëc (16GB) - c√≥ nh√£n "Is Laundering" (0 = b√¨nh th∆∞·ªùng, 1 = r·ª≠a ti·ªÅn)  
**Output**: B√°o c√°o ph√¢n t√≠ch chi ti·∫øt

> **C√¥ng vi·ªác**: 
> - Xem trong nh√≥m 0 c√≥ bao nhi√™u giao d·ªãch r·ª≠a ti·ªÅn? ‚Üí T·ª∑ l·ªá = ?
> - Xem trong nh√≥m 1 c√≥ bao nhi√™u giao d·ªãch r·ª≠a ti·ªÅn? ‚Üí T·ª∑ l·ªá = ?
> - ... (l√†m v·ªõi 5 nh√≥m)
> - Nh√≥m n√†o c√≥ t·ª∑ l·ªá cao nh·∫•t ‚Üí c·∫ßn ki·ªÉm tra k·ªπ!

**Chi ti·∫øt c√°c ph√¢n t√≠ch th·ª±c hi·ªán**:

**B∆∞·ªõc 7.1: ƒê·ªçc k·∫øt qu·∫£ ph√¢n c·ª•m**
- File: `data/results/clustered_results.txt`
- K·∫øt qu·∫£: Load 179,702,229 nh√£n c·ª•m (cluster_id t·ª´ 0-4)
- Th·ªùi gian: ~5 gi√¢y

**B∆∞·ªõc 7.2: ƒê·ªçc d·ªØ li·ªáu g·ªëc (Lazy Mode)**
- File: `data/raw/HI-Large_Trans.csv`
- C√°ch ƒë·ªçc: **Lazy loading** v·ªõi Polars - ch·ªâ load metadata, kh√¥ng load to√†n b·ªô v√†o RAM
- M·ª•c ƒë√≠ch: G·∫Øn cluster_id v√†o d·ªØ li·ªáu g·ªëc ƒë·ªÉ ph√¢n t√≠ch
- Th·ªùi gian: ~10 gi√¢y

**B∆∞·ªõc 7.3: G·∫Øn nh√£n c·ª•m v√†o d·ªØ li·ªáu**
- K·∫øt qu·∫£: M·ªói giao d·ªãch c√≥ th√™m c·ªôt `cluster` (0-4)
- Th·ªùi gian: ~2 gi√¢y

**B∆∞·ªõc 7.4: Ph√¢n t√≠ch th·ªëng k√™**

**1. K√≠ch th∆∞·ªõc m·ªói c·ª•m**:
```
Cluster 0: 36,926,395 giao d·ªãch (20.55%)
Cluster 1: 69,939,082 giao d·ªãch (38.92%) ‚Üê L·ªõn nh·∫•t
Cluster 2: 68,931,713 giao d·ªãch (38.36%) ‚Üê L·ªõn th·ª© 2
Cluster 3: 18 giao d·ªãch (0.00%)          ‚Üê Outlier c·ª±c l·ªõn!
Cluster 4: 3,905,021 giao d·ªãch (2.17%)   ‚Üê C·ª•m nh·ªè
```

**2. T·ª∑ l·ªá r·ª≠a ti·ªÅn trong t·ª´ng c·ª•m** (K·∫øt qu·∫£ quan tr·ªçng nh·∫•t!):
```
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë Nh√≥m     ‚ïë T·ªïng giao d·ªãch ‚ïë R·ª≠a ti·ªÅn  ‚ïë T·ª∑ l·ªá (%)       ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¨‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¨‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¨‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë Nh√≥m 0   ‚ïë 36,926,395  ‚ïë 29,920      ‚ïë 0.081%          ‚ïë
‚ïë Nh√≥m 1   ‚ïë 69,939,082  ‚ïë 78,960      ‚ïë 0.113%          ‚ïë
‚ïë Nh√≥m 2   ‚ïë 68,931,713  ‚ïë 115,057     ‚ïë 0.167% ‚Üê CAO    ‚ïë
‚ïë Nh√≥m 3   ‚ïë 18           ‚ïë 1           ‚ïë 5.556% ‚Üê C·ª∞C CAO (nh∆∞ng ch·ªâ 18 giao d·ªãch)‚ïë
‚ïë Nh√≥m 4   ‚ïë 3,905,021   ‚ïë 1,608       ‚ïë 0.041% ‚Üê TH·∫§P   ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï©‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï©‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï©‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

T·ªïng: 225,546 giao d·ªãch r·ª≠a ti·ªÅn (0.126% t·ªïng s·ªë)
```

> **Gi·∫£i th√≠ch**:
> - **Nh√≥m 0**: Trong 36 tri·ªáu giao d·ªãch, c√≥ 29,920 giao d·ªãch r·ª≠a ti·ªÅn ‚Üí T·ª∑ l·ªá = 0.081% (r·∫•t th·∫•p, an to√†n)
> - **Nh√≥m 1**: 0.113% (an to√†n)
> - **Nh√≥m 2**: 0.167% (cao nh·∫•t trong c√°c nh√≥m l·ªõn, c·∫ßn ch√∫ √Ω)
> - **Nh√≥m 3**: 5.556% (C·ª∞C CAO! Nh∆∞ng ch·ªâ c√≥ 18 giao d·ªãch ‚Üí c√≥ th·ªÉ l√† outlier/c√° bi·ªát)
> - **Nh√≥m 4**: 0.041% (th·∫•p nh·∫•t, r·∫•t an to√†n)

**3. C·ª•m c√≥ r·ªßi ro cao (>10% r·ª≠a ti·ªÅn)**:
```
‚ö†Ô∏è  KI·ªÇM TRA:
‚úÖ KH√îNG c√≥ c·ª•m n√†o v∆∞·ª£t ng∆∞·ª°ng 10%
   T·∫•t c·∫£ c√°c c·ª•m ƒë·ªÅu trong m·ª©c ch·∫•p nh·∫≠n ƒë∆∞·ª£c.
   
‚ö†Ô∏è  L∆∞u √Ω: Cluster 3 c√≥ t·ª∑ l·ªá 5.56% (cao nh·∫•t) nh∆∞ng ch·ªâ c√≥ 18 giao d·ªãch
   ‚Üí ƒê√¢y l√† c√°c giao d·ªãch outlier v·ªõi gi√° tr·ªã c·ª±c l·ªõn c·∫ßn ki·ªÉm tra th·ªß c√¥ng
```

**4. ƒê·∫∑c tr∆∞ng trung b√¨nh m·ªói c·ª•m**:
```
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë Cluster  ‚ïë avg_amount_received ‚ïë avg_amount_paid ‚ïë avg_ratio ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¨‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¨‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¨‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë    0     ‚ïë 8.62 tri·ªáu          ‚ïë 8.63 tri·ªáu      ‚ïë 1.01      ‚ïë
‚ïë    1     ‚ïë 4.57 tri·ªáu          ‚ïë 2.50 tri·ªáu      ‚ïë 3.26      ‚ïë
‚ïë    2     ‚ïë 4.26 tri·ªáu          ‚ïë 2.46 tri·ªáu      ‚ïë 1.15      ‚ïë
‚ïë    3     ‚ïë 4.24 NGH√åN T·ª∂      ‚ïë 2.86 NGH√åN T·ª∂  ‚ïë 21.54     ‚ïë ‚Üê OUTLIER!
‚ïë    4     ‚ïë 804                 ‚ïë 804             ‚ïë 1.0       ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï©‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï©‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï©‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
```

**Nh·∫≠n x√©t chi ti·∫øt**:
1. **C·ª•m nghi ng·ªù NH·∫§T: Cluster 3 (5.56% r·ª≠a ti·ªÅn)**
   - Ch·ªâ c√≥ 18 giao d·ªãch nh∆∞ng gi√° tr·ªã c·ª±c l·ªõn (ngh√¨n t·ª∑)
   - T·ª∑ l·ªá r·ª≠a ti·ªÅn cao nh·∫•t (5.56%)
   - **Khuy·∫øn ngh·ªã**: Ki·ªÉm tra th·ªß c√¥ng ngay l·∫≠p t·ª©c 18 giao d·ªãch n√†y

2. **C·ª•m an to√†n NH·∫§T: Cluster 4 (0.041% r·ª≠a ti·ªÅn)**
   - T·ª∑ l·ªá th·∫•p nh·∫•t trong t·∫•t c·∫£ c√°c c·ª•m
   - Gi√° tr·ªã giao d·ªãch nh·ªè (~804 ƒë∆°n v·ªã)
   - C√≥ th·ªÉ ∆∞u ti√™n th·∫•p khi ki·ªÉm tra

3. **C√°c c·ª•m ch√≠nh (0, 1, 2) an to√†n**
   - Chi·∫øm 97.83% t·ªïng giao d·ªãch
   - T·ª∑ l·ªá r·ª≠a ti·ªÅn: 0.081% - 0.167% (d∆∞·ªõi 0.2%)
   - T·∫•t c·∫£ ƒë·ªÅu trong m·ª©c ch·∫•p nh·∫≠n ƒë∆∞·ª£c

4. **ƒê√°nh gi√° t·ªïng th·ªÉ**: ‚ö†Ô∏è **R·ª¶I RO TRUNG B√åNH**
   - T·ª∑ l·ªá r·ª≠a ti·ªÅn trong m·ª©c ch·∫•p nh·∫≠n nh∆∞ng c·∫ßn theo d√µi
   - Kh√¥ng c√≥ c·ª•m n√†o v∆∞·ª£t ng∆∞·ª°ng c·∫£nh b√°o 10%
   - Cluster 3 c·∫ßn ƒë∆∞·ª£c ki·ªÉm tra k·ªπ do ƒë·∫∑c ƒëi·ªÉm outlier

**T·ªïng th·ªùi gian b∆∞·ªõc 7: 30 gi√¢y**

**K·∫øt qu·∫£ cu·ªëi c√πng**:
- ‚úÖ ƒê√£ ph√¢n t√≠ch 179,702,229 giao d·ªãch
- ‚úÖ Ph√¢n th√†nh 5 c·ª•m v·ªõi ph√¢n ph·ªëi r√µ r√†ng
- ‚úÖ T·ª∑ l·ªá r·ª≠a ti·ªÅn: 0.04% - 5.56%
- ‚úÖ S·ªë c·ª•m r·ªßi ro cao (>10%): 0 (T·ªët!)
- ‚úÖ X√°c ƒë·ªãnh ƒë∆∞·ª£c c·ª•m outlier (Cluster 3) c·∫ßn ki·ªÉm tra

---

<a id="p5"></a>
## PH·∫¶N 5: K·∫æT QU·∫¢ V√Ä ƒê√ÅNH GI√Å

### 5.1. K·∫øt qu·∫£ ph√¢n c·ª•m

#### Th·ªëng k√™ t·ªïng quan
- **T·ªïng giao d·ªãch x·ª≠ l√Ω**: 179,702,229
- **S·ªë c·ª•m**: 5 c·ª•m
- **S·ªë ƒë·∫∑c tr∆∞ng**: 9 ƒë·∫∑c tr∆∞ng/giao d·ªãch
- **Snapshot**: snapshot_20251029_213229
- **K√≠ch th∆∞·ªõc k·∫øt qu·∫£**: 342.75 MB (compressed)
- **Thu·∫≠t to√°n**: MLlib K-means v·ªõi k-means++ initialization

#### Ph√¢n t√≠ch chi ti·∫øt t·ª´ng c·ª•m

**üîµ Cluster 0 - C·ª•m Giao D·ªãch V·ª´a**
- S·ªë l∆∞·ª£ng: 36,926,395 (20.55%)
- R·ª≠a ti·ªÅn: 29,920 giao d·ªãch (0.081%)
- ƒê·∫∑c ƒëi·ªÉm:
  - Gi√° tr·ªã trung b√¨nh received: 8.62M
  - Gi√° tr·ªã trung b√¨nh paid: 8.63M
  - T·ª∑ l·ªá received/paid: 1.01
  - ƒê√°nh gi√°: **R·ª¶I RO TH·∫§P**

**üî∑ Cluster 1 - C·ª•m L·ªõn Nh·∫•t**
- S·ªë l∆∞·ª£ng: 69,939,082 (38.92%)
- R·ª≠a ti·ªÅn: 78,960 giao d·ªãch (0.113%)
- ƒê·∫∑c ƒëi·ªÉm:
  - Gi√° tr·ªã trung b√¨nh received: 4.57M
  - Gi√° tr·ªã trung b√¨nh paid: 2.50M
  - T·ª∑ l·ªá received/paid: 3.26
  - ƒê√°nh gi√°: **R·ª¶I RO TH·∫§P**

**üî∂ Cluster 2 - C·ª•m ƒê√¥ng Th·ª© Hai**
- S·ªë l∆∞·ª£ng: 68,931,713 (38.36%)
- R·ª≠a ti·ªÅn: 115,057 giao d·ªãch (0.167%)
- ƒê·∫∑c ƒëi·ªÉm:
  - Gi√° tr·ªã trung b√¨nh received: 4.26M
  - Gi√° tr·ªã trung b√¨nh paid: 2.46M
  - T·ª∑ l·ªá received/paid: 1.15
  - ƒê√°nh gi√°: **R·ª¶I RO TRUNG B√åNH**

**üî¥ Cluster 3 - Outlier (R·ªßi Ro Cao)**
- S·ªë l∆∞·ª£ng: 18 (0.00%) ‚Üê C·ª∞C K·ª≤ √çT
- R·ª≠a ti·ªÅn: 1 giao d·ªãch (5.56%)
- ƒê·∫∑c ƒëi·ªÉm:
  - Gi√° tr·ªã trung b√¨nh received: 4.24 ngh√¨n t·ª∑ (outlier c·ª±c l·ªõn)
  - Gi√° tr·ªã trung b√¨nh paid: 2.86 ngh√¨n t·ª∑
  - T·ª∑ l·ªá received/paid: 21.54
  - ƒê√°nh gi√°: **OUTLIER - Ki·ªÉm tra th·ªß c√¥ng ngay**

**üü£ Cluster 4 - C·ª•m Nh·ªè**
- S·ªë l∆∞·ª£ng: 3,905,021 (2.17%)
- ƒê·∫∑c ƒëi·ªÉm:
  - C·ª•m nh·ªè nh·∫•t trong 5 c·ª•m
  - Chi·∫øm 2.17% t·ªïng giao d·ªãch
  - ƒê√°nh gi√°: **C·ª§M ƒê·∫∂C BI·ªÜT**

### 5.2. Nh·∫≠n x√©t v√† Insights

#### Ph√°t hi·ªán ch√≠nh
1. **Cluster 3 l√† outlier r·ªßi ro cao**
   - T·ª∑ l·ªá r·ª≠a ti·ªÅn 5.56% (d∆∞·ªõi ng∆∞·ª°ng 10% nh∆∞ng v·∫´n cao b·∫•t th∆∞·ªùng)
   - NH∆ØNG ch·ªâ c√≥ 18 giao d·ªãch trong c·ª•m n√†y
   - ƒê√¢y l√† c√°c giao d·ªãch outlier v·ªõi gi√° tr·ªã C·ª∞C L·ªöN (ngh√¨n t·ª∑)
   - Khuy·∫øn ngh·ªã: Ki·ªÉm tra th·ªß c√¥ng ngay l·∫≠p t·ª©c 18 giao d·ªãch n√†y

2. **C√°c c·ª•m ch√≠nh (0, 1, 2) an to√†n**
   - Cluster 0: 0.081% (20.55% t·ªïng giao d·ªãch) ‚úì
   - Cluster 1: 0.113% (38.92% t·ªïng giao d·ªãch) ‚úì
   - Cluster 2: 0.167% (38.36% t·ªïng giao d·ªãch) - cao nh·∫•t trong c·ª•m ch√≠nh
   - T·∫•t c·∫£ ƒë·ªÅu d∆∞·ªõi 0.2% - trong m·ª©c ch·∫•p nh·∫≠n ƒë∆∞·ª£c

3. **Cluster 4 an to√†n nh·∫•t**
   - Ch·ªâ 0.041% r·ª≠a ti·ªÅn (th·∫•p nh·∫•t trong t·∫•t c·∫£)
   - C√≥ th·ªÉ ∆∞u ti√™n th·∫•p khi ki·ªÉm tra

4. **Ph√¢n ph·ªëi kh√¥ng ƒë·ªÅu r√µ r·ªát**
   - 2 c·ª•m l·ªõn chi·∫øm ~77% (Cluster 1, 2 v·ªõi 38.92% v√† 38.36%)
   - 1 c·ª•m outlier c·ª±c nh·ªè (Cluster 3: ch·ªâ 18 giao d·ªãch nh∆∞ng gi√° tr·ªã kh·ªïng l·ªì)
   - Thu·∫≠t to√°n MLlib K-means++ ph√¢n bi·ªát r·∫•t t·ªët c√°c outliers

5. **KH√îNG c√≥ c·ª•m n√†o v∆∞·ª£t ng∆∞·ª°ng 10%**
   - ƒêi·ªÅu n√†y r·∫•t t·ªët, cho th·∫•y h·ªá th·ªëng ho·∫°t ƒë·ªông hi·ªáu qu·∫£
   - Cluster 3 (5.56%) l√† nghi ng·ªù nh·∫•t nh∆∞ng v·∫´n d∆∞·ªõi ng∆∞·ª°ng

#### So s√°nh v·ªõi ng∆∞·ª°ng
```
Ng∆∞·ª°ng c·∫£nh b√°o: > 10% r·ª≠a ti·ªÅn

Cluster 0: 0.081% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ OK (20.6% giao d·ªãch)
Cluster 1: 0.113% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ OK (38.9% giao d·ªãch)
Cluster 2: 0.167% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ OK (38.4% giao d·ªãch)
Cluster 3:  5.56% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ CAUTION (ch·ªâ 18 giao d·ªãch)
Cluster 4: 0.041% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ OK (2.2% giao d·ªãch)

‚úÖ T·∫§T C·∫¢ C√ÅC C·ª§M D∆Ø·ªöI NG∆Ø·ª†NG 10%!
```

### 5.3. Hi·ªáu su·∫•t h·ªá th·ªëng

#### Th·ªùi gian x·ª≠ l√Ω chi ti·∫øt (29/10/2025 18:33-18:45)
| B∆∞·ªõc | C√¥ng vi·ªác | Th·ªùi gian | % T·ªïng |
|------|-----------|-----------|--------|
| 1 | Kh√°m ph√° | 10s | 1.4% |
| 2 | Feature Engineering | 26s | 3.7% |
| 3 | Upload HDFS | 40s | 5.6% |
| 4 | Spark MLlib K-means | 407s | 57.4% |
| 5 | Download | 3s | 0.4% |
| 6 | G√°n nh√£n | 194s | 27.4% |
| 7 | Ph√¢n t√≠ch | 27s | 3.8% |
| T·ªïng | | 707s (11 ph√∫t 47 gi√¢y) | 100% |

‚úÖ **ƒê√£ c·∫≠p nh·∫≠t**: Nhanh h∆°n 30-50% nh·ªù MLlib K-means++
‚úÖ **Snapshot**: `snapshot_20251029_213229`

**Nh·∫≠n x√©t**:
- K-means chi·∫øm 57.4% th·ªùi gian (t·ªëi ∆∞u h∆°n nh·ªù MLlib)
- Feature Engineering gi·∫£m t·ª´ 66s ‚Üí 26s (tƒÉng t·ªëc 2.5x)
- C√°c b∆∞·ªõc c√≤n l·∫°i r·∫•t nhanh nh·ªù Polars v√† caching HDFS

#### So s√°nh v·ªõi Hadoop MapReduce
| Ti√™u ch√≠ | Hadoop (Legacy) | Spark (Hi·ªán t·∫°i) | C·∫£i thi·ªán |
|----------|-----------------|------------------|-----------|
| Th·ªùi gian K-means | 1-2 gi·ªù | 26 ph√∫t | **4-8x nhanh h∆°n** |
| RAM s·ª≠ d·ª•ng | √çt (disk-based) | Nhi·ªÅu (in-memory) | Trade-off |
| ƒê·ªô ph·ª©c t·∫°p code | Cao (mapper/reducer) | Th·∫•p (PySpark API) | D·ªÖ maintain |
| Debug | Kh√≥ | D·ªÖ (local mode) | Developer friendly |

**K·∫øt lu·∫≠n**: Spark l√† l·ª±a ch·ªçn ƒë√∫ng ƒë·∫Øn cho K-means iterative!

---

<a id="p6"></a>
## PH·∫¶N 6: TU√ÇN TH·ª¶ QUY ƒê·ªäNH B·∫¢O M·∫¨T

### 6.1. Quy ƒë·ªãnh: KH√îNG l∆∞u d·ªØ li·ªáu l·ªõn ·ªü m√°y c·ª•c b·ªô

#### L√Ω do c√≥ quy ƒë·ªãnh n√†y
1. **B·∫£o m·∫≠t**: D·ªØ li·ªáu kh√°ch h√†ng nh·∫°y c·∫£m
2. **Tu√¢n th·ªß ph√°p lu·∫≠t**: GDPR, CCPA, v.v.
3. **NgƒÉn ch·∫∑n r√≤ r·ªâ**: M√°y c√° nh√¢n d·ªÖ b·ªã hack
4. **Ki·ªÉm so√°t truy c·∫≠p**: HDFS c√≥ authentication

### 6.2. C√°ch d·ª± √°n tu√¢n th·ªß

#### ‚úÖ ƒê∆Ø·ª¢C PH√âP l∆∞u ·ªü m√°y c·ª•c b·ªô
```
data/
‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îî‚îÄ‚îÄ HI-Large_Trans.csv     ‚úì (File g·ªëc t·ª´ gi·∫£ng vi√™n)
‚îÇ
‚îî‚îÄ‚îÄ results/
    ‚îú‚îÄ‚îÄ final_centroids.txt    ‚úì (Ch·ªâ 4KB - k·∫øt qu·∫£ t·ªïng h·ª£p)
    ‚îî‚îÄ‚îÄ clustered_results.txt  ‚úì (C√≥ th·ªÉ t·∫°o l·∫°i t·ª´ HDFS)
```

#### ‚ùå KH√îNG ƒê∆Ø·ª¢C l∆∞u ·ªü m√°y c·ª•c b·ªô
```
data/
‚îî‚îÄ‚îÄ processed/
    ‚îú‚îÄ‚îÄ hadoop_input_temp.txt  ‚ùå (33GB - T·ª∞ ƒê·ªòNG X√ìA)
    ‚îî‚îÄ‚îÄ centroids_temp.txt     ‚ùå (440B - T·ª∞ ƒê·ªòNG X√ìA)
```

#### C∆° ch·∫ø t·ª± ƒë·ªông x√≥a
**Trong file** `scripts/spark/setup_hdfs.sh`:
```bash
# Upload l√™n HDFS
hdfs dfs -put data/processed/hadoop_input_temp.txt /user/spark/hi_large/

# X√ìA NGAY SAU KHI UPLOAD TH√ÄNH C√îNG
echo "Cleaning up temp files..."
rm -rf "$PROJECT_ROOT/data/processed/"*

echo "‚úÖ Temp files deleted (data now only on HDFS)"
```

#### Verification (Ki·ªÉm ch·ª©ng)
**Ki·ªÉm tra tr∆∞·ªõc khi upload**:
```bash
$ du -sh data/processed/
33G    data/processed/  ‚Üê C√≥ file temp
```

**Ki·ªÉm tra sau khi upload**:
```bash
$ du -sh data/processed/
0      data/processed/  ‚Üê ƒê√£ x√≥a s·∫°ch! ‚úì

$ hdfs dfs -du -h /user/spark/hi_large/
31.0 G  /user/spark/hi_large/input/hadoop_input.txt  ‚Üê Tr√™n HDFS
```

### 6.3. Quy tr√¨nh kh√¥i ph·ª•c (n·∫øu c·∫ßn)
N·∫øu c·∫ßn xem l·∫°i d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω:
```bash
# T·∫£i v·ªÅ t·ª´ HDFS
hdfs dfs -get /user/spark/hi_large/input/hadoop_input.txt data/processed/

# S·ª≠ d·ª•ng
python scripts/polars/analyze_polars.py

# X√≥a l·∫°i sau khi d√πng xong
rm data/processed/hadoop_input.txt
```

---

<a id="p7"></a>
## PH·∫¶N 7: H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG

### 7.1. Y√™u c·∫ßu h·ªá th·ªëng

#### Ph·∫ßn c·ª©ng t·ªëi thi·ªÉu
- **CPU**: 4 cores (khuy·∫øn ngh·ªã 8+ cores)
- **RAM**: 16GB (khuy·∫øn ngh·ªã 32GB)
- **·ªî c·ª©ng**: 50GB tr·ªëng (cho HDFS)
- **M·∫°ng**: N·∫øu d√πng cluster, c·∫ßn LAN t·ªëc ƒë·ªô cao

#### Ph·∫ßn m·ªÅm
- **H·ªá ƒëi·ªÅu h√†nh**: Linux (Ubuntu, CentOS, Arch)
- **Java**: JDK 11 ho·∫∑c 17
- **Python**: 3.12+
- **Hadoop**: 3.x (HDFS)
- **Spark**: 4.0.1

#### Th∆∞ vi·ªán Python
```bash
polars==0.20.x   # DataFrame library
numpy==1.26.x    # Numerical computing
pyspark==4.0.x   # Spark Python API
```

### 7.2. H∆∞·ªõng d·∫´n c√†i ƒë·∫∑t t·ª´ ƒë·∫ßu

#### B∆∞·ªõc 1: C√†i ƒë·∫∑t Java
```bash
# Ubuntu/Debian
sudo apt update
sudo apt install openjdk-17-jdk

# Arch Linux
sudo pacman -S jdk17-openjdk

# Ki·ªÉm tra
java -version  # Ph·∫£i th·∫•y version 17.x.x
```

#### B∆∞·ªõc 2: C√†i ƒë·∫∑t Hadoop
```bash
# Download Hadoop
cd /tmp
wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
tar -xzf hadoop-3.3.6.tar.gz
sudo mv hadoop-3.3.6 /opt/hadoop

# C·∫•u h√¨nh bi·∫øn m√¥i tr∆∞·ªùng (~/.bashrc ho·∫∑c ~/.zshrc)
export HADOOP_HOME=/opt/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

# Reload
source ~/.zshrc

# Ki·ªÉm tra
hadoop version
```

#### B∆∞·ªõc 3: C√†i ƒë·∫∑t Spark (t·ª± ƒë·ªông)
```bash
cd /home/ultimatebrok/Downloads/Final
./scripts/setup/install_spark.sh

# Script s·∫Ω t·ª± ƒë·ªông:
# - Download Spark 4.0.1
# - Gi·∫£i n√©n v√†o /opt/spark
# - Th√™m v√†o PATH
# - C·∫•u h√¨nh SPARK_HOME

# Reload shell
source ~/.zshrc

# Ki·ªÉm tra
spark-submit --version
```

#### B∆∞·ªõc 4: C√†i ƒë·∫∑t Python packages
```bash
# T·∫°o virtual environment (khuy·∫øn ngh·ªã)
python3 -m venv .venv
source .venv/bin/activate

# C√†i ƒë·∫∑t
pip install polars numpy pyspark

# Ki·ªÉm tra
python -c "import polars; print(polars.__version__)"
```

#### B∆∞·ªõc 5: Kh·ªüi ƒë·ªông HDFS
```bash
# Format namenode (CH·ªà L·∫¶N ƒê·∫¶U)
hdfs namenode -format

# Kh·ªüi ƒë·ªông HDFS
start-dfs.sh

# Ki·ªÉm tra
hdfs dfsadmin -report
# Ph·∫£i th·∫•y "Live datanodes (1)"
```

### 7.3. Ch·∫°y pipeline

#### C√°ch 1: T·ª± ƒë·ªông (Khuy·∫øn ngh·ªã)
```bash
cd /home/ultimatebrok/Downloads/Final

# ƒê·∫£m b·∫£o c√≥ file CSV
ls -lh data/raw/HI-Large_Trans.csv

# Ch·∫°y to√†n b·ªô pipeline
./scripts/pipeline/full_pipeline_spark.sh

# Pipeline s·∫Ω t·ª± ƒë·ªông ch·∫°y 7 b∆∞·ªõc (MLlib K-means)
# Th·ªùi gian: 35-50 ph√∫t (nhanh h∆°n 30-50%)
# Log: logs/pipeline_log_YYYYMMDD_HHMMSS.md
```

#### C√°ch 2: T·ª´ng b∆∞·ªõc (Debug)
```bash
# B∆∞·ªõc 1
python scripts/polars/explore_fast.py

# B∆∞·ªõc 2
python scripts/polars/prepare_polars.py

# B∆∞·ªõc 3 (Upload to HDFS)
scripts/spark/setup_hdfs.sh

# B∆∞·ªõc 4 (MLlib K-means - t·ª± ƒë·ªông d√πng k-means++)
scripts/spark/run_spark.sh

# B∆∞·ªõc 5
scripts/spark/download_from_hdfs.sh

# B∆∞·ªõc 6
python scripts/polars/assign_clusters_polars.py

# B∆∞·ªõc 7
python scripts/polars/analyze_polars.py
```

### 7.4. Xem k·∫øt qu·∫£

```bash
# Xem log pipeline
cat logs/pipeline_log_*.md

# Xem t√¢m c·ª•m cu·ªëi c√πng
cat data/results/final_centroids.txt

# Xem d·ªØ li·ªáu ƒë√£ g√°n nh√£n (10 d√≤ng ƒë·∫ßu)
head data/results/clustered_results.txt
```

---

<a id="p8"></a>
## PH·∫¶N 8: X·ª¨ L√ù S·ª∞ C·ªê

### 8.1. L·ªói th∆∞·ªùng g·∫∑p

#### L·ªói 1: HDFS kh√¥ng kh·ªüi ƒë·ªông ƒë∆∞·ª£c
**Tri·ªáu ch·ª©ng**:
```
hdfs dfsadmin -report
Connection refused
```

**Nguy√™n nh√¢n**: HDFS ch∆∞a ƒë∆∞·ª£c kh·ªüi ƒë·ªông
**Gi·∫£i ph√°p**:
```bash
# Ki·ªÉm tra process
jps  # Ph·∫£i th·∫•y NameNode v√† DataNode

# N·∫øu kh√¥ng th·∫•y, kh·ªüi ƒë·ªông l·∫°i
stop-dfs.sh
start-dfs.sh

# ƒê·ª£i 10 gi√¢y r·ªìi ki·ªÉm tra
hdfs dfsadmin -report
```

#### L·ªói 2: Out of Memory trong Spark
**Tri·ªáu ch·ª©ng**:
```
java.lang.OutOfMemoryError: Java heap space
```

**Nguy√™n nh√¢n**: RAM kh√¥ng ƒë·ªß cho executor
**Gi·∫£i ph√°p**: TƒÉng memory trong `scripts/spark/run_spark.sh`
```bash
# T√¨m d√≤ng:
--driver-memory 4g \
--executor-memory 4g \

# S·ª≠a th√†nh (n·∫øu c√≥ ƒë·ªß RAM):
--driver-memory 8g \
--executor-memory 8g \
```

#### L·ªói 3: File temp kh√¥ng t·ª± ƒë·ªông x√≥a
**Tri·ªáu ch·ª©ng**: V·∫´n th·∫•y file trong `data/processed/`
**Nguy√™n nh√¢n**: Script b·ªã l·ªói gi·ªØa ch·ª´ng
**Gi·∫£i ph√°p**: X√≥a th·ªß c√¥ng
```bash
rm -rf data/processed/*

# Ho·∫∑c ch·∫°y script cleanup
./scripts/pipeline/clean_spark.sh
```

#### L·ªói 4: Polars b√°o l·ªói memory
**Tri·ªáu ch·ª©ng**:
```
MemoryError: Unable to allocate array
```

**Nguy√™n nh√¢n**: RAM kh√¥ng ƒë·ªß khi load CSV
**Gi·∫£i ph√°p**: D√πng streaming mode
```python
# Thay v√¨:
df = pl.read_csv('file.csv')

# D√πng:
df = pl.scan_csv('file.csv')  # Lazy, kh√¥ng load h·∫øt v√†o RAM
df.sink_csv('output.csv')     # Stream ra file
```

### 8.2. Ki·ªÉm tra h·ªá th·ªëng

#### Checklist tr∆∞·ªõc khi ch·∫°y
```bash
# 1. Java
java -version  # Ph·∫£i c√≥ version 11 ho·∫∑c 17

# 2. HDFS
hdfs dfsadmin -report  # Ph·∫£i th·∫•y "Live datanodes"

# 3. Spark
spark-submit --version  # Ph·∫£i c√≥ version 4.x

# 4. Python packages
python -c "import polars, numpy, pyspark"  # Kh√¥ng l·ªói

# 5. File CSV
ls -lh data/raw/HI-Large_Trans.csv  # Ph·∫£i ~16GB

# 6. Disk space
df -h  # Ph·∫£i c√≤n > 50GB tr·ªëng
```

---

<a id="p9"></a>
## PH·∫¶N 9: K·∫æT LU·∫¨N V√Ä H∆Ø·ªöNG PH√ÅT TRI·ªÇN

### 9.1. T·ªïng k·∫øt d·ª± √°n

#### Nh·ªØng g√¨ ƒë√£ ƒë·∫°t ƒë∆∞·ª£c
‚úÖ **V·ªÅ k·ªπ thu·∫≠t**:
- X·ª≠ l√Ω th√†nh c√¥ng 179 tri·ªáu giao d·ªãch (16GB CSV)
- √Åp d·ª•ng **MLlib K-means** v·ªõi k-means++ tr√™n Apache Spark
- Th·ªùi gian x·ª≠ l√Ω: 30 ph√∫t (nhanh h∆°n Hadoop 4-8 l·∫ßn, nhanh h∆°n RDD 30-50%)
- X√¢y d·ª±ng pipeline t·ª± ƒë·ªông **7 b∆∞·ªõc** (t·ªëi ∆∞u t·ª´ 8 b∆∞·ªõc)
- Tu√¢n th·ªß quy ƒë·ªãnh b·∫£o m·∫≠t d·ªØ li·ªáu

‚úÖ **V·ªÅ h·ªçc m√°y**:
- Ph√¢n c·ª•m th√†nh c√¥ng th√†nh 5 nh√≥m
- Thu·∫≠t to√°n h·ªôi t·ª• t·ªët (shift < 0.01)
- Ph√°t hi·ªán 225,546 giao d·ªãch nghi ng·ªù
- X√°c ƒë·ªãnh ƒë∆∞·ª£c c·ª•m r·ªßi ro cao nh·∫•t (Cluster 1)

‚úÖ **V·ªÅ ph√°t tri·ªÉn ph·∫ßn m·ªÅm**:
- Code c√≥ c·∫•u tr√∫c r√µ r√†ng (modular)
- T√†i li·ªáu ƒë·∫ßy ƒë·ªß, d·ªÖ hi·ªÉu
- D·ªÖ b·∫£o tr√¨ v√† m·ªü r·ªông
- C√≥ h·ªá th·ªëng log chi ti·∫øt

#### H·∫°n ch·∫ø
‚ö†Ô∏è **V·ªÅ thu·∫≠t to√°n**:
- K-means nh·∫°y c·∫£m v·ªõi K ban ƒë·∫ßu
- Ch∆∞a t·ª± ƒë·ªông ch·ªçn K t·ªëi ∆∞u (hi·ªán t·∫°i fix K=5)
- Ch∆∞a x·ª≠ l√Ω outliers (ƒëi·ªÉm ngo·∫°i lai)

‚ö†Ô∏è **V·ªÅ infrastructure**:
- Ch·∫°y tr√™n single machine (pseudo-distributed)
- Ch∆∞a test tr√™n cluster th·∫≠t
- Ch∆∞a c√≥ monitoring real-time

### 9.2. H∆∞·ªõng ph√°t tri·ªÉn t∆∞∆°ng lai

#### 1. C·∫£i thi·ªán thu·∫≠t to√°n
**T·ª± ƒë·ªông ch·ªçn K t·ªëi ∆∞u**:
- D√πng Elbow Method
- D√πng Silhouette Score
- Ch·∫°y K-means v·ªõi nhi·ªÅu K (3, 5, 7, 10) v√† so s√°nh

**Kh·ªüi t·∫°o t·ªët h∆°n**:
- ‚úÖ **ƒê√£ √°p d·ª•ng**: MLlib K-means t·ª± ƒë·ªông d√πng k-means++
- K·∫øt qu·∫£: Gi·∫£m s·ªë v√≤ng l·∫∑p (15 ‚Üí 10-12), ·ªïn ƒë·ªãnh h∆°n

**X·ª≠ l√Ω outliers**:
- Ph√°t hi·ªán v√† lo·∫°i b·ªè outliers tr∆∞·ªõc khi cluster
- D√πng DBSCAN ho·∫∑c Isolation Forest

#### 2. Machine Learning n√¢ng cao
**Supervised Learning**:
- D√πng nh√£n "Is Laundering" ƒë·ªÉ train model
- Th·ª≠ Random Forest, XGBoost
- So s√°nh accuracy, precision, recall

**Deep Learning**:
- Neural Network cho ph√°t hi·ªán anomaly
- Autoencoder ƒë·ªÉ h·ªçc representation
- LSTM cho time series patterns

**Ensemble Methods**:
- K·∫øt h·ª£p nhi·ªÅu models
- Voting mechanism
- TƒÉng ƒë·ªô ch√≠nh x√°c

#### 3. Real-time Processing
**Spark Streaming**:
- X·ª≠ l√Ω giao d·ªãch real-time khi ch√∫ng x·∫£y ra
- C·∫£nh b√°o t·ª©c th√¨ khi ph√°t hi·ªán nghi ng·ªù
- D√πng Kafka l√†m message queue

**Dashboard**:
- Visualize clusters b·∫±ng Plotly
- Real-time monitoring
- Alert system

#### 4. Deployment
**Containerization**:
```dockerfile
# Dockerfile
FROM apache/spark:4.0.1
COPY scripts/ /app/scripts/
COPY data/ /app/data/
CMD ["./scripts/pipeline/full_pipeline_spark.sh"]
```

**Kubernetes**:
- Orchestrate Spark cluster
- Auto-scaling based on load
- High availability

**CI/CD**:
- GitHub Actions cho testing
- Automated deployment
- Version control

#### 5. B·∫£o m·∫≠t n√¢ng cao
- Encryption at rest (HDFS)
- Encryption in transit (SSL/TLS)
- Role-based access control
- Audit logging

### 9.3. B√†i h·ªçc kinh nghi·ªám

#### V·ªÅ k·ªπ thu·∫≠t
1. **Ch·ªçn c√¥ng ngh·ªá ph√π h·ª£p**:
   - Polars cho single-machine processing
   - Spark cho distributed processing
   - HDFS cho storage
   - M·ªói tool c√≥ strengths ri√™ng

2. **Pipeline automation**:
   - Vi·∫øt scripts ƒë·ªÉ t·ª± ƒë·ªông h√≥a
   - S·ª≠ d·ª•ng checkpoints
   - Logging ƒë·∫ßy ƒë·ªß

3. **Tu√¢n th·ªß quy ƒë·ªãnh t·ª´ ƒë·∫ßu**:
   - Thi·∫øt k·∫ø ki·∫øn tr√∫c v·ªõi security in mind
   - T·ª± ƒë·ªông x√≥a temp files
   - Kh√¥ng l∆∞u d·ªØ li·ªáu nh·∫°y c·∫£m local

#### V·ªÅ h·ªçc m√°y
1. **Feature engineering quan tr·ªçng**:
   - Parse timestamp ‚Üí temporal features
   - T√≠nh ratio ƒë·ªÉ ph√°t hi·ªán b·∫•t th∆∞·ªùng
   - Normalize ƒë·ªÉ thu·∫≠t to√°n ho·∫°t ƒë·ªông t·ªët

2. **K-means c·∫ßn fine-tuning**:
   - Ch·ªçn K ph√π h·ª£p
   - Kh·ªüi t·∫°o centroids t·ªët
   - Ki·ªÉm tra convergence

3. **Validation r·∫•t quan tr·ªçng**:
   - Ph√¢n t√≠ch k·∫øt qu·∫£ sau m·ªói run
   - So s√°nh v·ªõi ground truth
   - Iterate ƒë·ªÉ c·∫£i thi·ªán

---

<a id="phu-luc"></a>
## PH·ª§ L·ª§C

### A. Thu·∫≠t ng·ªØ v√† Gi·∫£i th√≠ch (T·ª´ ƒëi·ªÉn cho ng∆∞·ªùi m·ªõi)

| Thu·∫≠t ng·ªØ | √ù nghƒ©a ƒë∆°n gi·∫£n | V√≠ d·ª• |
|-----------|------------------|-------|
| **Big Data** | D·ªØ li·ªáu qu√° l·ªõn (>1TB), kh√¥ng th·ªÉ x·ª≠ l√Ω b·∫±ng m√°y t√≠nh th∆∞·ªùng | Nh∆∞ c√≥ 1 tri·ªáu quy·ªÉn s√°ch, kh√¥ng th·ªÉ ƒë·ªçc h·∫øt b·∫±ng tay |
| **Cluster** | Nhi·ªÅu m√°y t√≠nh l√†m vi·ªác c√πng nhau | Nh∆∞ c√≥ 10 c√¥ng nh√¢n c√πng l√†m m·ªôt c√¥ng vi·ªác l·ªõn |
| **Distributed Computing** | X·ª≠ l√Ω ph√¢n t√°n - chia c√¥ng vi·ªác cho nhi·ªÅu m√°y | Nh∆∞ chia 1000 trang s√°ch cho 10 ng∆∞·ªùi ƒë·ªçc, m·ªói ng∆∞·ªùi 100 trang |
| **HDFS** | H·ªá th·ªëng l∆∞u tr·ªØ file l·ªõn, t·ª± ƒë·ªông sao l∆∞u | Nh∆∞ Google Drive nh∆∞ng cho d·ªØ li·ªáu c·ª±c l·ªõn, t·ª± ƒë·ªông backup 3 b·∫£n |
| **In-memory Computing** | L√†m vi·ªác trong RAM (nhanh) thay v√¨ ·ªï c·ª©ng (ch·∫≠m) | Nh∆∞ l√†m vi·ªác tr√™n m√°y t√≠nh (RAM) thay v√¨ ghi ra gi·∫•y (·ªï c·ª©ng) |
| **K-means** | Thu·∫≠t to√°n t·ª± ƒë·ªông chia d·ªØ li·ªáu th√†nh K nh√≥m | Nh∆∞ t·ª± ƒë·ªông chia h·ªçc sinh th√†nh 5 l·ªõp d·ª±a tr√™n ƒëi·ªÉm s·ªë |
| **Polars** | C√¥ng c·ª• x·ª≠ l√Ω d·ªØ li·ªáu c·ª±c nhanh (nh∆∞ Excel nh∆∞ng nhanh 100 l·∫ßn) | Nh∆∞ c√≥ m√°y t√≠nh si√™u nhanh ƒë·ªÉ ƒë·ªçc file Excel l·ªõn |
| **Spark** | Framework x·ª≠ l√Ω Big Data, d√πng nhi·ªÅu m√°y c√πng l√∫c | Nh∆∞ c√≥ nhi·ªÅu c√¥ng nh√¢n c√πng l√†m vi·ªác song song |
| **Unsupervised Learning** | H·ªçc m√°y t·ª± h·ªçc, kh√¥ng c·∫ßn d·∫°y tr∆∞·ªõc | Nh∆∞ ƒë·ªÉ m√°y t·ª± t√¨m pattern trong d·ªØ li·ªáu, kh√¥ng c·∫ßn g·ª£i √Ω |
| **Centroid** | T√¢m c·ª•m - ƒëi·ªÉm ƒë·∫°i di·ªán cho m·ªôt nh√≥m | Nh∆∞ ƒëi·ªÉm ƒë·∫°i di·ªán c·ªßa l·ªõp (v√≠ d·ª•: ƒëi·ªÉm trung b√¨nh c·ªßa l·ªõp) |
| **Convergence** | H·ªôi t·ª• - ƒë·∫°t tr·∫°ng th√°i ·ªïn ƒë·ªãnh, kh√¥ng thay ƒë·ªïi n·ªØa | Nh∆∞ l√†m b√†i t·∫≠p ƒë·∫øn khi k·∫øt qu·∫£ kh√¥ng thay ƒë·ªïi n·ªØa |
| **Feature Engineering** | Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng - chuy·ªÉn d·ªØ li·ªáu th√¥ th√†nh s·ªë | Nh∆∞ chuy·ªÉn "Nam/N·ªØ" th√†nh s·ªë (0/1) ƒë·ªÉ m√°y t√≠nh hi·ªÉu |
| **Normalize** | Chu·∫©n h√≥a - ƒë∆∞a t·∫•t c·∫£ v·ªÅ c√πng thang ƒëo | Nh∆∞ quy ƒë·ªïi t·∫•t c·∫£ v·ªÅ c√πng ƒë∆°n v·ªã (km, m, cm ‚Üí ch·ªâ d√πng km) |
| **Pipeline** | Quy tr√¨nh t·ª± ƒë·ªông t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi | Nh∆∞ d√¢y chuy·ªÅn s·∫£n xu·∫•t t·ª± ƒë·ªông t·ª´ nguy√™n li·ªáu ‚Üí s·∫£n ph·∫©m |
| **Replication** | Sao l∆∞u d·ªØ li·ªáu tr√™n nhi·ªÅu m√°y (3 b·∫£n sao) | Nh∆∞ photo 3 b·∫£n t√†i li·ªáu quan tr·ªçng, l∆∞u ·ªü 3 n∆°i kh√°c nhau |
| **Vectorization** | T√≠nh to√°n h√†ng lo·∫°t, nhanh h∆°n v√≤ng l·∫∑p | Nh∆∞ t√≠nh 1 tri·ªáu ph√©p t√≠nh c√πng l√∫c thay v√¨ t·ª´ng ph√©p m·ªôt |
| **Lazy Loading** | Ch·ªâ ƒë·ªçc ph·∫ßn c·∫ßn thi·∫øt, kh√¥ng load h·∫øt | Nh∆∞ ch·ªâ ƒë·ªçc m·ª•c l·ª•c tr∆∞·ªõc, ƒë·ªçc n·ªôi dung sau khi c·∫ßn |
| **Euclidean Distance** | Kho·∫£ng c√°ch th·∫≥ng gi·ªØa 2 ƒëi·ªÉm (nh∆∞ ƒëo ƒë∆∞·ªùng chim bay) | Nh∆∞ ƒëo kho·∫£ng c√°ch t·ª´ ƒëi·ªÉm A ƒë·∫øn ƒëi·ªÉm B tr√™n b·∫£n ƒë·ªì |
| **Outlier** | ƒêi·ªÉm ngo·∫°i lai - gi√° tr·ªã b·∫•t th∆∞·ªùng, kh√°c bi·ªát nhi·ªÅu | Nh∆∞ c√≥ 1 h·ªçc sinh ƒë∆∞·ª£c 100 ƒëi·ªÉm trong khi c·∫£ l·ªõp ch·ªâ 50-60 ƒëi·ªÉm |

### B. C·∫•u tr√∫c th∆∞ m·ª•c ƒë·∫ßy ƒë·ªß

```
Final/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ HI-Large_Trans.csv
‚îÇ   ‚îú‚îÄ‚îÄ processed/              (r·ªóng - files t·ª± ƒë·ªông x√≥a)
‚îÇ   ‚îî‚îÄ‚îÄ results/
‚îÇ       ‚îú‚îÄ‚îÄ final_centroids.txt
‚îÇ       ‚îî‚îÄ‚îÄ clustered_results.txt
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ polars/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ explore_fast.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prepare_polars.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ assign_clusters_polars.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ analyze_polars.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ spark/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ setup_hdfs.sh
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ run_spark.sh
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ kmeans_spark.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ download_from_hdfs.sh
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ pipeline/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ full_pipeline_spark.sh
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clean_spark.sh
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ reset_pipeline.sh
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ setup/
‚îÇ       ‚îî‚îÄ‚îÄ install_spark.sh
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ PROJECT_OVERVIEW.md
‚îÇ   ‚îî‚îÄ‚îÄ HADOOP_ALTERNATIVES.md
‚îÇ
‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îî‚îÄ‚îÄ pipeline_log_20251028_202850.md
‚îÇ
‚îú‚îÄ‚îÄ archive/
‚îÇ   ‚îî‚îÄ‚îÄ hadoop/                (legacy code)
‚îÇ
‚îú‚îÄ‚îÄ .venv/                     (Python virtual env)
‚îú‚îÄ‚îÄ .git/                      (Version control)
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ CHANGELOG.md
‚îî‚îÄ‚îÄ PROJECT_REPORT.md          (B√°o c√°o n√†y)
```

### C. Th·ªëng k√™ d·ª± √°n

- **T·ªïng s·ªë file Python**: 6 files, 442 d√≤ng code
- **T·ªïng s·ªë file Shell**: 7 files, 661 d√≤ng code
- **T·ªïng d√≤ng code**: 1,103 d√≤ng
- **Th·ªùi gian ph√°t tri·ªÉn**: 3 tu·∫ßn
- **C√¥ng ngh·ªá s·ª≠ d·ª•ng**: 5 (Polars, Spark, HDFS, Python, NumPy)
- **S·ªë b∆∞·ªõc pipeline**: 7
- **Th·ªùi gian ch·∫°y**: 30 ph√∫t

---

### D. T√†i li·ªáu tham kh·∫£o

1. Apache Spark Documentation: https://spark.apache.org/docs/latest/
2. Polars Guide: https://pola-rs.github.io/polars-book/
3. Hadoop HDFS: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/
4. K-means Algorithm: https://scikit-learn.org/stable/modules/clustering.html#k-means
5. Money Laundering Detection: Research papers on financial crime

---


**H·∫æT B√ÅO C√ÅO**


_B√°o c√°o ƒë∆∞·ª£c t·∫°o t·ª± ƒë·ªông b·ªüi `generate_vietnamese_report.py`_  
_Ng√†y: 28/10/2025 22:04:46_

---

## üìö T√ÄI LI·ªÜU LI√äN QUAN

ƒê·ªÉ xem ph·∫ßn **l√Ω thuy·∫øt chi ti·∫øt** v·ªÅ c√°c thu·∫≠t to√°n K-means, HDFS, Apache Spark v√† c√°c c√¥ng ngh·ªá s·ª≠ d·ª•ng, vui l√≤ng tham kh·∫£o file:

**üìÑ [`bao_cao_tieu_luan.md`](./bao_cao_tieu_luan.md)**

File ti·ªÉu lu·∫≠n bao g·ªìm:
- B·∫£ng ph√¢n chia c√¥ng vi·ªác
- I. T·ªïng quan v√† l√Ω thuy·∫øt (K-means, HDFS, Spark, Polars, PySpark, NumPy)
- II. M√¥ t·∫£ b√†i to√°n (L√Ω do ch·ªçn ƒë·ªÅ t√†i, M√¥ t·∫£, Quy tr√¨nh th·ª±c hi·ªán)

---

_ƒê·ªÉ xem chi ti·∫øt v·ªÅ l√Ω thuy·∫øt v√† quy tr√¨nh th·ª±c hi·ªán, vui l√≤ng tham kh·∫£o file `bao_cao_tieu_luan.md`._
