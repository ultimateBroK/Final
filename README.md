# ğŸ“Š Dá»± Ãn PhÃ¢n TÃ­ch Rá»­a Tiá»n - K-means Clustering

Pipeline phÃ¢n tÃ­ch 179 triá»‡u giao dá»‹ch sá»­ dá»¥ng Polars vÃ  Apache Spark (PySpark).

> ğŸ“š **Xem thÃªm:** [BÃ¡o cÃ¡o dá»± Ã¡n](bao_cao_du_an.md) Â· [HÆ°á»›ng dáº«n](03_docs/huong-dan.md) Â· [CÃ i Ä‘áº·t](03_docs/cai-dat.md) Â· [Jupyter](03_docs/jupyter.md)

## Má»¥c lá»¥c
- [NÃ¢ng cáº¥p tá»« Hadoop sang Spark](#nang-cap)
- [Cáº¥u trÃºc thÆ° má»¥c](#cau-truc)
- [CÃ i Ä‘áº·t](#cai-dat)
- [Chuáº©n bá»‹ dá»¯ liá»‡u](#chuan-bi-du-lieu)
- [HDFS-Only Workflow](#hdfs-workflow)
- [Cháº¡y Pipeline](#chay-pipeline)
- [Dá»n dáº¹p Project](#don-dep)
- [Dá»¯ liá»‡u trÃªn HDFS](#du-lieu-hdfs)
- [Chi tiáº¿t Pipeline Steps](#chi-tiet-steps)
- [Kiáº¿n trÃºc há»‡ thá»‘ng](#kien-truc)
- [So sÃ¡nh Hadoop vs Spark](#so-sanh)
- [Lá»£i Ã­ch Apache Spark](#loi-ich)
- [So sÃ¡nh vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c](#phuong-phap-khac)

<a id="nang-cap"></a>
## âš¡ NÃ¢ng cáº¥p tá»« Hadoop sang Spark

Project Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t Ä‘á»ƒ sá»­ dá»¥ng **Apache Spark** thay vÃ¬ Hadoop MapReduce, giÃºp:
- âš¡ **Xá»­ lÃ½ nhanh hÆ¡n** nhiá»u láº§n (in-memory computing)
- ğŸ”§ **Dá»… cÃ i Ä‘áº·t hÆ¡n** (khÃ´ng cáº§n HDFS)
- ğŸ“ **Code Ä‘Æ¡n giáº£n hÆ¡n** (PySpark API)
- ğŸš€ **Scale tá»‘t hÆ¡n** vá»›i big data

<a id="cau-truc"></a>
## Cáº¥u trÃºc thÆ° má»¥c

```
Final/
â”œâ”€â”€ 01_data/                      # Dá»¯ liá»‡u
â”‚   â”œâ”€â”€ raw/                      # CSV gá»‘c (HI-Large_Trans.csv)
â”‚   â”œâ”€â”€ processed/                # Temp (tá»± Ä‘á»™ng xÃ³a sau upload HDFS)
â”‚   â””â”€â”€ results/                  # Káº¿t quáº£ (táº£i vá» tá»« HDFS)
â”œâ”€â”€ 02_scripts/                   # Scripts
â”‚   â”œâ”€â”€ polars/                   # Data processing
â”‚   â”‚   â”œâ”€â”€ 01_explore_fast.py
â”‚   â”‚   â”œâ”€â”€ 02_prepare_polars.py
â”‚   â”‚   â”œâ”€â”€ 04_assign_clusters.py
â”‚   â”‚   â””â”€â”€ 05_analyze.py
â”‚   â”œâ”€â”€ spark/                    # PySpark MLlib K-means
â”‚   â”‚   â”œâ”€â”€ setup_hdfs.sh
â”‚   â”‚   â”œâ”€â”€ run_spark.sh
â”‚   â”‚   â”œâ”€â”€ kmeans_spark.py
â”‚   â”‚   â””â”€â”€ download_from_hdfs.sh
â”‚   â”œâ”€â”€ pipeline/                 # Orchestration
â”‚   â”‚   â”œâ”€â”€ full_pipeline_spark.sh
â”‚   â”‚   â”œâ”€â”€ clean_spark.sh
â”‚   â”‚   â””â”€â”€ reset_pipeline.sh
â”‚   â”œâ”€â”€ setup/                    # Installation
â”‚   â”‚   â”œâ”€â”€ install_spark.sh
â”‚   â”‚   â””â”€â”€ setup_jupyter_kernel.sh
â”‚   â””â”€â”€ data/                     # Utilities
â”‚       â”œâ”€â”€ snapshot_results.py
â”‚       â””â”€â”€ visualize_results.py
â”œâ”€â”€ 03_docs/                      # TÃ i liá»‡u
â”‚   â”œâ”€â”€ cai-dat.md                # HÆ°á»›ng dáº«n cÃ i Ä‘áº·t
â”‚   â”œâ”€â”€ cau-truc.md               # Cáº¥u trÃºc dá»± Ã¡n
â”‚   â”œâ”€â”€ hadoop-alternatives.md    # So sÃ¡nh phÆ°Æ¡ng phÃ¡p
â”‚   â”œâ”€â”€ huong-dan.md              # HÆ°á»›ng dáº«n cháº¡y
â”‚   â”œâ”€â”€ jupyter.md                # Setup Jupyter
â”‚   â”œâ”€â”€ migration.md              # Migration guide
â”‚   â””â”€â”€ tong-quan.md              # Tá»•ng quan dá»± Ã¡n
â”œâ”€â”€ 04_logs/                      # Logs
â”œâ”€â”€ 05_snapshots/                 # Snapshots
â”œâ”€â”€ 06_visualizations/            # Visualization
â”‚   â”œâ”€â”€ phan-tich.ipynb           # Notebook phÃ¢n tÃ­ch
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ BAO_CAO_DU_AN.md              # BÃ¡o cÃ¡o chÃ­nh (gá»™p)
â”œâ”€â”€ changelog.md                  # Lá»‹ch sá»­ thay Ä‘á»•i
â”œâ”€â”€ README.md                     # File nÃ y
â””â”€â”€ requirements.txt              # Dependencies
```

<a id="cai-dat"></a>
## CÃ i Ä‘áº·t

### Apache Spark

```bash
# CÃ i Ä‘áº·t Spark (CachyOS/Arch Linux)
./scripts/setup/install_spark.sh

# Reload shell Ä‘á»ƒ Ã¡p dá»¥ng biáº¿n mÃ´i trÆ°á»ng
source ~/.zshrc

# Kiá»ƒm tra cÃ i Ä‘áº·t
spark-submit --version
```

### Python Dependencies

```bash
# KÃ­ch hoáº¡t virtual environment (náº¿u cÃ³)
source .venv/bin/activate

# CÃ i Ä‘áº·t packages
pip install polars numpy pyspark
```

<a id="chuan-bi-du-lieu"></a>
## Chuáº©n bá»‹ dá»¯ liá»‡u

Äáº·t file CSV gá»‘c vÃ o thÆ° má»¥c raw:

```bash
cp /path/to/HI-Large_Trans.csv 01_data/raw/
```

<a id="hdfs-workflow"></a>
## âš ï¸ QUAN TRá»ŒNG: HDFS-Only Workflow

Project nÃ y tuÃ¢n thá»§ quy táº¯c **KHÃ”NG lÆ°u dá»¯ liá»‡u lá»›n á»Ÿ local**.

### Workflow

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Raw CSV     â”‚ â”€â”€â”€> â”‚ Temp Files   â”‚ â”€â”€â”€> â”‚ HDFS        â”‚
â”‚ (01_data/   â”‚      â”‚ (táº¡m thá»i)   â”‚      â”‚ (permanent) â”‚
â”‚  raw/)      â”‚      â”‚              â”‚      â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚                      â”‚
                            â”‚ (auto delete)        â”‚
                            â–¼                      â–¼
                      [XÃ³a ngay sau            [Spark
                       khi upload]              processing]
```

### CÃ¡ch thá»©c hoáº¡t Ä‘á»™ng:

1. **Polars** Ä‘á»c CSV gá»‘c vÃ  táº¡o temp files trong `01_data/processed/`
2. **setup_hdfs.sh** upload files lÃªn HDFS vÃ  **tá»± Ä‘á»™ng xÃ³a** temp files
3. **Spark** xá»­ lÃ½ dá»¯ liá»‡u trá»±c tiáº¿p trÃªn HDFS (distributed)
4. Káº¿t quáº£ Ä‘Æ°á»£c lÆ°u trÃªn HDFS táº¡i `/user/spark/hi_large/`
5. *(TÃ¹y chá»n)* Táº£i káº¿t quáº£ nhá» vá» `01_data/results/` Ä‘á»ƒ phÃ¢n tÃ­ch

### LÆ°u Ã½

- âœ… Temp files tá»± Ä‘á»™ng bá»‹ xÃ³a sau khi upload HDFS
- âœ… Dá»¯ liá»‡u chÃ­nh chá»‰ tá»“n táº¡i trÃªn HDFS
- âœ… Chá»‰ lÆ°u káº¿t quáº£ phÃ¢n tÃ­ch nhá» á»Ÿ local (centroid, logs)
- âš ï¸ CÃ³ thá»ƒ sá»­a `HDFS_BASE` trong scripts náº¿u dÃ¹ng cluster khÃ¡c

<a id="chay-pipeline"></a>
## Cháº¡y Pipeline

### Quick Start

```bash
# Cháº¡y toÃ n bá»™ pipeline (V2 khuyáº¿n nghá»‹)
./02_scripts/pipeline/full_pipeline_spark_v2.sh

# TÃ¹y chá»n flags (KMeans):
#   --seed N       : Ä‘áº·t seed (vd 42)
#   --k N          : sá»‘ cá»¥m K (vd 5)
#   --max-iter N   : sá»‘ vÃ²ng láº·p tá»‘i Ä‘a (vd 15)
#   --tol FLOAT    : ngÆ°á»¡ng há»™i tá»¥ (vd 1e-4)
# Äiá»u khiá»ƒn luá»“ng:
#   --reset, --from-step N, --skip-step N, --dry-run

# VÃ­ dá»¥: K=6, maxIter=20, seed=33, tol=1e-5
./02_scripts/pipeline/full_pipeline_spark_v2.sh --k 6 --max-iter 20 --seed 33 --tol 1e-5
```

Pipeline sáº½ tá»± Ä‘á»™ng:
1. KhÃ¡m phÃ¡ dá»¯ liá»‡u vá»›i Polars
2. Chuáº©n bá»‹ features vÃ  normalize
3. Upload lÃªn HDFS (vÃ  xÃ³a temp files)
4. Cháº¡y K-means MLlib trÃªn Spark (âš¡ k-means++)
5. Táº£i káº¿t quáº£ vÃ  phÃ¢n tÃ­ch

### Manual Steps (náº¿u cáº§n debug)

```bash
# 1. KhÃ¡m phÃ¡ dá»¯ liá»‡u
python 02_scripts/polars/01_explore_fast.py

# 2. Chuáº©n bá»‹ features (táº¡o temp files)
python 02_scripts/polars/02_prepare_polars.py

# 3. Upload lÃªn HDFS vÃ  XÃ“A temp files
02_scripts/spark/setup_hdfs.sh

# 4. Cháº¡y Spark MLlib K-means trÃªn HDFS (âš¡ k-means++ auto)
02_scripts/spark/run_spark.sh

# 5. (TÃ¹y chá»n) Táº£i káº¿t quáº£ vá»
02_scripts/spark/download_from_hdfs.sh

# 6. GÃ¡n clusters
python 02_scripts/polars/04_assign_clusters.py

# 7. PhÃ¢n tÃ­ch káº¿t quáº£
python 02_scripts/polars/05_analyze.py

# 8. (TÃ¹y chá»n) Táº¡o snapshot káº¿t quáº£
python 02_scripts/data/snapshot_results.py

# 9. (TÃ¹y chá»n) Trá»±c quan hÃ³a
python 02_scripts/data/visualize_results.py
```

### Logs & Snapshots

Logs Ä‘Æ°á»£c lÆ°u táº¡i `04_logs/pipeline_log_*.md` vá»›i timestamp.
Snapshots Ä‘Æ°á»£c lÆ°u táº¡i `05_snapshots/snapshot_*/` vá»›i timestamp.
Visualization Ä‘Æ°á»£c lÆ°u táº¡i `06_visualizations/`.

#### Latest snapshot

- TÃªn: `snapshot_20251029_213229`
- Thá»i gian: `2025-10-29 21:32:30`
- KÃ­ch thÆ°á»›c: `342.75 MB`
- ÄÆ°á»ng dáº«n: `05_snapshots/snapshot_20251029_213229/`
- Files:
  - `final_centroids.txt` (436 bytes)
  - `clustered_results.txt` (342.75 MB)
  - `suspicious_transactions.csv` (558 bytes)
  - `pipeline_log.md`

Tham chiáº¿u: xem bÃ¡o cÃ¡o cáº­p nháº­t trong `bao_cao_du_an.md`.

<a id="don-dep"></a>
## Dá»n dáº¹p Project

```bash
# XÃ³a táº¥t cáº£ temp files, logs, vÃ  checkpoints
./02_scripts/pipeline/clean_spark.sh

# Reset chá»‰ pipeline checkpoints (giá»¯ láº¡i data)
./02_scripts/pipeline/reset_pipeline.sh

# Sau khi clean, cháº¡y láº¡i pipeline
./02_scripts/pipeline/full_pipeline_spark.sh
```

<a id="du-lieu-hdfs"></a>
## Dá»¯ liá»‡u trÃªn HDFS

### Cáº¥u trÃºc HDFS

```text
/user/spark/hi_large/
â”œâ”€â”€ input/
â”‚   â””â”€â”€ hadoop_input.txt      # Dá»¯ liá»‡u Ä‘Ã£ normalize (~33GB)
â””â”€â”€ output_centroids/         # Final centroids tá»« MLlib (k-means++)
    â””â”€â”€ part-00000
```

### Kiá»ƒm tra dá»¯ liá»‡u

```bash path=null start=null
# Xem cáº¥u trÃºc
hdfs dfs -ls /user/spark/hi_large/

# Kiá»ƒm tra kÃ­ch thÆ°á»›c
hdfs dfs -du -h /user/spark/hi_large/

# Xem ná»™i dung centroids
hdfs dfs -cat /user/spark/hi_large/output_centroids/part-00000
```

### Download káº¿t quáº£ (tÃ¹y chá»n)

Káº¿t quáº£ nhá» Ä‘Æ°á»£c táº£i vá» `01_data/results/` Ä‘á»ƒ phÃ¢n tÃ­ch local.

### Snapshots & Visualizations

```bash
# Táº¡o snapshot káº¿t quáº£ hiá»‡n táº¡i
python 02_scripts/data/snapshot_results.py

# Xem danh sÃ¡ch snapshots
python 02_scripts/data/snapshot_results.py --list

# Táº¡o biá»ƒu Ä‘á»“ trá»±c quan
python 02_scripts/data/visualize_results.py
```

<a id="chi-tiet-steps"></a>
## Chi tiáº¿t Pipeline Steps

| BÆ°á»›c | Script | MÃ´ táº£ | Thá»i gian |
|------|--------|-------|----------|
| 1 | `02_scripts/polars/01_explore_fast.py` | KhÃ¡m phÃ¡ dá»¯ liá»‡u nhanh | ~30s |
| 2 | `02_scripts/polars/02_prepare_polars.py` | Feature engineering & normalize | ~10 phÃºt |
| 3 | `02_scripts/spark/setup_hdfs.sh` | Upload HDFS & xÃ³a temp files | ~5 phÃºt |
| 4 | `02_scripts/spark/run_spark.sh` | K-means MLlib (âš¡ k-means++) | ~10-15 phÃºt |
| 5 | `02_scripts/spark/download_from_hdfs.sh` | Táº£i centroids tá»« HDFS | ~30s |
| 6 | `02_scripts/polars/04_assign_clusters.py` | GÃ¡n clusters cho data | ~10 phÃºt |
| 7 | `02_scripts/polars/05_analyze.py` | PhÃ¢n tÃ­ch & bÃ¡o cÃ¡o | ~2 phÃºt |
| 8 | `02_scripts/data/snapshot_results.py` | Snapshot káº¿t quáº£ | ~10s |
| 9 | `02_scripts/data/visualize_results.py` | Táº¡o biá»ƒu Ä‘á»“ trá»±c quan | ~2 phÃºt |

**Tá»•ng thá»i gian**: ~30-40 phÃºt (âš¡ Nhanh hÆ¡n 30-50% nhá» MLlib!)

<a id="kien-truc"></a>
## Kiáº¿n trÃºc há»‡ thá»‘ng

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     HDFS-Only Workflow                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    RAW CSV              POLARS              HDFS             SPARK
       â”‚                   â”‚                  â”‚                â”‚
   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚16GB CSVâ”‚â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ Temp   â”‚â”€â”€â”€â”€â”€â”€â”€>â”‚ 33GB   â”‚â”€â”€â”€â”€â”€â”€>â”‚K-means â”‚
   â”‚01_data/â”‚         â”‚ Files  â”‚ upload â”‚Storage â”‚ read  â”‚Cluster â”‚
   â”‚  raw/  â”‚         â”‚        â”‚        â”‚        â”‚       â”‚        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚                                   â”‚
                           â”‚ (auto delete)                     â”‚
                           â–¼                                   â–¼
                      [XÃ³a ngay]                       [Results HDFS]
                                                              â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                        â”‚                                     â”‚
                        â–¼                                     â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ 01_data/results/â”‚                  â”‚  05_snapshots/  â”‚
               â”‚  (small files)  â”‚                  â”‚  06_visualizations/â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Äáº·c Ä‘iá»ƒm

- âœ… **KHÃ”NG lÆ°u dá»¯ liá»‡u lá»›n local** - Temp files tá»± Ä‘á»™ng xÃ³a sau upload
- âœ… **Storage chá»‰ trÃªn HDFS** - TuÃ¢n thá»§ quy Ä‘á»‹nh khÃ´ng lÆ°u local  
- âœ… **Distributed processing** - Spark xá»­ lÃ½ song song trÃªn cluster
- âœ… **Scalable** - ThÃªm nodes Ä‘á»ƒ tÄƒng performance
- âœ… **Fault-tolerant** - HDFS replication Ä‘áº£m báº£o an toÃ n dá»¯ liá»‡u

<a id="so-sanh"></a>
## So sÃ¡nh Hadoop vs Spark

| TiÃªu chÃ­ | Hadoop MapReduce | Apache Spark (HDFS) |
|----------|------------------|---------------------|
| **Tá»‘c Ä‘á»™** | Cháº­m (Ä‘á»c/ghi disk) | Nhanh hÆ¡n 10-100x |
| **Storage** | HDFS | HDFS |
| **Processing** | Disk-based | In-memory |
| **Code** | DÃ i (mapper/reducer) | Ngáº¯n gá»n (PySpark API) |
| **PhÃ¹ há»£p** | Batch processing lá»›n | Iterative algorithms |

<a id="loi-ich"></a>
## Lá»£i Ã­ch Apache Spark

| TiÃªu chÃ­ | Lá»£i Ã­ch |
|----------|--------|
| âš¡ **Tá»‘c Ä‘á»™** | Nhanh hÆ¡n Hadoop 10-100x vá»›i K-means (in-memory) |
| ğŸ’¾ **Memory** | In-memory processing giáº£m I/O disk |
| ğŸ¯ **API** | PySpark DataFrame API Ä‘Æ¡n giáº£n, dá»… há»c |
| ğŸ”§ **Debug** | Local mode khÃ´ng cáº§n cluster Ä‘á»ƒ test |
| ğŸ“ˆ **Scale** | Horizontal scaling - thÃªm nodes dá»… dÃ ng |
| ğŸ›¡ï¸ **Production** | Fault-tolerant, mature ecosystem |

<a id="phuong-phap-khac"></a>
### So sÃ¡nh vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c:

Xem chi tiáº¿t táº¡i: [`03_docs/HADOOP_ALTERNATIVES.md`](03_docs/HADOOP_ALTERNATIVES.md)
