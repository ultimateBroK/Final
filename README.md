# Polars + PySpark Pipeline

Pipeline phÃ¢n tÃ­ch dá»¯ liá»‡u HI-Large_Trans.csv sá»­ dá»¥ng Polars vÃ  Apache Spark (PySpark).

> ğŸ“š Xem thÃªm: [HÆ¯á»šNG DáºªN CHáº Y](HUONG_DAN_CHAY.md) Â· [BÃO CÃO](BAO_CAO_TIEU_LUAN.md) Â· [HADOOP_ALTERNATIVES](docs/HADOOP_ALTERNATIVES.md)

## Má»¥c lá»¥c
- [NÃ¢ng cáº¥p tá»« Hadoop sang Spark](#nang-cap)
- [Cáº¥u trÃºc thÆ° má»¥c](#cau-truc)
- [CÃ i Ä‘áº·t](#cai-dat)
- [Chuáº©n bá»‹ dá»¯ liá»‡u](#chuan-bi-du-lieu)
- [HDFS-Only Workflow](#hdfs-workflow)
- [Cháº¡y Pipeline](#chay-pipeline)
- [Dá»n dáº¹p Project](#don-dep)
- [Dá»¯ liá»‡u trÃªn HDFS](#du-lieu-hdfs)
- [Chi tiáº¿t Pipeline Steps](#chi-tiet-steps)
- [Kiáº¿n trÃºc há»‡ thá»‘ng](#kien-truc)
- [So sÃ¡nh Hadoop vs Spark](#so-sanh)
- [Lá»£i Ã­ch Apache Spark](#loi-ich)
- [So sÃ¡nh vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c](#phuong-phap-khac)

<a id="nang-cap"></a>
## âš¡ NÃ¢ng cáº¥p tá»« Hadoop sang Spark

Project Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t Ä‘á»ƒ sá»­ dá»¥ng **Apache Spark** thay vÃ¬ Hadoop MapReduce, giÃºp:
- âš¡ **Xá»­ lÃ½ nhanh hÆ¡n** nhiá»u láº§n (in-memory computing)
- ğŸ”§ **Dá»… cÃ i Ä‘áº·t hÆ¡n** (khÃ´ng cáº§n HDFS)
- ğŸ“ **Code Ä‘Æ¡n giáº£n hÆ¡n** (PySpark API)
- ğŸš€ **Scale tá»‘t hÆ¡n** vá»›i big data

<a id="cau-truc"></a>
## Cáº¥u trÃºc thÆ° má»¥c

```
Final/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # CSV gá»‘c (HI-Large_Trans.csv)
â”‚   â”œâ”€â”€ processed/              # Temp files (tá»± Ä‘á»™ng xÃ³a sau upload HDFS)
â”‚   â””â”€â”€ results/                # Káº¿t quáº£ tá»« HDFS (tÃ¹y chá»n táº£i vá»)
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ HADOOP_ALTERNATIVES.md  # So sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p clustering
â”‚   â””â”€â”€ Polars_Hadoop_HI_Large.md  # Legacy Hadoop workflow
â”œâ”€â”€ logs/                       # Pipeline execution logs
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ polars/                 # Data processing vá»›i Polars
â”‚   â”‚   â”œâ”€â”€ explore_fast.py
â”‚   â”‚   â”œâ”€â”€ prepare_polars.py
â”‚   â”‚   â”œâ”€â”€ init_centroids.py
â”‚   â”‚   â”œâ”€â”€ assign_clusters_polars.py
â”‚   â”‚   â””â”€â”€ analyze_polars.py
â”‚   â”œâ”€â”€ spark/                  # PySpark implementation
â”‚   â”‚   â”œâ”€â”€ setup_hdfs.sh
â”‚   â”‚   â”œâ”€â”€ run_spark.sh
â”‚   â”‚   â”œâ”€â”€ kmeans_spark.py
â”‚   â”‚   â””â”€â”€ download_from_hdfs.sh
â”‚   â”œâ”€â”€ pipeline/               # Pipeline orchestration
â”‚   â”‚   â”œâ”€â”€ full_pipeline_spark.sh
â”‚   â”‚   â”œâ”€â”€ clean_spark.sh
â”‚   â”‚   â””â”€â”€ reset_pipeline.sh
â”‚   â””â”€â”€ setup/                  # Installation
â”‚       â””â”€â”€ install_spark.sh
â”œâ”€â”€ archive/
â”‚   â””â”€â”€ hadoop/                 # Legacy MapReduce code
â”œâ”€â”€ CHANGELOG.md
â””â”€â”€ README.md
```

<a id="cai-dat"></a>
## CÃ i Ä‘áº·t

### Apache Spark

```bash
# CÃ i Ä‘áº·t Spark (CachyOS/Arch Linux)
./scripts/setup/install_spark.sh

# Reload shell Ä‘á»ƒ Ã¡p dá»¥ng biáº¿n mÃ´i trÆ°á»ng
source ~/.zshrc

# Kiá»ƒm tra cÃ i Ä‘áº·t
spark-submit --version
```

### Python Dependencies

```bash
# KÃ­ch hoáº¡t virtual environment (náº¿u cÃ³)
source .venv/bin/activate

# CÃ i Ä‘áº·t packages
pip install polars numpy pyspark
```

<a id="chuan-bi-du-lieu"></a>
## Chuáº©n bá»‹ dá»¯ liá»‡u

Äáº·t file CSV gá»‘c vÃ o thÆ° má»¥c raw:

```bash
cp /path/to/HI-Large_Trans.csv data/raw/
```

<a id="hdfs-workflow"></a>
## âš ï¸ QUAN TRá»ŒNG: HDFS-Only Workflow

Project nÃ y tuÃ¢n thá»§ quy táº¯c **KHÃ”NG lÆ°u dá»¯ liá»‡u lá»›n á»Ÿ local**.

### Workflow:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Raw CSV     â”‚ â”€â”€â”€> â”‚ Temp Files   â”‚ â”€â”€â”€> â”‚ HDFS        â”‚
â”‚ (data/raw/) â”‚      â”‚ (táº¡m thá»i)   â”‚      â”‚ (permanent) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚                      â”‚
                            â”‚ (auto delete)        â”‚
                            â–¼                      â–¼
                      [XÃ³a ngay sau            [Spark
                       khi upload]              processing]
```

### CÃ¡ch thá»©c hoáº¡t Ä‘á»™ng:

1. **Polars** Ä‘á»c CSV gá»‘c vÃ  táº¡o temp files trong `data/processed/`
2. **setup_hdfs.sh** upload files lÃªn HDFS vÃ  **tá»± Ä‘á»™ng xÃ³a** temp files
3. **Spark** xá»­ lÃ½ dá»¯ liá»‡u trá»±c tiáº¿p trÃªn HDFS (distributed)
4. Káº¿t quáº£ Ä‘Æ°á»£c lÆ°u trÃªn HDFS táº¡i `/user/spark/hi_large/`
5. *(TÃ¹y chá»n)* Táº£i káº¿t quáº£ nhá» vá» `data/results/` Ä‘á»ƒ phÃ¢n tÃ­ch

### LÆ°u Ã½:

- âœ… Temp files tá»± Ä‘á»™ng bá»‹ xÃ³a sau khi upload HDFS
- âœ… Dá»¯ liá»‡u chÃ­nh chá»‰ tá»“n táº¡i trÃªn HDFS
- âœ… Chá»‰ lÆ°u káº¿t quáº£ phÃ¢n tÃ­ch nhá» á»Ÿ local (centroid, logs)
- âš ï¸ CÃ³ thá»ƒ sá»­a `HDFS_BASE` trong scripts náº¿u dÃ¹ng cluster khÃ¡c

<a id="chay-pipeline"></a>
## Cháº¡y Pipeline

### Quick Start

```bash
# Cháº¡y toÃ n bá»™ pipeline tá»± Ä‘á»™ng
./scripts/pipeline/full_pipeline_spark.sh
```

Pipeline sáº½ tá»± Ä‘á»™ng:
1. KhÃ¡m phÃ¡ dá»¯ liá»‡u vá»›i Polars
2. Chuáº©n bá»‹ features vÃ  normalize
3. Khá»Ÿi táº¡o centroids
4. Upload lÃªn HDFS (vÃ  xÃ³a temp files)
5. Cháº¡y K-means trÃªn Spark
6. Táº£i káº¿t quáº£ vÃ  phÃ¢n tÃ­ch

### Manual Steps (náº¿u cáº§n debug)

```bash
# 1. KhÃ¡m phÃ¡ dá»¯ liá»‡u
python scripts/polars/explore_fast.py

# 2. Chuáº©n bá»‹ features (táº¡o temp files)
python scripts/polars/prepare_polars.py

# 3. Khá»Ÿi táº¡o centroids (táº¡o temp files)
python scripts/polars/init_centroids.py

# 4. Upload lÃªn HDFS vÃ  XÃ“A temp files
scripts/spark/setup_hdfs.sh

# 5. Cháº¡y Spark K-means trÃªn HDFS
scripts/spark/run_spark.sh

# 6. (TÃ¹y chá»n) Táº£i káº¿t quáº£ vá»
scripts/spark/download_from_hdfs.sh

# 7. GÃ¡n clusters
python scripts/polars/assign_clusters_polars.py

# 8. PhÃ¢n tÃ­ch káº¿t quáº£
python scripts/polars/analyze_polars.py
```

### Logs

Logs Ä‘Æ°á»£c lÆ°u táº¡i `logs/pipeline_log_*.md` vá»›i timestamp.

<a id="don-dep"></a>
## Dá»n dáº¹p Project

```bash
# XÃ³a táº¥t cáº£ temp files, logs, vÃ  checkpoints
./scripts/pipeline/clean_spark.sh

# Reset chá»‰ pipeline checkpoints (giá»¯ láº¡i data)
./scripts/pipeline/reset_pipeline.sh

# Sau khi clean, cháº¡y láº¡i pipeline
./scripts/pipeline/full_pipeline_spark.sh
```

<a id="du-lieu-hdfs"></a>
## Dá»¯ liá»‡u trÃªn HDFS

### Cáº¥u trÃºc HDFS:

```
/user/spark/hi_large/
â”œâ”€â”€ input/
â”‚   â””â”€â”€ hadoop_input.txt      # Dá»¯ liá»‡u Ä‘Ã£ normalize (~33GB)
â”œâ”€â”€ centroids.txt             # K centroids ban Ä‘áº§u
â””â”€â”€ output_centroids/         # Final centroids tá»« Spark
    â””â”€â”€ part-00000
```

### Kiá»ƒm tra dá»¯ liá»‡u:

```bash
# Xem cáº¥u trÃºc
hdfs dfs -ls /user/spark/hi_large/

# Kiá»ƒm tra kÃ­ch thÆ°á»›c
hdfs dfs -du -h /user/spark/hi_large/

# Xem ná»™i dung centroids
hdfs dfs -cat /user/spark/hi_large/output_centroids/part-00000
```

### Download káº¿t quáº£ (tÃ¹y chá»n):

Káº¿t quáº£ nhá» Ä‘Æ°á»£c táº£i vá» `data/results/` Ä‘á»ƒ phÃ¢n tÃ­ch local.

<a id="chi-tiet-steps"></a>
## Chi tiáº¿t Pipeline Steps

| BÆ°á»›c | Script | MÃ´ táº£ | Thá»i gian |
|------|--------|-------|----------|
| 1 | `scripts/polars/explore_fast.py` | KhÃ¡m phÃ¡ dá»¯ liá»‡u nhanh | ~30s |
| 2 | `scripts/polars/prepare_polars.py` | Feature engineering & normalize | ~10 phÃºt |
| 3 | `scripts/polars/init_centroids.py` | Khá»Ÿi táº¡o K centroids | ~30s |
| 4 | `scripts/spark/setup_hdfs.sh` | Upload HDFS & xÃ³a temp files | ~5 phÃºt |
| 5 | `scripts/spark/run_spark.sh` | K-means trÃªn Spark (HDFS) | ~15-30 phÃºt |
| 6 | `scripts/spark/download_from_hdfs.sh` | Táº£i centroids tá»« HDFS | ~30s |
| 7 | `scripts/polars/assign_clusters_polars.py` | GÃ¡n clusters cho data | ~10 phÃºt |
| 8 | `scripts/polars/analyze_polars.py` | PhÃ¢n tÃ­ch & bÃ¡o cÃ¡o | ~2 phÃºt |

**Tá»•ng thá»i gian**: ~40-60 phÃºt (tÃ¹y cluster configuration)

<a id="kien-truc"></a>
## Kiáº¿n trÃºc há»‡ thá»‘ng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     HDFS-Only Workflow                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    RAW CSV              POLARS              HDFS             SPARK
       â”‚                   â”‚                  â”‚                â”‚
   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚16GB CSVâ”‚â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ Temp   â”‚â”€â”€â”€â”€â”€â”€â”€>â”‚ 33GB   â”‚â”€â”€â”€â”€â”€â”€>â”‚K-means â”‚
   â”‚data/rawâ”‚         â”‚ Files  â”‚ upload â”‚Storage â”‚ read  â”‚Cluster â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚                                   â”‚
                           â”‚ (auto delete)                     â”‚
                           â–¼                                   â–¼
                      [XÃ³a ngay]                       [Results HDFS]
                                                              â”‚
                                                              â”‚
                                                              â–¼
                                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                      â”‚  data/results/ â”‚
                                                      â”‚  (small files) â”‚
                                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Äáº·c Ä‘iá»ƒm:

- âœ… **KHÃ”NG lÆ°u dá»¯ liá»‡u lá»›n local** - Temp files tá»± Ä‘á»™ng xÃ³a sau upload
- âœ… **Storage chá»‰ trÃªn HDFS** - TuÃ¢n thá»§ quy Ä‘á»‹nh khÃ´ng lÆ°u local  
- âœ… **Distributed processing** - Spark xá»­ lÃ½ song song trÃªn cluster
- âœ… **Scalable** - ThÃªm nodes Ä‘á»ƒ tÄƒng performance
- âœ… **Fault-tolerant** - HDFS replication Ä‘áº£m báº£o an toÃ n dá»¯ liá»‡u

<a id="so-sanh"></a>
## So sÃ¡nh Hadoop vs Spark

| TiÃªu chÃ­ | Hadoop MapReduce | Apache Spark (HDFS) |
|----------|------------------|---------------------|
| **Tá»‘c Ä‘á»™** | Cháº­m (Ä‘á»c/ghi disk) | Nhanh hÆ¡n 10-100x |
| **Storage** | HDFS | HDFS |
| **Processing** | Disk-based | In-memory |
| **Code** | DÃ i (mapper/reducer) | Ngáº¯n gá»n (PySpark API) |
| **PhÃ¹ há»£p** | Batch processing lá»›n | Iterative algorithms |

<a id="loi-ich"></a>
## Lá»£i Ã­ch Apache Spark

| TiÃªu chÃ­ | Lá»£i Ã­ch |
|----------|--------|
| âš¡ **Tá»‘c Ä‘á»™** | Nhanh hÆ¡n Hadoop 10-100x vá»›i K-means (in-memory) |
| ğŸ’¾ **Memory** | In-memory processing giáº£m I/O disk |
| ğŸ¯ **API** | PySpark DataFrame API Ä‘Æ¡n giáº£n, dá»… há»c |
| ğŸ”§ **Debug** | Local mode khÃ´ng cáº§n cluster Ä‘á»ƒ test |
| ğŸ“ˆ **Scale** | Horizontal scaling - thÃªm nodes dá»… dÃ ng |
| ğŸ›¡ï¸ **Production** | Fault-tolerant, mature ecosystem |

<a id="phuong-phap-khac"></a>
### So sÃ¡nh vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c:

Xem chi tiáº¿t táº¡i: [`docs/HADOOP_ALTERNATIVES.md`](docs/HADOOP_ALTERNATIVES.md)
