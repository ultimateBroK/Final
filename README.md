# Polars + PySpark Pipeline

Pipeline phÃ¢n tÃ­ch dá»¯ liá»‡u HI-Large_Trans.csv sá»­ dá»¥ng Polars vÃ  Apache Spark (PySpark).

## âš¡ NÃ¢ng cáº¥p tá»« Hadoop sang Spark

Project Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t Ä‘á»ƒ sá»­ dá»¥ng **Apache Spark** thay vÃ¬ Hadoop MapReduce, giÃºp:
- âš¡ **Xá»­ lÃ½ nhanh hÆ¡n** nhiá»u láº§n (in-memory computing)
- ğŸ”§ **Dá»… cÃ i Ä‘áº·t hÆ¡n** (khÃ´ng cáº§n HDFS)
- ğŸ“ **Code Ä‘Æ¡n giáº£n hÆ¡n** (PySpark API)
- ğŸš€ **Scale tá»‘t hÆ¡n** vá»›i big data

## Cáº¥u trÃºc thÆ° má»¥c (ADHD-friendly)

```
data/
  raw/                # Input gá»‘c (CSV lá»›n)
  processed/          # Temp files (xÃ³a sau khi upload HDFS)
  results/            # Káº¿t quáº£ táº£i vá» tá»« HDFS (tÃ¹y chá»n)
docs/                 # TÃ i liá»‡u
logs/                 # Log pipeline
scripts/
  polars/             # Data processing vá»›i Polars
    â””â”€ explore_fast.py, prepare_polars.py, init_centroids.py,
       assign_clusters_polars.py, analyze_polars.py
  spark/              # PySpark K-means implementation
    â””â”€ setup_hdfs.sh, run_spark.sh, kmeans_spark.py,
       download_from_hdfs.sh
  pipeline/           # Pipeline orchestration
    â””â”€ full_pipeline_spark.sh, clean_spark.sh, reset_pipeline.sh
  setup/              # Installation scripts
    â””â”€ install_spark.sh
archive/              # Legacy Hadoop code (khÃ´ng dÃ¹ng)
```

## CÃ i Ä‘áº·t Apache Spark

```bash
# Cháº¡y script cÃ i Ä‘áº·t (CachyOS/Arch Linux)
./scripts/setup/install_spark.sh

# Sau khi cÃ i Ä‘áº·t, reload shell
source ~/.zshrc

# Kiá»ƒm tra cÃ i Ä‘áº·t
spark-submit --version
```

## Chuáº©n bá»‹ dá»¯ liá»‡u

- Äáº·t file CSV vÃ o: `data/raw/HI-Large_Trans.csv`

## âš ï¸ QUAN TRá»ŒNG: Dá»¯ liá»‡u chá»‰ lÆ°u trÃªn HDFS

Project nÃ y **KHÃ”NG lÆ°u dá»¯ liá»‡u lá»›n á»Ÿ local**. Táº¥t cáº£ dá»¯ liá»‡u Ä‘á»u Ä‘Æ°á»£c xá»­ lÃ½ vÃ  lÆ°u trÃªn HDFS.

### Quy trÃ¬nh lÃ m viá»‡c:

```bash
# 1) Chuáº©n bá»‹ dá»¯ liá»‡u (táº¡o temp file)
cd scripts/polars
python prepare_polars.py
python init_centroids.py

# 2) Upload lÃªn HDFS vÃ  XÃ“A temp files local
../spark/setup_hdfs.sh

# 3) Cháº¡y Spark trÃªn HDFS (YARN/cluster)
../spark/run_spark.sh

# 4) (Tuá»³ chá»n) Táº£i káº¿t quáº£ vá» Ä‘á»ƒ phÃ¢n tÃ­ch
../spark/download_from_hdfs.sh
```

**LÆ°u Ã½:**
- Temp files sáº½ tá»± Ä‘á»™ng bá»‹ XÃ“A sau khi upload lÃªn HDFS
- Dá»¯ liá»‡u chá»‰ tá»“n táº¡i trÃªn HDFS, khÃ´ng trÃªn local
- Báº¡n cÃ³ thá»ƒ sá»­a `HDFS_BASE` trong cÃ¡c script náº¿u dÃ¹ng cluster khÃ¡c

## Cháº¡y Pipeline

```bash
# ToÃ n bá»™ pipeline vá»›i PySpark + HDFS
./scripts/pipeline/full_pipeline_spark.sh
```

Log sáº½ lÆ°u táº¡i `logs/pipeline_log_*.md`.

**Hadoop legacy code Ä‘Ã£ Ä‘Æ°á»£c archive** - khÃ´ng cÃ²n sá»­ dá»¥ng ná»¯a.

## Clean Project

```bash
# Dá»n dáº¹p toÃ n bá»™ Ä‘á»ƒ cháº¡y láº¡i tá»« Ä‘áº§u
./scripts/pipeline/clean_spark.sh

# Sau Ä‘Ã³ cháº¡y láº¡i pipeline
./scripts/pipeline/full_pipeline_spark.sh
```

## Dá»¯ liá»‡u trÃªn HDFS

Táº¥t cáº£ dá»¯ liá»‡u Ä‘Æ°á»£c lÆ°u trÃªn HDFS táº¡i: `/user/spark/hi_large/`

- `input/hadoop_input.txt` - Dá»¯ liá»‡u Ä‘Ã£ normalize
- `centroids.txt` - Centroids ban Ä‘áº§u  
- `output_centroids/` - Centroids cuá»‘i cÃ¹ng tá»« Spark
- Káº¿t quáº£ Ä‘Æ°á»£c táº£i vá» `data/results/` khi cáº§n phÃ¢n tÃ­ch

## Pipeline Steps

1. **`scripts/polars/explore_fast.py`** - KhÃ¡m phÃ¡ dá»¯ liá»‡u
2. **`scripts/polars/prepare_polars.py`** - Feature engineering (táº¡o temp file)
3. **`scripts/polars/init_centroids.py`** - Khá»Ÿi táº¡o centroids (temp file)
4. **`scripts/spark/setup_hdfs.sh`** - Upload lÃªn HDFS & xÃ³a temp files ğŸ’¾
5. **`scripts/spark/run_spark.sh`** - K-means vá»›i PySpark trÃªn HDFS âš¡
6. **`scripts/spark/download_from_hdfs.sh`** - Táº£i káº¿t quáº£ vá» (tÃ¹y chá»n)
7. **`scripts/polars/assign_clusters_polars.py`** - GÃ¡n nhÃ£n cluster
8. **`scripts/polars/analyze_polars.py`** - PhÃ¢n tÃ­ch káº¿t quáº£

## Kiáº¿n trÃºc: HDFS-Only Workflow

```
[Temp Local] --> [Upload + Delete] --> [Spark on HDFS] --> [Results on HDFS]
      â”‚                   â”‚                     â”‚                    â”‚
   Polars          setup_hdfs.sh           PySpark         (download náº¿u cáº§n)
  (temp file)      (xÃ³a ngay)           (in-memory)      
```

- âœ… **KHÃ”NG lÆ°u dá»¯ liá»‡u lá»›n local** - Temp files tá»± Ä‘á»™ng xÃ³a
- âœ… **Dá»¯ liá»‡u chá»‰ trÃªn HDFS** - TuÃ¢n thá»§ quy táº¯c lÆ°u trá»¯
- âœ… **Distributed processing** - Spark xá»­ lÃ½ song song
- âœ… **Scalable** - ThÃªm nodes Ä‘á»ƒ tÄƒng performance

## So sÃ¡nh Hadoop vs Spark

| TiÃªu chÃ­ | Hadoop MapReduce | Apache Spark (HDFS) |
|----------|------------------|---------------------|
| **Tá»‘c Ä‘á»™** | Cháº­m (Ä‘á»c/ghi disk) | Nhanh hÆ¡n 10-100x |
| **Storage** | HDFS | HDFS |
| **Processing** | Disk-based | In-memory |
| **Code** | DÃ i (mapper/reducer) | Ngáº¯n gá»n (PySpark API) |
| **PhÃ¹ há»£p** | Batch processing lá»›n | Iterative algorithms |

## Lá»£i Ã­ch khi dÃ¹ng Spark

- âš¡ **Nhanh hÆ¡n 10-100x** vá»›i iterative algorithms nhÆ° K-means
- ğŸ’¾ **In-memory processing** - giáº£m I/O disk
- ğŸ¯ **API Ä‘Æ¡n giáº£n** - code ngáº¯n gá»n hÆ¡n
- ğŸ”§ **Dá»… debug** - cháº¡y local khÃ´ng cáº§n cluster
