# Polars + PySpark Pipeline Execution Log

**Thời gian bắt đầu:** 2025-10-29 01:44:14
**File log:** /home/ultimatebrok/Downloads/Final/logs/pipeline_log_20251029_014414.md

---

## Thực thi Pipeline

=== POLARS + PYSPARK PIPELINE ===
Thời gian bắt đầu: 2025-10-29 01:44:14
### Bước 1: Khám phá dữ liệu

======================================================================
🔍 BƯỚC 1: KHÁM PHÁ DỮ LIỆU
======================================================================
Đang đọc file: /home/ultimatebrok/Downloads/Final/data/raw/HI-Large_Trans.csv
Vui lòng đợi...

✅ Đã load metadata thành công!

📋 SCHEMA (Cấu trúc dữ liệu):
----------------------------------------------------------------------
<bound method LazyFrame.collect_schema of <LazyFrame at 0x7F84771BAC60>>

📊 LẤY MẪU 100,000 DÒNG ĐẦU:
----------------------------------------------------------------------
Dữ liệu mẫu:
shape: (100_000, 11)
┌────────────┬───────────┬───────────┬─────────┬───┬───────────┬───────────┬───────────┬───────────┐
│ Timestamp  ┆ From Bank ┆ Account   ┆ To Bank ┆ … ┆ Amount    ┆ Payment   ┆ Payment   ┆ Is Launde │
│ ---        ┆ ---       ┆ ---       ┆ ---     ┆   ┆ Paid      ┆ Currency  ┆ Format    ┆ ring      │
│ str        ┆ i64       ┆ str       ┆ i64     ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │
│            ┆           ┆           ┆         ┆   ┆ f64       ┆ str       ┆ str       ┆ i64       │
╞════════════╪═══════════╪═══════════╪═════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡
│ 2022/08/01 ┆ 20        ┆ 800104D70 ┆ 20      ┆ … ┆ 6794.63   ┆ US Dollar ┆ Reinvestm ┆ 0         │
│ 00:17      ┆           ┆           ┆         ┆   ┆           ┆           ┆ ent       ┆           │
│ 2022/08/01 ┆ 3196      ┆ 800107150 ┆ 3196    ┆ … ┆ 7739.29   ┆ US Dollar ┆ Reinvestm ┆ 0         │
│ 00:02      ┆           ┆           ┆         ┆   ┆           ┆           ┆ ent       ┆           │
│ 2022/08/01 ┆ 1208      ┆ 80010E430 ┆ 1208    ┆ … ┆ 1880.23   ┆ US Dollar ┆ Reinvestm ┆ 0         │
│ 00:17      ┆           ┆           ┆         ┆   ┆           ┆           ┆ ent       ┆           │
│ 2022/08/01 ┆ 1208      ┆ 80010E650 ┆ 20      ┆ … ┆ 7.3966883 ┆ US Dollar ┆ Cheque    ┆ 0         │
│ 00:03      ┆           ┆           ┆         ┆   ┆ e7        ┆           ┆           ┆           │
│ 2022/08/01 ┆ 1208      ┆ 80010E650 ┆ 20      ┆ … ┆ 4.5868454 ┆ US Dollar ┆ Cheque    ┆ 0         │
│ 00:02      ┆           ┆           ┆         ┆   ┆ e7        ┆           ┆           ┆           │
│ …          ┆ …         ┆ …         ┆ …       ┆ … ┆ …         ┆ …         ┆ …         ┆ …         │
│ 2022/08/01 ┆ 111667    ┆ 80E5D9320 ┆ 111667  ┆ … ┆ 23.93     ┆ US Dollar ┆ Reinvestm ┆ 0         │
│ 00:06      ┆           ┆           ┆         ┆   ┆           ┆           ┆ ent       ┆           │
│ 2022/08/01 ┆ 25994     ┆ 80E5DBF50 ┆ 25994   ┆ … ┆ 23.41     ┆ US Dollar ┆ Reinvestm ┆ 0         │
│ 00:18      ┆           ┆           ┆         ┆   ┆           ┆           ┆ ent       ┆           │
│ 2022/08/01 ┆ 310328    ┆ 80E5ED350 ┆ 310328  ┆ … ┆ 3297.82   ┆ US Dollar ┆ Reinvestm ┆ 0         │
│ 00:04      ┆           ┆           ┆         ┆   ┆           ┆           ┆ ent       ┆           │
│ 2022/08/01 ┆ 5838      ┆ 80E5ED580 ┆ 5838    ┆ … ┆ 3032.53   ┆ US Dollar ┆ Reinvestm ┆ 0         │
│ 00:19      ┆           ┆           ┆         ┆   ┆           ┆           ┆ ent       ┆           │
│ 2022/08/01 ┆ 339249    ┆ 80E5ED620 ┆ 339249  ┆ … ┆ 13043.39  ┆ US Dollar ┆ Reinvestm ┆ 0         │
│ 00:26      ┆           ┆           ┆         ┆   ┆           ┆           ┆ ent       ┆           │
└────────────┴───────────┴───────────┴─────────┴───┴───────────┴───────────┴───────────┴───────────┘

Thống kê mô tả:
shape: (9, 12)
┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐
│ statistic ┆ Timestamp ┆ From Bank ┆ Account   ┆ … ┆ Amount    ┆ Payment   ┆ Payment   ┆ Is Laund │
│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ Paid      ┆ Currency  ┆ Format    ┆ ering    │
│ str       ┆ str       ┆ f64       ┆ str       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │
│           ┆           ┆           ┆           ┆   ┆ f64       ┆ str       ┆ str       ┆ f64      │
╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡
│ count     ┆ 100000    ┆ 100000.0  ┆ 100000    ┆ … ┆ 100000.0  ┆ 100000    ┆ 100000    ┆ 100000.0 │
│ null_coun ┆ 0         ┆ 0.0       ┆ 0         ┆ … ┆ 0.0       ┆ 0         ┆ 0         ┆ 0.0      │
│ t         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │
│ mean      ┆ null      ┆ 49669.967 ┆ null      ┆ … ┆ 1.1422e6  ┆ null      ┆ null      ┆ 0.00001  │
│           ┆           ┆ 7         ┆           ┆   ┆           ┆           ┆           ┆          │
│ std       ┆ null      ┆ 80891.260 ┆ null      ┆ … ┆ 2.9890e7  ┆ null      ┆ null      ┆ 0.003162 │
│           ┆           ┆ 568       ┆           ┆   ┆           ┆           ┆           ┆          │
│ min       ┆ 2022/08/0 ┆ 0.0       ┆ 100428660 ┆ … ┆ 0.01      ┆ Australia ┆ ACH       ┆ 0.0      │
│           ┆ 1 00:00   ┆           ┆           ┆   ┆           ┆ n Dollar  ┆           ┆          │
│ 25%       ┆ null      ┆ 4214.0    ┆ null      ┆ … ┆ 20.53     ┆ null      ┆ null      ┆ 0.0      │
│ 50%       ┆ null      ┆ 15240.0   ┆ null      ┆ … ┆ 2513.06   ┆ null      ┆ null      ┆ 0.0      │
│ 75%       ┆ null      ┆ 31996.0   ┆ null      ┆ … ┆ 28769.41  ┆ null      ┆ null      ┆ 0.0      │
│ max       ┆ 2022/08/0 ┆ 339249.0  ┆ 80E5ED620 ┆ … ┆ 5.1154e9  ┆ Yuan      ┆ Wire      ┆ 1.0      │
│           ┆ 1 00:29   ┆           ┆           ┆   ┆           ┆           ┆           ┆          │
└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘

💰 PHÂN TÍCH TỶ LỆ RỬA TIỀN:
----------------------------------------------------------------------
shape: (2, 1)
┌───────────────┐
│ Is Laundering │
│ ---           │
│ struct[2]     │
╞═══════════════╡
│ {1,225546}    │
│ {0,179476683} │
└───────────────┘

💵 TOP 10 LOẠI TIỀN TỆ PHỔ BIẾN:
----------------------------------------------------------------------
shape: (10, 1)
┌──────────────────────────┐
│ Receiving Currency       │
│ ---                      │
│ struct[2]                │
╞══════════════════════════╡
│ {"Rupee",4178243}        │
│ {"Saudi Riyal",3228262}  │
│ {"Yen",4841570}          │
│ {"US Dollar",65292945}   │
│ {"Mexican Peso",4801493} │
│ {"Swiss Franc",4829099}  │
│ {"Euro",41290069}        │
│ {"Bitcoin",3958153}      │
│ {"Ruble",5571567}        │
│ {"Shekel",8047876}       │
└──────────────────────────┘

======================================================================
✅ HOÀN TẤT KHÁM PHÁ DỮ LIỆU!
======================================================================

💡 GỢI Ý TIẾP THEO:
   Chạy bước 2: python scripts/polars/prepare_polars.py

⏱️  **Bước 1 hoàn thành trong 13s**

### Bước 2: Chuẩn bị đặc trưng với Polars

======================================================================
🔧 BƯỚC 2: XỬ LÝ VÀ TRÍCH XUẤT ĐẶC TRƯNG
======================================================================
Đọc file: /home/ultimatebrok/Downloads/Final/data/raw/HI-Large_Trans.csv
Vui lòng đợi (có thể mất 5-10 phút)...

✅ Đã load 179,702,229 dòng vào RAM

🌟 TRÍCH XUẤT ĐẶC TRƯNG TỪ DỮ LIỆU THÔ...
✅ Đã trích xuất 10 đặc trưng

🔢 MÃ HÓA BIẾN PHÂN LOẠI (CATEGORICAL ENCODING)...
✅ Đã mã hóa thành số

📊 Có 9 đặc trưng số cho K-means

📊 CHUẨN HÓA DỮ LIỆU (Min-Max Scaling)...
✅ Đã chuẩn hóa 9 đặc trưng

💾 LƯU FILE TẠM THỜI CHO HDFS...
⚠️  LƯU Ý: File này sẽ tự động xóa sau khi upload HDFS!

   Đang ghi: /home/ultimatebrok/Downloads/Final/data/processed/hadoop_input_temp.txt
   Vui lòng đợi (có thể mất 3-5 phút)...

======================================================================
✅ HOÀN TẤT XỬ LÝ DỮ LIỆU!
======================================================================
📄 File tạm: /home/ultimatebrok/Downloads/Final/data/processed/hadoop_input_temp.txt
📊 Kích thước: 31.07 GB
📊 Số dòng: 179,702,229
📊 Số đặc trưng: 9
📊 Các đặc trưng: ['amount_received', 'amount_paid', 'amount_ratio', 'hour', 'day_of_week', 'route_hash', 'recv_curr_encoded', 'payment_curr_encoded', 'payment_format_encoded']

⚠️  QUAN TRỌNG:
   File này chỉ tồn tại TẠM THỜI!
   Nó sẽ BỊ XÓA sau khi upload lên HDFS (Bước 4)
   Dữ liệu chỉ lưu trên HDFS để tuân thủ quy định bảo mật

💡 GỢI Ý TIẾP THEO:
   Chạy bước 3: python scripts/polars/init_centroids.py

⏱️  **Bước 2 hoàn thành trong 1m 5s**

### Bước 3: Khởi tạo tâm cụm

======================================================================
🎯 BƯỚC 3: KHỚI TẠO TÂM CỤM BAN ĐẦU
======================================================================

Kiểm tra file input: /home/ultimatebrok/Downloads/Final/data/processed/hadoop_input_temp.txt
✅ File input tồn tại!

📊 ĐỌC DỮ LIỆU ĐÃ CHUẨN HÓA...
✅ Đã load 179,702,229 dòng với 9 đặc trưng

🎲 LẤY MẪU NGẪU NHIÊN ĐỂ KHỚI TẠO...
   Số mẫu: 100,000 dòng
✅ Đã lấy mẫu 100,000 điểm

🎯 KHỚI TẠO 5 TÂM CỤM BAN ĐẦU...
   Thuật toán: Chọn ngẫu nhiên 5 điểm từ 100,000 mẫu

✅ Đã chọn 5 tâm cụm ban đầu
   Mỗi tâm cụm có 9 đặc trưng

💾 LƯU TÂM CỤM VÀO FILE TẠM THỜI...
⚠️  LƯU Ý: File này sẽ tự động xóa sau khi upload HDFS!

   Đang ghi: /home/ultimatebrok/Downloads/Final/data/processed/centroids_temp.txt
======================================================================
✅ HOÀN TẤT KHỚI TẠO TÂM CỤM!
======================================================================
📄 File tạm: /home/ultimatebrok/Downloads/Final/data/processed/centroids_temp.txt
📊 Kích thước: 433 bytes (~0.4 KB)
📊 Số cụm: 5
📊 Số đặc trưng mỗi cụm: 9

📄 NỘI DUNG FILE (preview):
----------------------------------------------------------------------
Cluster 0: [-0.003, -0.003, -0.001, 0.722, -0.346...]
Cluster 1: [-0.003, -0.003, -0.001, 1.410, -1.454...]
Cluster 2: [-0.003, -0.003, -0.001, -1.204, -1.454...]
Cluster 3: [-0.003, -0.003, -0.001, 0.034, 1.315...]
Cluster 4: [-0.002, -0.002, -0.001, 1.548, -1.454...]

⚠️  QUAN TRỌNG:
   Cả 2 file tạm (hadoop_input_temp.txt và centroids_temp.txt)
   sẽ BỊ XÓA sau khi upload lên HDFS (Bước 4)
   Dữ liệu chỉ lưu trên HDFS để tuân thủ quy định bảo mật

💡 GỢI Ý TIẾP THEO:
   Chạy bước 4: ./scripts/spark/setup_hdfs.sh

⏱️  **Bước 3 hoàn thành trong 13s**

### Bước 4: Upload dữ liệu lên HDFS

=== THIẾT LẬP HDFS CHO PYSPARK K-MEANS ===
✅ HDFS có thể truy cập

✅ Đã tìm thấy file dữ liệu tạm

Đang tạo thư mục HDFS...
Đang dọn dẹp dữ liệu cũ trong HDFS...
Deleted /user/spark/hi_large/input/hadoop_input.txt
Deleted /user/spark/hi_large/centroids.txt
Deleted /user/spark/hi_large/output_centroids

Đang upload dữ liệu đầu vào lên HDFS...
  Nguồn: /home/ultimatebrok/Downloads/Final/data/processed/hadoop_input_temp.txt
  Đích: /user/spark/hi_large/input/hadoop_input.txt

Đang upload tâm cụm lên HDFS...
  Nguồn: /home/ultimatebrok/Downloads/Final/data/processed/centroids_temp.txt
  Đích: /user/spark/hi_large/centroids.txt

Đang dọn dẹp file tạm...
✅ Đã xóa file tạm (dữ liệu chỉ còn trên HDFS)

Đang xác minh upload...
  ✅ Dữ liệu đầu vào: 31.1 G
  ✅ Tâm cụm: 433 433

Cấu trúc thư mục HDFS:
-rw-r--r--   1 ultimatebrok supergroup        433 2025-10-29 01:46 /user/spark/hi_large/centroids.txt
drwxr-xr-x   - ultimatebrok supergroup          0 2025-10-29 01:46 /user/spark/hi_large/input
-rw-r--r--   1 ultimatebrok supergroup 33362225336 2025-10-29 01:46 /user/spark/hi_large/input/hadoop_input.txt
drwxr-xr-x   - ultimatebrok supergroup           0 2025-10-28 16:52 /user/spark/hi_large/output

===================================
✅ HOÀN TẤT THIẾT LẬP HDFS!
===================================

Đường dẫn HDFS:
  Đầu vào: hdfs://localhost:9000/user/spark/hi_large/input/hadoop_input.txt
  Tâm cụm: hdfs://localhost:9000/user/spark/hi_large/centroids.txt
  Đầu ra: hdfs://localhost:9000/user/spark/hi_large/output_centroids

Bước tiếp theo: Chạy PySpark job
  ./scripts/spark/run_spark.sh
⏱️  **Bước 4 hoàn thành trong 41s**

### Bước 5: Chạy PySpark K-means trên HDFS

=== PYSPARK K-MEANS VỚI HDFS ===
Cấu hình:
  CPU cores: 24
  Số executor: 4
  Executor cores: 4
  Executor memory: 4g
  Driver memory: 4g

Đường dẫn HDFS:
  Đầu vào: hdfs://localhost:9000/user/spark/hi_large/input/hadoop_input.txt
  Tâm cụm: hdfs://localhost:9000/user/spark/hi_large/centroids.txt
  Đầu ra: hdfs://localhost:9000/user/spark/hi_large/output_centroids

Đang dọn dẹp kết quả cũ...
Đang chạy PySpark K-means clustering trên HDFS...
Số lần lặp tối đa: 15

WARNING: Using incubator modules: jdk.incubator.vector
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
25/10/29 01:46:31 WARN Utils: Your hostname, brokie-hx370, resolves to a loopback address: 127.0.1.1; using 192.168.1.10 instead (on interface wlan0)
25/10/29 01:46:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
HDFS Đầu vào: hdfs://localhost:9000/user/spark/hi_large/input/hadoop_input.txt
HDFS Tâm cụm: hdfs://localhost:9000/user/spark/hi_large/centroids.txt
HDFS Đầu ra: hdfs://localhost:9000/user/spark/hi_large/output_centroids

Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
25/10/29 01:46:32 INFO SparkContext: Running Spark version 4.0.1
25/10/29 01:46:32 INFO SparkContext: OS info Linux, 6.17.5-2-cachyos, amd64
25/10/29 01:46:32 INFO SparkContext: Java version 17.0.16
25/10/29 01:46:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/10/29 01:46:32 INFO ResourceUtils: ==============================================================
25/10/29 01:46:32 INFO ResourceUtils: No custom resources configured for spark.driver.
25/10/29 01:46:32 INFO ResourceUtils: ==============================================================
25/10/29 01:46:32 INFO SparkContext: Submitted application: K-means Clustering - HDFS
25/10/29 01:46:32 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/10/29 01:46:32 INFO ResourceProfile: Limiting resource is cpus at 4 tasks per executor
25/10/29 01:46:32 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/10/29 01:46:32 INFO SecurityManager: Changing view acls to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: Changing modify acls to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: Changing view acls groups to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: Changing modify acls groups to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ultimatebrok groups with view permissions: EMPTY; users with modify permissions: ultimatebrok; groups with modify permissions: EMPTY; RPC SSL disabled
25/10/29 01:46:32 INFO Utils: Successfully started service 'sparkDriver' on port 41517.
25/10/29 01:46:32 INFO SparkEnv: Registering MapOutputTracker
25/10/29 01:46:32 INFO SparkEnv: Registering BlockManagerMaster
25/10/29 01:46:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/10/29 01:46:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/10/29 01:46:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/10/29 01:46:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7337e060-8b86-400e-93eb-fe255173b67f
25/10/29 01:46:32 INFO SparkEnv: Registering OutputCommitCoordinator
25/10/29 01:46:32 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/10/29 01:46:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/10/29 01:46:32 INFO SecurityManager: Changing view acls to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: Changing modify acls to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: Changing view acls groups to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: Changing modify acls groups to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ultimatebrok groups with view permissions: EMPTY; users with modify permissions: ultimatebrok; groups with modify permissions: EMPTY; RPC SSL disabled
25/10/29 01:46:32 INFO LocalSparkCluster: Starting a local Spark cluster with 4 workers.
25/10/29 01:46:32 INFO SecurityManager: Changing view acls to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: Changing modify acls to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: Changing view acls groups to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: Changing modify acls groups to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ultimatebrok groups with view permissions: EMPTY; users with modify permissions: ultimatebrok; groups with modify permissions: EMPTY; RPC SSL disabled
25/10/29 01:46:32 INFO Utils: Successfully started service 'sparkMaster' on port 45357.
25/10/29 01:46:32 INFO Master: Starting Spark master at spark://192.168.1.10:45357
25/10/29 01:46:32 INFO Master: Running Spark version 4.0.1
25/10/29 01:46:32 INFO JettyUtils: Start Jetty 0.0.0.0:0 for MasterUI
25/10/29 01:46:32 INFO Utils: Successfully started service 'MasterUI' on port 36869.
25/10/29 01:46:32 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://192.168.1.10:36869
25/10/29 01:46:32 INFO SecurityManager: Changing view acls to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: Changing modify acls to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: Changing view acls groups to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: Changing modify acls groups to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ultimatebrok groups with view permissions: EMPTY; users with modify permissions: ultimatebrok; groups with modify permissions: EMPTY; RPC SSL disabled
25/10/29 01:46:32 INFO Master: I have been elected leader! New state: ALIVE
25/10/29 01:46:32 INFO Utils: Successfully started service 'sparkWorker1' on port 40779.
25/10/29 01:46:32 INFO Worker: Worker decommissioning not enabled.
25/10/29 01:46:32 INFO SecurityManager: Changing view acls to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: Changing modify acls to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: Changing view acls groups to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: Changing modify acls groups to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ultimatebrok groups with view permissions: EMPTY; users with modify permissions: ultimatebrok; groups with modify permissions: EMPTY; RPC SSL disabled
25/10/29 01:46:32 INFO Worker: Starting Spark worker 192.168.1.10:40779 with 4 cores, 4.0 GiB RAM
25/10/29 01:46:32 INFO Worker: Running Spark version 4.0.1
25/10/29 01:46:32 INFO Worker: Spark home: /home/ultimatebrok/Downloads/Final/.venv/lib/python3.12/site-packages/pyspark
25/10/29 01:46:32 INFO ResourceUtils: ==============================================================
25/10/29 01:46:32 INFO ResourceUtils: No custom resources configured for spark.worker.
25/10/29 01:46:32 INFO ResourceUtils: ==============================================================
25/10/29 01:46:32 INFO JettyUtils: Start Jetty 0.0.0.0:0 for WorkerUI
25/10/29 01:46:32 INFO Utils: Successfully started service 'sparkWorker2' on port 43139.
25/10/29 01:46:32 INFO Worker: Worker decommissioning not enabled.
25/10/29 01:46:32 INFO SecurityManager: Changing view acls to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: Changing modify acls to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: Changing view acls groups to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: Changing modify acls groups to: ultimatebrok
25/10/29 01:46:32 INFO Worker: Starting Spark worker 192.168.1.10:43139 with 4 cores, 4.0 GiB RAM
25/10/29 01:46:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ultimatebrok groups with view permissions: EMPTY; users with modify permissions: ultimatebrok; groups with modify permissions: EMPTY; RPC SSL disabled
25/10/29 01:46:32 INFO Worker: Running Spark version 4.0.1
25/10/29 01:46:32 INFO Worker: Spark home: /home/ultimatebrok/Downloads/Final/.venv/lib/python3.12/site-packages/pyspark
25/10/29 01:46:32 INFO Utils: Successfully started service 'WorkerUI' on port 35283.
25/10/29 01:46:32 INFO ResourceUtils: ==============================================================
25/10/29 01:46:32 INFO ResourceUtils: No custom resources configured for spark.worker.
25/10/29 01:46:32 INFO ResourceUtils: ==============================================================
25/10/29 01:46:32 INFO JettyUtils: Start Jetty 0.0.0.0:0 for WorkerUI
25/10/29 01:46:32 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://192.168.1.10:35283
25/10/29 01:46:32 INFO Worker: Connecting to master 192.168.1.10:45357...
25/10/29 01:46:32 INFO Utils: Successfully started service 'WorkerUI' on port 38371.
25/10/29 01:46:32 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://192.168.1.10:38371
25/10/29 01:46:32 INFO Worker: Connecting to master 192.168.1.10:45357...
25/10/29 01:46:32 INFO Utils: Successfully started service 'sparkWorker3' on port 36061.
25/10/29 01:46:32 INFO Worker: Worker decommissioning not enabled.
25/10/29 01:46:32 INFO SecurityManager: Changing view acls to: ultimatebrok
25/10/29 01:46:32 INFO Worker: Starting Spark worker 192.168.1.10:36061 with 4 cores, 4.0 GiB RAM
25/10/29 01:46:32 INFO Worker: Running Spark version 4.0.1
25/10/29 01:46:32 INFO SecurityManager: Changing modify acls to: ultimatebrok
25/10/29 01:46:32 INFO Worker: Spark home: /home/ultimatebrok/Downloads/Final/.venv/lib/python3.12/site-packages/pyspark
25/10/29 01:46:32 INFO SecurityManager: Changing view acls groups to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: Changing modify acls groups to: ultimatebrok
25/10/29 01:46:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ultimatebrok groups with view permissions: EMPTY; users with modify permissions: ultimatebrok; groups with modify permissions: EMPTY; RPC SSL disabled
25/10/29 01:46:32 INFO ResourceUtils: ==============================================================
25/10/29 01:46:32 INFO ResourceUtils: No custom resources configured for spark.worker.
25/10/29 01:46:32 INFO ResourceUtils: ==============================================================
25/10/29 01:46:32 INFO JettyUtils: Start Jetty 0.0.0.0:0 for WorkerUI
25/10/29 01:46:32 INFO Utils: Successfully started service 'WorkerUI' on port 40075.
25/10/29 01:46:32 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://192.168.1.10:40075
25/10/29 01:46:32 INFO Worker: Connecting to master 192.168.1.10:45357...
25/10/29 01:46:32 INFO Utils: Successfully started service 'sparkWorker4' on port 44033.
25/10/29 01:46:32 INFO Worker: Worker decommissioning not enabled.
25/10/29 01:46:32 INFO Worker: Starting Spark worker 192.168.1.10:44033 with 4 cores, 4.0 GiB RAM
25/10/29 01:46:32 INFO Worker: Running Spark version 4.0.1
25/10/29 01:46:32 INFO Worker: Spark home: /home/ultimatebrok/Downloads/Final/.venv/lib/python3.12/site-packages/pyspark
25/10/29 01:46:32 INFO ResourceUtils: ==============================================================
25/10/29 01:46:32 INFO ResourceUtils: No custom resources configured for spark.worker.
25/10/29 01:46:32 INFO ResourceUtils: ==============================================================
25/10/29 01:46:32 INFO TransportClientFactory: Successfully created connection to /192.168.1.10:45357 after 1 ms (0 ms spent in bootstraps)
25/10/29 01:46:32 INFO TransportClientFactory: Successfully created connection to /192.168.1.10:45357 after 28 ms (0 ms spent in bootstraps)
25/10/29 01:46:32 INFO TransportClientFactory: Successfully created connection to /192.168.1.10:45357 after 15 ms (0 ms spent in bootstraps)
25/10/29 01:46:32 INFO JettyUtils: Start Jetty 0.0.0.0:0 for WorkerUI
25/10/29 01:46:32 INFO Utils: Successfully started service 'WorkerUI' on port 37207.
25/10/29 01:46:32 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://192.168.1.10:37207
25/10/29 01:46:32 INFO Worker: Connecting to master 192.168.1.10:45357...
25/10/29 01:46:32 INFO TransportClientFactory: Successfully created connection to /192.168.1.10:45357 after 1 ms (0 ms spent in bootstraps)
25/10/29 01:46:32 INFO Master: Registering worker 192.168.1.10:40779 with 4 cores, 4.0 GiB RAM
25/10/29 01:46:32 INFO Master: Registering worker 192.168.1.10:36061 with 4 cores, 4.0 GiB RAM
25/10/29 01:46:32 INFO Master: Registering worker 192.168.1.10:44033 with 4 cores, 4.0 GiB RAM
25/10/29 01:46:32 INFO Worker: Successfully registered with master spark://192.168.1.10:45357
25/10/29 01:46:32 INFO Worker: Worker cleanup enabled; old application directories will be deleted in: /home/ultimatebrok/Downloads/Final/.venv/lib/python3.12/site-packages/pyspark/work
25/10/29 01:46:32 INFO Master: Registering worker 192.168.1.10:43139 with 4 cores, 4.0 GiB RAM
25/10/29 01:46:32 INFO Worker: Successfully registered with master spark://192.168.1.10:45357
25/10/29 01:46:32 INFO Worker: Worker cleanup enabled; old application directories will be deleted in: /home/ultimatebrok/Downloads/Final/.venv/lib/python3.12/site-packages/pyspark/work
25/10/29 01:46:32 INFO Worker: Successfully registered with master spark://192.168.1.10:45357
25/10/29 01:46:32 INFO Worker: Worker cleanup enabled; old application directories will be deleted in: /home/ultimatebrok/Downloads/Final/.venv/lib/python3.12/site-packages/pyspark/work
25/10/29 01:46:32 INFO Worker: Successfully registered with master spark://192.168.1.10:45357
25/10/29 01:46:32 INFO Worker: Worker cleanup enabled; old application directories will be deleted in: /home/ultimatebrok/Downloads/Final/.venv/lib/python3.12/site-packages/pyspark/work
25/10/29 01:46:32 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://192.168.1.10:45357...
25/10/29 01:46:32 INFO TransportClientFactory: Successfully created connection to /192.168.1.10:45357 after 1 ms (0 ms spent in bootstraps)
25/10/29 01:46:32 INFO Master: Registering app K-means Clustering - HDFS
25/10/29 01:46:32 INFO Master: Registered app K-means Clustering - HDFS with ID app-20251029014632-0000
25/10/29 01:46:32 INFO Master: Start scheduling for app app-20251029014632-0000 with rpId: 0
25/10/29 01:46:32 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20251029014632-0000
25/10/29 01:46:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38723.
25/10/29 01:46:33 INFO NettyBlockTransferService: Server created on 192.168.1.10:38723
25/10/29 01:46:33 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/10/29 01:46:33 INFO Master: Launching executor app-20251029014632-0000/0 on worker worker-20251029014632-192.168.1.10-44033
25/10/29 01:46:33 INFO Master: Launching executor app-20251029014632-0000/1 on worker worker-20251029014632-192.168.1.10-43139
25/10/29 01:46:33 INFO Master: Launching executor app-20251029014632-0000/2 on worker worker-20251029014632-192.168.1.10-40779
25/10/29 01:46:33 INFO Master: Launching executor app-20251029014632-0000/3 on worker worker-20251029014632-192.168.1.10-36061
25/10/29 01:46:33 INFO Worker: Asked to launch executor app-20251029014632-0000/0 for K-means Clustering - HDFS
25/10/29 01:46:33 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20251029014632-0000/0 on worker-20251029014632-192.168.1.10-44033 (192.168.1.10:44033) with 4 core(s)
25/10/29 01:46:33 INFO Worker: Asked to launch executor app-20251029014632-0000/2 for K-means Clustering - HDFS
25/10/29 01:46:33 INFO Worker: Asked to launch executor app-20251029014632-0000/1 for K-means Clustering - HDFS
25/10/29 01:46:33 INFO StandaloneSchedulerBackend: Granted executor ID app-20251029014632-0000/0 on hostPort 192.168.1.10:44033 with 4 core(s), 4.0 GiB RAM
25/10/29 01:46:33 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20251029014632-0000/1 on worker-20251029014632-192.168.1.10-43139 (192.168.1.10:43139) with 4 core(s)
25/10/29 01:46:33 INFO StandaloneSchedulerBackend: Granted executor ID app-20251029014632-0000/1 on hostPort 192.168.1.10:43139 with 4 core(s), 4.0 GiB RAM
25/10/29 01:46:33 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20251029014632-0000/2 on worker-20251029014632-192.168.1.10-40779 (192.168.1.10:40779) with 4 core(s)
25/10/29 01:46:33 INFO StandaloneSchedulerBackend: Granted executor ID app-20251029014632-0000/2 on hostPort 192.168.1.10:40779 with 4 core(s), 4.0 GiB RAM
25/10/29 01:46:33 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20251029014632-0000/3 on worker-20251029014632-192.168.1.10-36061 (192.168.1.10:36061) with 4 core(s)
25/10/29 01:46:33 INFO StandaloneSchedulerBackend: Granted executor ID app-20251029014632-0000/3 on hostPort 192.168.1.10:36061 with 4 core(s), 4.0 GiB RAM
25/10/29 01:46:33 INFO Worker: Asked to launch executor app-20251029014632-0000/3 for K-means Clustering - HDFS
25/10/29 01:46:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.10, 38723, None)
25/10/29 01:46:33 INFO SecurityManager: Changing view acls to: ultimatebrok
25/10/29 01:46:33 INFO SecurityManager: Changing view acls to: ultimatebrok
25/10/29 01:46:33 INFO SecurityManager: Changing view acls to: ultimatebrok
25/10/29 01:46:33 INFO SecurityManager: Changing view acls to: ultimatebrok
25/10/29 01:46:33 INFO SecurityManager: Changing modify acls to: ultimatebrok
25/10/29 01:46:33 INFO SecurityManager: Changing modify acls to: ultimatebrok
25/10/29 01:46:33 INFO SecurityManager: Changing modify acls to: ultimatebrok
25/10/29 01:46:33 INFO SecurityManager: Changing modify acls to: ultimatebrok
25/10/29 01:46:33 INFO SecurityManager: Changing view acls groups to: ultimatebrok
25/10/29 01:46:33 INFO SecurityManager: Changing view acls groups to: ultimatebrok
25/10/29 01:46:33 INFO SecurityManager: Changing modify acls groups to: ultimatebrok
25/10/29 01:46:33 INFO SecurityManager: Changing view acls groups to: ultimatebrok
25/10/29 01:46:33 INFO SecurityManager: Changing view acls groups to: ultimatebrok
25/10/29 01:46:33 INFO SecurityManager: Changing modify acls groups to: ultimatebrok
25/10/29 01:46:33 INFO SecurityManager: Changing modify acls groups to: ultimatebrok
25/10/29 01:46:33 INFO SecurityManager: Changing modify acls groups to: ultimatebrok
25/10/29 01:46:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ultimatebrok groups with view permissions: EMPTY; users with modify permissions: ultimatebrok; groups with modify permissions: EMPTY; RPC SSL disabled
25/10/29 01:46:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ultimatebrok groups with view permissions: EMPTY; users with modify permissions: ultimatebrok; groups with modify permissions: EMPTY; RPC SSL disabled
25/10/29 01:46:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ultimatebrok groups with view permissions: EMPTY; users with modify permissions: ultimatebrok; groups with modify permissions: EMPTY; RPC SSL disabled
25/10/29 01:46:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ultimatebrok groups with view permissions: EMPTY; users with modify permissions: ultimatebrok; groups with modify permissions: EMPTY; RPC SSL disabled
25/10/29 01:46:33 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.10:38723 with 2.2 GiB RAM, BlockManagerId(driver, 192.168.1.10, 38723, None)
25/10/29 01:46:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.10, 38723, None)
25/10/29 01:46:33 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.10, 38723, None)
25/10/29 01:46:33 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-17-openjdk/bin/java" "-cp" "/home/ultimatebrok/Downloads/Final/.venv/lib/python3.12/site-packages/pyspark/conf:/home/ultimatebrok/Downloads/Final/.venv/lib/python3.12/site-packages/pyspark/jars/slf4j-api-2.0.16.jar:/home/ultimatebrok/Downloads/Final/.venv/lib/python3.12/site-packages/pyspark/jars/*:/opt/hadoop/etc/hadoop/" "-Xmx4096M" "-Dspark.driver.port=41517" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-modules=jdk.incubator.vector" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "-Dio.netty.tryReflectionSetAccessible=true" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@192.168.1.10:41517" "--executor-id" "0" "--hostname" "192.168.1.10" "--cores" "4" "--app-id" "app-20251029014632-0000" "--worker-url" "spark://Worker@192.168.1.10:44033" "--resourceProfileId" "0"
25/10/29 01:46:33 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-17-openjdk/bin/java" "-cp" "/home/ultimatebrok/Downloads/Final/.venv/lib/python3.12/site-packages/pyspark/conf:/home/ultimatebrok/Downloads/Final/.venv/lib/python3.12/site-packages/pyspark/jars/slf4j-api-2.0.16.jar:/home/ultimatebrok/Downloads/Final/.venv/lib/python3.12/site-packages/pyspark/jars/*:/opt/hadoop/etc/hadoop/" "-Xmx4096M" "-Dspark.driver.port=41517" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-modules=jdk.incubator.vector" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "-Dio.netty.tryReflectionSetAccessible=true" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@192.168.1.10:41517" "--executor-id" "1" "--hostname" "192.168.1.10" "--cores" "4" "--app-id" "app-20251029014632-0000" "--worker-url" "spark://Worker@192.168.1.10:43139" "--resourceProfileId" "0"
25/10/29 01:46:33 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-17-openjdk/bin/java" "-cp" "/home/ultimatebrok/Downloads/Final/.venv/lib/python3.12/site-packages/pyspark/conf:/home/ultimatebrok/Downloads/Final/.venv/lib/python3.12/site-packages/pyspark/jars/slf4j-api-2.0.16.jar:/home/ultimatebrok/Downloads/Final/.venv/lib/python3.12/site-packages/pyspark/jars/*:/opt/hadoop/etc/hadoop/" "-Xmx4096M" "-Dspark.driver.port=41517" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-modules=jdk.incubator.vector" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "-Dio.netty.tryReflectionSetAccessible=true" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@192.168.1.10:41517" "--executor-id" "2" "--hostname" "192.168.1.10" "--cores" "4" "--app-id" "app-20251029014632-0000" "--worker-url" "spark://Worker@192.168.1.10:40779" "--resourceProfileId" "0"
25/10/29 01:46:33 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-17-openjdk/bin/java" "-cp" "/home/ultimatebrok/Downloads/Final/.venv/lib/python3.12/site-packages/pyspark/conf:/home/ultimatebrok/Downloads/Final/.venv/lib/python3.12/site-packages/pyspark/jars/slf4j-api-2.0.16.jar:/home/ultimatebrok/Downloads/Final/.venv/lib/python3.12/site-packages/pyspark/jars/*:/opt/hadoop/etc/hadoop/" "-Xmx4096M" "-Dspark.driver.port=41517" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-modules=jdk.incubator.vector" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "-Dio.netty.tryReflectionSetAccessible=true" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@192.168.1.10:41517" "--executor-id" "3" "--hostname" "192.168.1.10" "--cores" "4" "--app-id" "app-20251029014632-0000" "--worker-url" "spark://Worker@192.168.1.10:36061" "--resourceProfileId" "0"
25/10/29 01:46:33 INFO Master: Start scheduling for app app-20251029014632-0000 with rpId: 0
25/10/29 01:46:33 INFO Master: Start scheduling for app app-20251029014632-0000 with rpId: 0
25/10/29 01:46:33 INFO Master: Start scheduling for app app-20251029014632-0000 with rpId: 0
25/10/29 01:46:33 INFO Master: Start scheduling for app app-20251029014632-0000 with rpId: 0
25/10/29 01:46:33 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20251029014632-0000/2 is now RUNNING
25/10/29 01:46:33 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20251029014632-0000/3 is now RUNNING
25/10/29 01:46:33 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20251029014632-0000/1 is now RUNNING
25/10/29 01:46:33 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20251029014632-0000/0 is now RUNNING
25/10/29 01:46:33 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/10/29 01:46:34 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.10:45090) with ID 3, ResourceProfileId 0
25/10/29 01:46:34 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.10:45108) with ID 2, ResourceProfileId 0
25/10/29 01:46:34 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.10:45128) with ID 1, ResourceProfileId 0
25/10/29 01:46:34 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.10:35833 with 2.2 GiB RAM, BlockManagerId(3, 192.168.1.10, 35833, None)
25/10/29 01:46:34 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.10:44205 with 2.2 GiB RAM, BlockManagerId(2, 192.168.1.10, 44205, None)
=== PYSPARK K-MEANS CLUSTERING ===
Đầu vào: hdfs://localhost:9000/user/spark/hi_large/input/hadoop_input.txt
Tâm cụm: hdfs://localhost:9000/user/spark/hi_large/centroids.txt
Số lần lặp tối đa: 15

Đang đọc dữ liệu từ HDFS...
Đã load 179,702,229 điểm từ hdfs://localhost:9000/user/spark/hi_large/input/hadoop_input.txt
Đang đọc tâm cụm từ hdfs://localhost:9000/user/spark/hi_large/centroids.txt
Đã khởi tạo 5 tâm cụm

=== Lần lặp 1/15 ===
Độ dịch chuyển tâm cụm: 3.235698

=== Lần lặp 2/15 ===
Độ dịch chuyển tâm cụm: 0.432947

=== Lần lặp 3/15 ===
Độ dịch chuyển tâm cụm: 0.229976

=== Lần lặp 4/15 ===
Độ dịch chuyển tâm cụm: 0.143134

=== Lần lặp 5/15 ===
Độ dịch chuyển tâm cụm: 0.113795

=== Lần lặp 6/15 ===
Độ dịch chuyển tâm cụm: 0.093823

=== Lần lặp 7/15 ===
Độ dịch chuyển tâm cụm: 0.064364

=== Lần lặp 8/15 ===
Độ dịch chuyển tâm cụm: 0.051341

=== Lần lặp 9/15 ===
Độ dịch chuyển tâm cụm: 0.048377

=== Lần lặp 10/15 ===
Độ dịch chuyển tâm cụm: 0.053732

=== Lần lặp 11/15 ===
Độ dịch chuyển tâm cụm: 0.065842

=== Lần lặp 12/15 ===
Độ dịch chuyển tâm cụm: 0.090034

=== Lần lặp 13/15 ===
Độ dịch chuyển tâm cụm: 0.064394

=== Lần lặp 14/15 ===
Độ dịch chuyển tâm cụm: 0.036974

=== Lần lặp 15/15 ===
Độ dịch chuyển tâm cụm: 0.027681


Đang lưu tâm cụm cuối cùng vào HDFS: hdfs://localhost:9000/user/spark/hi_large/output_centroids

Kích thước các cụm:
  Cụm 0: 54,769,409 điểm (30.48%)
  Cụm 1: 48,758,953 điểm (27.13%)
  Cụm 2: 12,028,812 điểm (6.69%)
  Cụm 3: 52,250,781 điểm (29.08%)
  Cụm 4: 11,894,274 điểm (6.62%)

✅ Hoàn thành thuật toán K-means clustering!

✅ PySpark K-means hoàn thành!
Tâm cụm cuối cùng đã lưu vào HDFS: hdfs://localhost:9000/user/spark/hi_large/output_centroids

Để xem kết quả:
  hdfs dfs -cat /user/spark/hi_large/output_centroids/part-*
⏱️  **Bước 5 hoàn thành trong 27m 12s**

### Bước 6: Tải kết quả từ HDFS

=== TẢI KẾT QUẢ TỪ HDFS ===
✅ Đã tìm thấy kết quả trong HDFS

Đang tải tâm cụm cuối cùng...
  Từ: /user/spark/hi_large/output_centroids
  Đến: /home/ultimatebrok/Downloads/Final/data/processed/final_centroids.txt
✅ Đã tải tâm cụm cuối cùng

Thông tin file local:
  Kích thước: 4,0K
  Số dòng (cụm): 5

Xem trước:
0.000904,-0.000975,-0.000284,-0.253267,0.001022,-1.017996,-0.312873,-0.326412,-0.394462
-0.001889,-0.001293,-0.001119,0.140210,-0.953760,0.550492,-0.376812,-0.371990,0.188914
-0.000537,0.000476,0.011740,-0.861240,0.008797,-0.074549,2.314222,2.328195,-0.000088
-0.001620,-0.000956,-0.001112,0.132610,0.876193,0.554546,-0.378161,-0.373485,0.205698
0.011187,0.013501,-0.001112,0.886969,-0.009831,0.097185,2.304992,2.313215,0.148116

✅ Hoàn thành tải xuống!

Bước tiếp theo:
  1. Gán cụm: cd /home/ultimatebrok/Downloads/Final/scripts/polars && python assign_clusters_polars.py
  2. Phân tích kết quả: cd /home/ultimatebrok/Downloads/Final/scripts/polars && python analyze_polars.py
⏱️  **Bước 6 hoàn thành trong 3s**

### Bước 7: Gán cụm với Polars

======================================================================
🏷️  BƯỚC 7: GÁN NHÃN CỤM CHO TỮNG GIAO DỊCH
======================================================================

🎯 ĐỌC TÂM CỤM CUỐI CÙNG TỪ BƯỚC 6...
   File: /home/ultimatebrok/Downloads/Final/data/processed/final_centroids.txt

✅ Đã load 5 tâm cụm
   Mỗi tâm cụm có 9 đặc trưng

📂 ĐỌC DỮ LIỆU TỪ HDFS...
   File HDFS: /user/spark/hi_large/input/hadoop_input.txt
   (179M dòng, 33GB - đã chuẩn hóa)

🔄 Streaming từ HDFS (có thể mất vài phút)...
📊 Đang đọc CSV từ HDFS stream...

✅ Đã load 179,702,229 bản ghi từ HDFS

📊 CHUYỂN SANG NUMPY VÀ TÍNH KHOẢNG CÁCH...
   Dữ liệu: 179,702,229 dòng x 9 cột
   Tâm cụm: 5 cụm x 9 đặc trưng

🔢 TÍNH KHOẢNG CÁCH EUCLIDEAN (Batch Processing)...
   Xử lý 1 triệu giao dịch mỗi lần để tiết kiệm RAM

    Đã xử lý 1,000,000/179,702,229 giao dịch (0.6%)
    Đã xử lý 11,000,000/179,702,229 giao dịch (6.1%)
    Đã xử lý 21,000,000/179,702,229 giao dịch (11.7%)
    Đã xử lý 31,000,000/179,702,229 giao dịch (17.3%)
    Đã xử lý 41,000,000/179,702,229 giao dịch (22.8%)
    Đã xử lý 51,000,000/179,702,229 giao dịch (28.4%)
    Đã xử lý 61,000,000/179,702,229 giao dịch (33.9%)
    Đã xử lý 71,000,000/179,702,229 giao dịch (39.5%)
    Đã xử lý 81,000,000/179,702,229 giao dịch (45.1%)
    Đã xử lý 91,000,000/179,702,229 giao dịch (50.6%)
    Đã xử lý 101,000,000/179,702,229 giao dịch (56.2%)
    Đã xử lý 111,000,000/179,702,229 giao dịch (61.8%)
    Đã xử lý 121,000,000/179,702,229 giao dịch (67.3%)
    Đã xử lý 131,000,000/179,702,229 giao dịch (72.9%)
    Đã xử lý 141,000,000/179,702,229 giao dịch (78.5%)
    Đã xử lý 151,000,000/179,702,229 giao dịch (84.0%)
    Đã xử lý 161,000,000/179,702,229 giao dịch (89.6%)
    Đã xử lý 171,000,000/179,702,229 giao dịch (95.2%)
    Đã xử lý 179,702,229/179,702,229 giao dịch (100.0%)

✅ Đã hoàn thành tính toán cho 179,702,229 giao dịch

💾 LƯU KẾT QUẢ...
   File: /home/ultimatebrok/Downloads/Final/data/results/clustered_results.txt
======================================================================
✅ HOÀN TẤT GÁN NHÃN CỤM!
======================================================================
📄 File kết quả: /home/ultimatebrok/Downloads/Final/data/results/clustered_results.txt
📊 Kích thước: 342.75 MB
📊 Tổng giao dịch: 179,702,229

📊 PHÂN PHỐI CỤM:
   Cluster 0: 54,769,397 giao dịch (30.48%)
   Cluster 1: 48,758,964 giao dịch (27.13%)
   Cluster 2: 12,028,821 giao dịch (6.69%)
   Cluster 3: 52,250,789 giao dịch (29.08%)
   Cluster 4: 11,894,258 giao dịch (6.62%)

💡 GỢI Ý TIẾP THEO:
   Chạy bước 8: python scripts/polars/analyze_polars.py

⏱️  **Bước 7 hoàn thành trong 3m 21s**

### Bước 8: Phân tích kết quả

======================================================================
📊 BƯỚC 8: PHÂN TÍCH KẾT QUẢ
======================================================================

📂 ĐỌC KẾT QUẢ PHÂN CỤM TỪ BƯỚC 7...
   File: /home/ultimatebrok/Downloads/Final/data/results/clustered_results.txt
✅ Đã load 179,702,229 nhãn cụm

📊 ĐỌC DỮ LIỆU GỐC (Lazy Mode)...
   File: /home/ultimatebrok/Downloads/Final/data/raw/HI-Large_Trans.csv
✅ Đã load metadata (chưa load toàn bộ vào RAM)

🎯 THÊM CỘT 'cluster' VÀO DATA...
✅ Đã gắn nhãn cụm cho mỗi giao dịch

======================================================================
📋 PHÂN TÍCH CHI TIẾT
======================================================================

📈 THỐNG KÊ TỔNG QUAN:
   Tổng số giao dịch: 179,702,229
   Số cụm: 5

📉 KÍCH THƯỚC MỖI CỤM:
----------------------------------------------------------------------
shape: (5, 2)
┌─────────┬──────────┐
│ cluster ┆ count    │
│ ---     ┆ ---      │
│ i64     ┆ u32      │
╞═════════╪══════════╡
│ 0       ┆ 54769397 │
│ 1       ┆ 48758964 │
│ 2       ┆ 12028821 │
│ 3       ┆ 52250789 │
│ 4       ┆ 11894258 │
└─────────┴──────────┘

💰 TỶ LỆ RỬA TIỀN TRONG TỮNG CỤM:
----------------------------------------------------------------------
shape: (5, 4)
┌─────────┬──────────┬──────────────────┬─────────────────┐
│ cluster ┆ total    ┆ laundering_count ┆ laundering_rate │
│ ---     ┆ ---      ┆ ---              ┆ ---             │
│ i64     ┆ u32      ┆ i64              ┆ f64             │
╞═════════╪══════════╪══════════════════╪═════════════════╡
│ 0       ┆ 54769397 ┆ 48365            ┆ 0.088307        │
│ 1       ┆ 48758964 ┆ 69348            ┆ 0.142226        │
│ 2       ┆ 12028821 ┆ 6842             ┆ 0.05688         │
│ 3       ┆ 52250789 ┆ 92071            ┆ 0.17621         │
│ 4       ┆ 11894258 ┆ 8920             ┆ 0.074994        │
└─────────┴──────────┴──────────────────┴─────────────────┘

⚠️  CỤM CÓ RỦI RO CAO (>10% rửa tiền):
----------------------------------------------------------------------
✅ KHÔNG có cụm nào vượt ngưỡng 10%
   Tất cả các cụm đều trong mức chấp nhận được.

📊 ĐẶC TRƯNG TRUNG BÌNH MỖI CỤM:
----------------------------------------------------------------------
shape: (5, 4)
┌─────────┬─────────────────────┬─────────────────┬───────────┐
│ cluster ┆ avg_amount_received ┆ avg_amount_paid ┆ avg_ratio │
│ ---     ┆ ---                 ┆ ---             ┆ ---       │
│ i64     ┆ f64                 ┆ f64             ┆ f64       │
╞═════════╪═════════════════════╪═════════════════╪═══════════╡
│ 0       ┆ 7.2806e6            ┆ 2.6325e6        ┆ 1.703137  │
│ 1       ┆ 2.1848e6            ┆ 2.1848e6        ┆ 0.999988  │
│ 2       ┆ 4.6350e6            ┆ 4.6374e6        ┆ 11.811974 │
│ 3       ┆ 2.6538e6            ┆ 2.6351e6        ┆ 1.005274  │
│ 4       ┆ 2.5882e7            ┆ 2.2697e7        ┆ 1.005467  │
└─────────┴─────────────────────┴─────────────────┴───────────┘

💡 NHẬN XÉT:
----------------------------------------------------------------------
1. CỤm nghi ngờ NHẤT: Cluster 3 (0.18% rửa tiền)
   ➡️  Nên kiểm tra kỹ các giao dịch trong cụm này

2. Cụm an toàn NHẤT: Cluster 2 (0.06% rửa tiền)
   ➡️  Có thể ưu tiên thấp khi kiểm tra

3. Đánh giá tổng thể: ✅ RỦI RO THẤP
   Hệ thống hoạt động tốt, tỷ lệ rửa tiền thấp

======================================================================
✅ HOÀN TẤT PHÂN TÍCH!
======================================================================

📊 TÓM TẢT:
   - Đã phân tích 179,702,229 giao dịch
   - Phân thành 5 cụm
   - Tỷ lệ rửa tiền: 0.06% - 0.18%
   - Số cụm rủi ro cao: 0 (✅ Tốt!)

🎉 PIPELINE HOÀN TẤT!
   Tất cả 8 bước đã chạy thành công.
   Xem kết quả trong thư mục data/results/

⏱️  **Bước 8 hoàn thành trong 30s**


---

## Tổng kết

✅ **Pipeline hoàn thành thành công!**

**Thời gian kết thúc:** 2025-10-29 02:17:32
**Tổng thời gian chạy:** 33m 18s

---

*Log đã lưu tại: /home/ultimatebrok/Downloads/Final/logs/pipeline_log_20251029_014414.md*
