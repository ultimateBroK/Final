# ğŸ“Š BÃO CÃO Dá»° ÃN: PHÃT HIá»†N Rá»¬A TIá»€N Báº°NG Há»ŒC MÃY

## PhÃ¢n TÃ­ch 179 Triá»‡u Giao Dá»‹ch vá»›i Apache Spark

## Má»¥c lá»¥c
- [TÃ³m táº¯t Ä‘iá»u hÃ nh](#tom-tat)
- [Pháº§n 1: Giá»›i thiá»‡u dá»± Ã¡n](#p1)
- [Pháº§n 2: Dá»¯ liá»‡u vÃ  tiá»n xá»­ lÃ½](#p2)
- [Pháº§n 3: Kiáº¿n trÃºc há»‡ thá»‘ng](#p3)
- [Pháº§n 4: Quy trÃ¬nh xá»­ lÃ½ (Pipeline)](#p4)
- [Pháº§n 5: Káº¿t quáº£ vÃ  Ä‘Ã¡nh giÃ¡](#p5)
- [Pháº§n 6: TuÃ¢n thá»§ quy Ä‘á»‹nh báº£o máº­t](#p6)
- [Pháº§n 7: HÆ°á»›ng dáº«n sá»­ dá»¥ng](#p7)
- [Pháº§n 8: Xá»­ lÃ½ sá»± cá»‘](#p8)
- [Pháº§n 9: Káº¿t luáº­n vÃ  hÆ°á»›ng phÃ¡t triá»ƒn](#p9)
- [Phá»¥ lá»¥c](#phu-luc)

---

- NgÃ y láº­p bÃ¡o cÃ¡o: 28/10/2025 22:04:46
- Vá»‹ trÃ­ dá»± Ã¡n: `/home/ultimatebrok/Downloads/Final`
- NgÆ°á»i thá»±c hiá»‡n: Sinh viÃªn
- Giáº£ng viÃªn hÆ°á»›ng dáº«n: [TÃªn giáº£ng viÃªn]

---

<a id="tom-tat"></a>
## TÃ“M Táº®T ÄIá»€U HÃ€NH

### BÃ i toÃ¡n
PhÃ¡t hiá»‡n cÃ¡c giao dá»‹ch nghi ngá» rá»­a tiá»n trong táº­p dá»¯ liá»‡u lá»›n chá»©a **179 triá»‡u giao dá»‹ch** (kÃ­ch thÆ°á»›c 16GB), sá»­ dá»¥ng ká»¹ thuáº­t phÃ¢n cá»¥m K-means trÃªn ná»n táº£ng xá»­ lÃ½ phÃ¢n tÃ¡n Apache Spark.

### Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c
- âœ… Xá»­ lÃ½ thÃ nh cÃ´ng 179,702,229 giao dá»‹ch
- âœ… PhÃ¢n thÃ nh 5 cá»¥m vá»›i tá»· lá»‡ rá»­a tiá»n khÃ¡c nhau (0.01% - 0.17%)
- âœ… Thá»i gian xá»­ lÃ½: 33 phÃºt (nhanh hÆ¡n Hadoop 4-8 láº§n)
- âœ… PhÃ¡t hiá»‡n 225,546 giao dá»‹ch nghi ngá» rá»­a tiá»n (0.13% tá»•ng sá»‘)
- âœ… TuÃ¢n thá»§ quy Ä‘á»‹nh: KHÃ”NG lÆ°u dá»¯ liá»‡u lá»›n á»Ÿ mÃ¡y cá»¥c bá»™

### CÃ´ng nghá»‡ sá»­ dá»¥ng
- **Polars**: ThÆ° viá»‡n xá»­ lÃ½ dá»¯ liá»‡u siÃªu nhanh (nhanh hÆ¡n Pandas 10-100 láº§n)
- **Apache Spark**: Há»‡ thá»‘ng xá»­ lÃ½ phÃ¢n tÃ¡n trong bá»™ nhá»›
- **HDFS**: Há»‡ thá»‘ng lÆ°u trá»¯ phÃ¢n tÃ¡n cá»§a Hadoop
- **Python**: NgÃ´n ngá»¯ láº­p trÃ¬nh chÃ­nh
- **K-means**: Thuáº­t toÃ¡n phÃ¢n cá»¥m há»c mÃ¡y

---

<a id="p1"></a>
## PHáº¦N 1: GIá»šI THIá»†U Dá»° ÃN

### 1.1. Bá»‘i cáº£nh vÃ  Äá»™ng lá»±c

#### Váº¥n Ä‘á» rá»­a tiá»n trong thá»±c táº¿
Rá»­a tiá»n lÃ  hÃ nh vi che giáº¥u nguá»“n gá»‘c báº¥t há»£p phÃ¡p cá»§a tiá»n báº±ng cÃ¡ch chuyá»ƒn qua nhiá»u 
giao dá»‹ch phá»©c táº¡p. CÃ¡c tá»• chá»©c tÃ i chÃ­nh pháº£i phÃ¡t hiá»‡n vÃ  bÃ¡o cÃ¡o cÃ¡c giao dá»‹ch nghi ngá» 
theo quy Ä‘á»‹nh phÃ¡p luáº­t.

#### ThÃ¡ch thá»©c vá»›i dá»¯ liá»‡u lá»›n
- **Khá»‘i lÆ°á»£ng khá»•ng lá»“**: HÃ ng trÄƒm triá»‡u giao dá»‹ch má»—i thÃ¡ng
- **Tá»‘c Ä‘á»™ xá»­ lÃ½**: Cáº§n phÃ¢n tÃ­ch nhanh Ä‘á»ƒ phÃ¡t hiá»‡n ká»‹p thá»i
- **Äá»™ chÃ­nh xÃ¡c**: Giáº£m thiá»ƒu cáº£nh bÃ¡o giáº£ (false positive)
- **TuÃ¢n thá»§ quy Ä‘á»‹nh**: Báº£o máº­t dá»¯ liá»‡u khÃ¡ch hÃ ng

#### Giáº£i phÃ¡p cá»§a dá»± Ã¡n
Sá»­ dá»¥ng **há»c mÃ¡y khÃ´ng giÃ¡m sÃ¡t (Unsupervised Learning)** vá»›i thuáº­t toÃ¡n K-means Ä‘á»ƒ:
- Tá»± Ä‘á»™ng phÃ¢n nhÃ³m giao dá»‹ch cÃ³ Ä‘áº·c Ä‘iá»ƒm tÆ°Æ¡ng tá»±
- PhÃ¡t hiá»‡n cÃ¡c cá»¥m cÃ³ tá»· lá»‡ rá»­a tiá»n cao báº¥t thÆ°á»ng
- Xá»­ lÃ½ song song trÃªn nhiá»u mÃ¡y tÃ­nh (distributed computing)
- Äáº£m báº£o tuÃ¢n thá»§ quy Ä‘á»‹nh vá» báº£o máº­t dá»¯ liá»‡u

### 1.2. Má»¥c tiÃªu dá»± Ã¡n

#### Má»¥c tiÃªu chÃ­nh
1. **PhÃ¢n tÃ­ch dá»¯ liá»‡u giao dá»‹ch quy mÃ´ lá»›n**
   - Xá»­ lÃ½ file CSV 16GB chá»©a 179 triá»‡u báº£n ghi
   - TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng (feature extraction) tá»« dá»¯ liá»‡u thÃ´
   - Chuáº©n hÃ³a dá»¯ liá»‡u Ä‘á»ƒ thuáº­t toÃ¡n hoáº¡t Ä‘á»™ng hiá»‡u quáº£

2. **PhÃ¢n cá»¥m giao dá»‹ch báº±ng K-means**
   - Chia 179 triá»‡u giao dá»‹ch thÃ nh 5 cá»¥m
   - Má»—i cá»¥m Ä‘áº¡i diá»‡n cho má»™t pattern giao dá»‹ch
   - Sá»­ dá»¥ng Apache Spark Ä‘á»ƒ xá»­ lÃ½ phÃ¢n tÃ¡n

3. **PhÃ¡t hiá»‡n giao dá»‹ch nghi ngá»**
   - PhÃ¢n tÃ­ch tá»· lá»‡ rá»­a tiá»n trong tá»«ng cá»¥m
   - XÃ¡c Ä‘á»‹nh cá»¥m cÃ³ tá»· lá»‡ báº¥t thÆ°á»ng cao
   - Xuáº¥t danh sÃ¡ch giao dá»‹ch cáº§n kiá»ƒm tra thá»§ cÃ´ng

4. **TuÃ¢n thá»§ quy Ä‘á»‹nh báº£o máº­t**
   - KHÃ”NG lÆ°u dá»¯ liá»‡u lá»›n á»Ÿ mÃ¡y cá»¥c bá»™
   - Chá»‰ lÆ°u trÃªn HDFS (Hadoop Distributed File System)
   - Tá»± Ä‘á»™ng xÃ³a file táº¡m sau khi xá»­ lÃ½

#### Má»¥c tiÃªu phá»¥
- Há»c vÃ  Ã¡p dá»¥ng cÃ´ng nghá»‡ Big Data (Spark, HDFS)
- So sÃ¡nh hiá»‡u suáº¥t giá»¯a Hadoop MapReduce vÃ  Apache Spark
- XÃ¢y dá»±ng quy trÃ¬nh tá»± Ä‘á»™ng (pipeline) tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i
- Viáº¿t tÃ i liá»‡u chi tiáº¿t, dá»… hiá»ƒu cho ngÆ°á»i khÃ¡c

---

<a id="p2"></a>
## PHáº¦N 2: Dá»® LIá»†U VÃ€ TIá»€N Xá»¬ LÃ

### 2.1. MÃ´ táº£ táº­p dá»¯ liá»‡u

#### ThÃ´ng tin cÆ¡ báº£n
- **TÃªn file**: `HI-Large_Trans.csv`
- **KÃ­ch thÆ°á»›c**: 16 GB (gigabyte)
- **Sá»‘ báº£n ghi**: 179,702,229 giao dá»‹ch
- **Nguá»“n**: Táº­p dá»¯ liá»‡u mÃ´ phá»ng giao dá»‹ch ngÃ¢n hÃ ng quá»‘c táº¿

#### Cáº¥u trÃºc dá»¯ liá»‡u (11 cá»™t)

| TÃªn cá»™t | Ã nghÄ©a | Kiá»ƒu dá»¯ liá»‡u | VÃ­ dá»¥ |
|---------|---------|--------------|-------|
| `Timestamp` | Thá»i gian giao dá»‹ch | Chuá»—i | "2022/08/01 00:17" |
| `From Bank` | MÃ£ ngÃ¢n hÃ ng gá»­i | Sá»‘ nguyÃªn | 20, 3196, 1208 |
| `Account` | MÃ£ tÃ i khoáº£n gá»­i | Chuá»—i | "800104D70" |
| `To Bank` | MÃ£ ngÃ¢n hÃ ng nháº­n | Sá»‘ nguyÃªn | 20, 3196 |
| `Account.1` | MÃ£ tÃ i khoáº£n nháº­n | Chuá»—i | "800107150" |
| `Amount Received` | Sá»‘ tiá»n nháº­n Ä‘Æ°á»£c | Sá»‘ thá»±c | 6794.63 |
| `Receiving Currency` | Loáº¡i tiá»n nháº­n | Chuá»—i | "US Dollar", "Yuan" |
| `Amount Paid` | Sá»‘ tiá»n tráº£ | Sá»‘ thá»±c | 7739.29 |
| `Payment Currency` | Loáº¡i tiá»n tráº£ | Chuá»—i | "US Dollar", "Bitcoin" |
| `Payment Format` | HÃ¬nh thá»©c thanh toÃ¡n | Chuá»—i | "Reinvestment", "Cheque" |
| `Is Laundering` | NhÃ£n rá»­a tiá»n | 0 hoáº·c 1 | 0 = BÃ¬nh thÆ°á»ng, 1 = Rá»­a tiá»n |

#### Thá»‘ng kÃª mÃ´ táº£
- **Tá»· lá»‡ rá»­a tiá»n tá»•ng thá»ƒ**: 0.126% (225,546 / 179,702,229)
- **Loáº¡i tiá»n phá»• biáº¿n nháº¥t**: Euro (23%), Yuan (7.2%), Mexican Peso (2.7%)
- **GiÃ¡ trá»‹ giao dá»‹ch trung bÃ¬nh**: ~1.14 triá»‡u Ä‘Æ¡n vá»‹ tiá»n tá»‡
- **Khoáº£ng giÃ¡ trá»‹**: Tá»« 0.01 Ä‘áº¿n hÆ¡n 5 tá»· Ä‘Æ¡n vá»‹

### 2.2. Quy trÃ¬nh tiá»n xá»­ lÃ½ dá»¯ liá»‡u

#### BÆ°á»›c 1: KhÃ¡m phÃ¡ dá»¯ liá»‡u (Data Exploration)
**Script**: `scripts/polars/explore_fast.py`
**Thá»i gian**: ~30 giÃ¢y
**CÃ´ng viá»‡c**:
- Äá»c nhanh 100,000 dÃ²ng Ä‘áº§u Ä‘á»ƒ hiá»ƒu cáº¥u trÃºc
- Xem kiá»ƒu dá»¯ liá»‡u cá»§a tá»«ng cá»™t (sá»‘, chuá»—i)
- Kiá»ƒm tra giÃ¡ trá»‹ thiáº¿u (missing values)
- Thá»‘ng kÃª mÃ´ táº£: min, max, mean, median
- PhÃ¢n tÃ­ch phÃ¢n phá»‘i cá»§a nhÃ£n rá»­a tiá»n

**Ká»¹ thuáº­t sá»­ dá»¥ng**:
- **Lazy Loading**: Chá»‰ Ä‘á»c metadata, khÃ´ng load toÃ n bá»™ vÃ o RAM
- **Polars DataFrame**: ThÆ° viá»‡n nhanh viáº¿t báº±ng Rust
- **Statistical Summary**: TÃ­nh toÃ¡n song song

#### BÆ°á»›c 2: TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng (Feature Engineering)
**Script**: `scripts/polars/prepare_polars.py`
**Thá»i gian**: ~10 phÃºt
**CÃ´ng viá»‡c**:

1. **PhÃ¢n tÃ­ch thá»i gian (Temporal Features)**
   - Parse chuá»—i timestamp thÃ nh datetime
   - TrÃ­ch xuáº¥t giá» trong ngÃ y (0-23)
   - TrÃ­ch xuáº¥t ngÃ y trong tuáº§n (0-6)
   - **LÃ½ do**: Rá»­a tiá»n thÆ°á»ng xáº£y ra vÃ o giá» khÃ´ng bÃ¬nh thÆ°á»ng

2. **TÃ­nh toÃ¡n tá»· lá»‡ (Ratio Features)**
   - `amount_ratio = Amount Received / Amount Paid`
   - **LÃ½ do**: Tá»· lá»‡ báº¥t thÆ°á»ng cÃ³ thá»ƒ lÃ  dáº¥u hiá»‡u rá»­a tiá»n
   - Xá»­ lÃ½ chia cho 0 (division by zero)

3. **MÃ£ hÃ³a tuyáº¿n Ä‘Æ°á»ng (Route Hash)**
   - Hash(From Bank, To Bank) â†’ má»™t sá»‘ duy nháº¥t
   - **LÃ½ do**: PhÃ¡t hiá»‡n tuyáº¿n chuyá»ƒn tiá»n láº·p láº¡i nghi ngá»

4. **MÃ£ hÃ³a biáº¿n phÃ¢n loáº¡i (Categorical Encoding)**
   - Chuyá»ƒn chuá»—i thÃ nh sá»‘ (One-Hot hoáº·c Label Encoding)
   - VÃ­ dá»¥: "US Dollar" â†’ 0, "Yuan" â†’ 1, "Bitcoin" â†’ 2
   - **LÃ½ do**: Thuáº­t toÃ¡n K-means chá»‰ lÃ m viá»‡c vá»›i sá»‘

5. **Chuáº©n hÃ³a (Normalization)**
   - Min-Max Scaling: ÄÆ°a táº¥t cáº£ vá» khoáº£ng [0, 1]
   - CÃ´ng thá»©c: `(x - min) / (max - min)`
   - **LÃ½ do**: CÃ¡c Ä‘áº·c trÆ°ng cÃ³ scale khÃ¡c nhau sáº½ áº£nh hÆ°á»Ÿng káº¿t quáº£

**Äáº§u ra**:
- File: `data/processed/hadoop_input_temp.txt` (Táº M THá»œI)
- KÃ­ch thÆ°á»›c: 33GB (sau khi normalize)
- 9 cá»™t Ä‘áº·c trÆ°ng sá»‘: `[amount_received, amount_paid, amount_ratio, hour, day_of_week, route_hash, recv_curr_encoded, payment_curr_encoded, payment_format_encoded]`
- **LÆ°u Ã½**: File nÃ y sáº½ Bá»Š XÃ“A tá»± Ä‘á»™ng sau khi upload lÃªn HDFS

#### BÆ°á»›c 3: Khá»Ÿi táº¡o tÃ¢m cá»¥m ban Ä‘áº§u (Centroid Initialization)
**Script**: `scripts/polars/init_centroids.py`
**Thá»i gian**: ~30 giÃ¢y
**CÃ´ng viá»‡c**:
- Láº¥y máº«u ngáº«u nhiÃªn 100,000 giao dá»‹ch
- Chá»n ngáº«u nhiÃªn K=5 Ä‘iá»ƒm lÃ m tÃ¢m cá»¥m ban Ä‘áº§u
- **LÃ½ do**: K-means cáº§n Ä‘iá»ƒm khá»Ÿi táº¡o Ä‘á»ƒ báº¯t Ä‘áº§u thuáº­t toÃ¡n

**Äáº§u ra**:
- File: `data/processed/centroids_temp.txt` (Táº M THá»œI)
- 5 dÃ²ng, má»—i dÃ²ng lÃ  9 sá»‘ (tá»a Ä‘á»™ cá»§a 1 tÃ¢m cá»¥m)
- **LÆ°u Ã½**: File nÃ y cÅ©ng sáº½ Bá»Š XÃ“A sau khi upload HDFS

---

<a id="p3"></a>
## PHáº¦N 3: KIáº¾N TRÃšC Há»† THá»NG

### 3.1. SÆ¡ Ä‘á»“ tá»•ng quan

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              KIáº¾N TRÃšC Há»† THá»NG PHÃ‚N TÃN                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dá»® LIá»†U    â”‚   16GB CSV (179M giao dá»‹ch)
â”‚   Äáº¦U VÃ€O    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   POLARS     â”‚   Xá»­ lÃ½ dá»¯ liá»‡u cá»¥c bá»™ (1 mÃ¡y)
â”‚  (MÃ¡y cÃ¡     â”‚   - Äá»c CSV nhanh
â”‚   nhÃ¢n)      â”‚   - Feature engineering
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   - Chuáº©n hÃ³a
       â”‚
       â”‚ Táº¡o file temp 33GB
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     HDFS     â”‚   Há»‡ thá»‘ng lÆ°u trá»¯ phÃ¢n tÃ¡n
â”‚  (Nhiá»u mÃ¡y  â”‚   - Chia nhá» thÃ nh blocks
â”‚    tÃ­nh)     â”‚   - Sao lÆ°u tá»± Ä‘á»™ng (replication)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   - Fault-tolerant
       â”‚
       â”‚ ğŸ—‘ï¸  XÃ“A file temp cá»¥c bá»™
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    SPARK     â”‚   Xá»­ lÃ½ phÃ¢n tÃ¡n song song
â”‚  (Cluster)   â”‚   - Äá»c tá»« HDFS
â”‚              â”‚   - K-means trong RAM
â”‚  [Master]    â”‚   - LÆ°u káº¿t quáº£ vá» HDFS
â”‚  [Worker 1]  â”‚
â”‚  [Worker 2]  â”‚
â”‚  [Worker 3]  â”‚
â”‚  [Worker 4]  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Káº¾T QUáº¢     â”‚   File nhá» (~4KB)
â”‚  (Cá»¥c bá»™)    â”‚   - 5 tÃ¢m cá»¥m cuá»‘i cÃ¹ng
â”‚              â”‚   - BÃ¡o cÃ¡o phÃ¢n tÃ­ch
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2. Giáº£i thÃ­ch cÃ¡c thÃ nh pháº§n

#### Polars - Xá»­ lÃ½ dá»¯ liá»‡u nhanh
**Vai trÃ²**: Äá»c vÃ  xá»­ lÃ½ CSV á»Ÿ mÃ¡y cá»¥c bá»™
**Táº¡i sao dÃ¹ng Polars**:
- Nhanh hÆ¡n Pandas 10-100 láº§n
- Viáº¿t báº±ng Rust (ngÃ´n ngá»¯ hiá»‡u suáº¥t cao)
- Há»— trá»£ lazy evaluation (tÃ­nh toÃ¡n khi cáº§n)
- Xá»­ lÃ½ Ä‘Æ°á»£c file lá»›n hÆ¡n RAM

**So sÃ¡nh vá»›i Pandas**:
```
Pandas:  Äá»c 16GB CSV â†’ 45 phÃºt
Polars:  Äá»c 16GB CSV â†’ 4-5 phÃºt âš¡
```

#### HDFS - LÆ°u trá»¯ phÃ¢n tÃ¡n
**Vai trÃ²**: LÆ°u trá»¯ file lá»›n trÃªn nhiá»u mÃ¡y
**CÃ¡ch hoáº¡t Ä‘á»™ng**:
1. File 33GB Ä‘Æ°á»£c chia thÃ nh cÃ¡c block 128MB
2. Má»—i block Ä‘Æ°á»£c sao lÆ°u 3 báº£n trÃªn 3 mÃ¡y khÃ¡c nhau
3. Náº¿u 1 mÃ¡y há»ng, váº«n cÃ²n 2 báº£n sao khÃ¡c

**Cáº¥u trÃºc thÆ° má»¥c HDFS trong dá»± Ã¡n**:
```
/user/spark/hi_large/
â”œâ”€â”€ input/
â”‚   â””â”€â”€ hadoop_input.txt          (33GB - Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½)
â”œâ”€â”€ centroids.txt                 (440 bytes - TÃ¢m cá»¥m ban Ä‘áº§u)
â””â”€â”€ output_centroids/             (ThÆ° má»¥c káº¿t quáº£)
    â””â”€â”€ part-00000                (TÃ¢m cá»¥m cuá»‘i cÃ¹ng)
```

**Lá»£i Ã­ch**:
- âœ… KhÃ´ng giá»›i háº¡n dung lÆ°á»£ng (thÃªm mÃ¡y = thÃªm khÃ´ng gian)
- âœ… An toÃ n (replication)
- âœ… TuÃ¢n thá»§ quy Ä‘á»‹nh (khÃ´ng lÆ°u local)

#### Apache Spark - Xá»­ lÃ½ phÃ¢n tÃ¡n
**Vai trÃ²**: Cháº¡y K-means trÃªn nhiá»u mÃ¡y song song
**Kiáº¿n trÃºc Spark**:

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   MASTER    â”‚  â† Äiá»u phá»‘i cÃ´ng viá»‡c
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
               â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚        â”‚        â”‚
   â”Œâ”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”
   â”‚ W1  â”‚  â”‚ W2  â”‚  â”‚ W3  â”‚  â† Workers (CÃ´ng nhÃ¢n)
   â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜
   44M rows 44M rows 44M rows   (Chia Ä‘á»u dá»¯ liá»‡u)
```

**CÃ¡ch Spark xá»­ lÃ½ K-means**:
1. **PhÃ¢n chia dá»¯ liá»‡u**: 179M rows â†’ 4 pháº§n (4 workers)
2. **Xá»­ lÃ½ song song**: Má»—i worker tÃ­nh khoáº£ng cÃ¡ch cá»§a pháº§n cá»§a mÃ¬nh
3. **Tá»•ng há»£p**: Master thu tháº­p káº¿t quáº£ vÃ  cáº­p nháº­t tÃ¢m cá»¥m
4. **Láº·p láº¡i**: 15 láº§n cho Ä‘áº¿n khi há»™i tá»¥

**Táº¡i sao Spark nhanh**:
- **In-memory computing**: Giá»¯ dá»¯ liá»‡u trong RAM, khÃ´ng ghi disk
- **Lazy evaluation**: Chá»‰ tÃ­nh toÃ¡n khi cáº§n thiáº¿t
- **Pipeline optimization**: Tá»± Ä‘á»™ng tá»‘i Æ°u chuá»—i cÃ¡c phÃ©p toÃ¡n

**Cáº¥u hÃ¬nh Spark trong dá»± Ã¡n**:
- **Driver memory**: 4GB (bá»™ nhá»› chÆ°Æ¡ng trÃ¬nh chÃ­nh)
- **Executor memory**: 4GB Ã— 4 = 16GB (bá»™ nhá»› workers)
- **Cores**: 4 cores/worker Ã— 4 workers = 16 cores
- **Parallelism**: Xá»­ lÃ½ 16 partition cÃ¹ng lÃºc

---

<a id="p4"></a>
## PHáº¦N 4: QUY TRÃŒNH Xá»¬ LÃ (PIPELINE)

### 4.1. Tá»•ng quan quy trÃ¬nh 8 bÆ°á»›c

```
BÆ¯á»šC 1        BÆ¯á»šC 2        BÆ¯á»šC 3        BÆ¯á»šC 4
KhÃ¡m phÃ¡  â†’   Xá»­ lÃ½    â†’   Khá»Ÿi táº¡o  â†’   Upload
 (30s)        (10 phÃºt)      (30s)       (5 phÃºt)

BÆ¯á»šC 5        BÆ¯á»šC 6        BÆ¯á»šC 7        BÆ¯á»šC 8
K-means   â†’   Táº£i vá»   â†’   GÃ¡n nhÃ£n  â†’   PhÃ¢n tÃ­ch
(15-30p)       (30s)       (10 phÃºt)     (2 phÃºt)

Tá»”NG THá»œI GIAN: 40-60 phÃºt
```

### 4.2. Chi tiáº¿t tá»«ng bÆ°á»›c

#### BÆ¯á»šC 1: KhÃ¡m phÃ¡ dá»¯ liá»‡u ğŸ”
**Má»¥c Ä‘Ã­ch**: Hiá»ƒu cáº¥u trÃºc vÃ  Ä‘áº·c Ä‘iá»ƒm cá»§a dá»¯ liá»‡u
**File thá»±c thi**: `scripts/polars/explore_fast.py`
**Thá»i gian**: ~30 giÃ¢y
**Input**: `data/raw/HI-Large_Trans.csv` (16GB)
**Output**: Thá»‘ng kÃª in ra mÃ n hÃ¬nh

**CÃ¡c phÃ¢n tÃ­ch thá»±c hiá»‡n**:
1. Äá»c 100,000 dÃ²ng Ä‘áº§u (Ä‘áº¡i diá»‡n)
2. Xem schema: TÃªn cá»™t, kiá»ƒu dá»¯ liá»‡u
3. Thá»‘ng kÃª mÃ´ táº£: min, max, mean, median, std
4. PhÃ¢n tÃ­ch nhÃ£n: Bao nhiÃªu % rá»­a tiá»n?
5. Top loáº¡i tiá»n tá»‡ phá»• biáº¿n

**Káº¿t quáº£ vÃ­ dá»¥**:
```
Total rows: 179,702,229
Laundering rate: 0.126%
Top currencies: Euro (23%), Yuan (7.2%)
```

#### BÆ¯á»šC 2: Xá»­ lÃ½ vÃ  trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng ğŸ”§
**Má»¥c Ä‘Ã­ch**: Chuyá»ƒn dá»¯ liá»‡u thÃ´ thÃ nh dáº¡ng sá»‘ Ä‘á»ƒ thuáº­t toÃ¡n xá»­ lÃ½
**File thá»±c thi**: `scripts/polars/prepare_polars.py`
**Thá»i gian**: ~10 phÃºt
**Input**: `data/raw/HI-Large_Trans.csv` (16GB)
**Output**: `data/processed/hadoop_input_temp.txt` (33GB, Táº M THá»œI)

**CÃ¡c bÆ°á»›c xá»­ lÃ½**:
1. **Parse timestamp**: "2022/08/01 00:17" â†’ giá»=0, ngÃ y=0 (Thá»© 2)
2. **TÃ­nh ratio**: amount_ratio = 6794.63 / 7739.29 = 0.878
3. **Hash route**: hash(20, 20) = 400 (vÃ­ dá»¥)
4. **Encode currency**: "US Dollar" â†’ 0, "Yuan" â†’ 1
5. **Normalize**: ÄÆ°a táº¥t cáº£ vá» [0, 1]

**Táº¡i sao láº¡i tÄƒng tá»« 16GB lÃªn 33GB?**
- Dá»¯ liá»‡u gá»‘c: Chá»‰ cÃ³ 11 cá»™t
- Sau xá»­ lÃ½: ThÃªm nhiá»u cá»™t Ä‘áº·c trÆ°ng
- Má»—i sá»‘ float64 = 8 bytes
- 179M rows Ã— 9 features Ã— 8 bytes â‰ˆ 12GB + overhead â‰ˆ 33GB

#### BÆ¯á»šC 3: Khá»Ÿi táº¡o tÃ¢m cá»¥m ğŸ¯
**Má»¥c Ä‘Ã­ch**: Chá»n Ä‘iá»ƒm báº¯t Ä‘áº§u cho thuáº­t toÃ¡n K-means
**File thá»±c thi**: `scripts/polars/init_centroids.py`
**Thá»i gian**: ~30 giÃ¢y
**Input**: `data/processed/hadoop_input_temp.txt`
**Output**: `data/processed/centroids_temp.txt` (440 bytes)

**Thuáº­t toÃ¡n**:
1. Sample ngáº«u nhiÃªn 100,000 dÃ²ng
2. Chá»n ngáº«u nhiÃªn K=5 dÃ²ng lÃ m tÃ¢m cá»¥m
3. LÆ°u 5 dÃ²ng nÃ y vÃ o file

**VÃ­ dá»¥ tÃ¢m cá»¥m**:
```
Cluster 0: [0.12, 0.34, 0.56, 0.78, 0.90, 0.11, 0.33, 0.55, 0.77]
Cluster 1: [0.88, 0.22, 0.44, 0.66, 0.11, 0.99, 0.22, 0.44, 0.66]
...
```

#### BÆ¯á»šC 4: Upload lÃªn HDFS â˜ï¸
**Má»¥c Ä‘Ã­ch**: Chuyá»ƒn dá»¯ liá»‡u lÃªn há»‡ thá»‘ng lÆ°u trá»¯ phÃ¢n tÃ¡n
**File thá»±c thi**: `scripts/spark/setup_hdfs.sh`
**Thá»i gian**: ~5 phÃºt
**Input**: 2 file temp cá»¥c bá»™
**Output**: Dá»¯ liá»‡u trÃªn HDFS

**CÃ¡c bÆ°á»›c thá»±c hiá»‡n**:
1. Kiá»ƒm tra HDFS Ä‘ang cháº¡y: `hdfs dfsadmin -report`
2. Táº¡o thÆ° má»¥c: `hdfs dfs -mkdir -p /user/spark/hi_large/input`
3. Upload input: `hdfs dfs -put hadoop_input_temp.txt /user/.../input/`
4. Upload centroids: `hdfs dfs -put centroids_temp.txt /user/.../`
5. **XÃ“A file temp cá»¥c bá»™**: `rm -rf data/processed/*`
6. Verify: Kiá»ƒm tra kÃ­ch thÆ°á»›c file trÃªn HDFS

**ğŸ”’ TuÃ¢n thá»§ quy Ä‘á»‹nh**:
- Sau bÆ°á»›c nÃ y, KHÃ”NG cÃ²n dá»¯ liá»‡u lá»›n á»Ÿ mÃ¡y cá»¥c bá»™
- Chá»‰ tá»“n táº¡i trÃªn HDFS (phÃ¢n tÃ¡n, an toÃ n)
- Náº¿u cáº§n, cÃ³ thá»ƒ táº£i láº¡i tá»« HDFS

#### BÆ¯á»šC 5: Cháº¡y K-means trÃªn Spark ğŸš€
**Má»¥c Ä‘Ã­ch**: PhÃ¢n cá»¥m 179 triá»‡u giao dá»‹ch
**File thá»±c thi**: `scripts/spark/run_spark.sh` + `kmeans_spark.py`
**Thá»i gian**: 15-30 phÃºt (tÃ¹y pháº§n cá»©ng)
**Input**: Dá»¯ liá»‡u tá»« HDFS
**Output**: TÃ¢m cá»¥m cuá»‘i cÃ¹ng trÃªn HDFS

**Thuáº­t toÃ¡n K-means**:
```
KHá» Táº O:
  - K=5 tÃ¢m cá»¥m ban Ä‘áº§u (tá»« bÆ°á»›c 3)
  - Max iterations = 15

Láº¶P Láº I 15 Láº¦N:
  1. GÃ¡n má»—i giao dá»‹ch vÃ o cá»¥m gáº§n nháº¥t
     - TÃ­nh khoáº£ng cÃ¡ch Euclidean Ä‘áº¿n 5 tÃ¢m cá»¥m
     - Chá»n cá»¥m cÃ³ khoáº£ng cÃ¡ch nhá» nháº¥t
  
  2. Cáº­p nháº­t tÃ¢m cá»¥m
     - TÃ­nh trung bÃ¬nh táº¥t cáº£ Ä‘iá»ƒm trong má»—i cá»¥m
     - TÃ¢m cá»¥m má»›i = trung bÃ¬nh cÃ¡c Ä‘iá»ƒm
  
  3. Kiá»ƒm tra há»™i tá»¥
     - TÃ­nh Ä‘á»™ dá»‹ch chuyá»ƒn tÃ¢m cá»¥m
     - Náº¿u < threshold â†’ Dá»«ng láº¡i

Káº¾T QUáº¢:
  - 5 tÃ¢m cá»¥m cuá»‘i cÃ¹ng
  - Má»—i cá»¥m chá»©a bao nhiÃªu Ä‘iá»ƒm
```

**QuÃ¡ trÃ¬nh há»™i tá»¥ (tá»« log thá»±c táº¿)**:
```
Iteration  1: Centroid shift = 2.232  (chÆ°a á»•n Ä‘á»‹nh)
Iteration  2: Centroid shift = 1.409
Iteration  5: Centroid shift = 0.383
Iteration 10: Centroid shift = 0.046
Iteration 15: Centroid shift = 0.010  (Ä‘Ã£ há»™i tá»¥ âœ“)
```

**PhÃ¢n phá»‘i káº¿t quáº£**:
```
Cluster 0:  40,034,828 giao dá»‹ch (22.28%)
Cluster 1:  42,665,741 giao dá»‹ch (23.74%)
Cluster 2:  24,884,738 giao dá»‹ch (13.85%)
Cluster 3:  50,933,660 giao dá»‹ch (28.34%)  â† Lá»›n nháº¥t
Cluster 4:  21,183,262 giao dá»‹ch (11.79%)
```

#### BÆ¯á»šC 6: Táº£i káº¿t quáº£ vá» ğŸ“¥
**Má»¥c Ä‘Ã­ch**: Láº¥y tÃ¢m cá»¥m cuá»‘i cÃ¹ng tá»« HDFS
**File thá»±c thi**: `scripts/spark/download_from_hdfs.sh`
**Thá»i gian**: ~30 giÃ¢y
**Input**: `/user/spark/hi_large/output_centroids/` trÃªn HDFS
**Output**: `data/results/final_centroids.txt` (~4KB)

**CÃ¡c bÆ°á»›c**:
1. `hdfs dfs -cat /user/.../output_centroids/part-*`
2. LÆ°u vÃ o file cá»¥c bá»™
3. Verify: Kiá»ƒm tra cÃ³ Ä‘Ãºng 5 dÃ²ng

**Táº¡i sao Ä‘Æ°á»£c phÃ©p táº£i vá»?**
- File ráº¥t nhá» (~4KB)
- Chá»‰ chá»©a káº¿t quáº£ tá»•ng há»£p, khÃ´ng pháº£i dá»¯ liá»‡u gá»‘c
- Cáº§n thiáº¿t cho bÆ°á»›c phÃ¢n tÃ­ch tiáº¿p theo

#### BÆ¯á»šC 7: GÃ¡n nhÃ£n cá»¥m cho tá»«ng giao dá»‹ch ğŸ·ï¸
**Má»¥c Ä‘Ã­ch**: XÃ¡c Ä‘á»‹nh má»—i giao dá»‹ch thuá»™c cá»¥m nÃ o
**File thá»±c thi**: `scripts/polars/assign_clusters_polars.py`
**Thá»i gian**: ~10 phÃºt
**Input**: 
  - CSV gá»‘c tá»« HDFS (streaming)
  - 5 tÃ¢m cá»¥m tá»« bÆ°á»›c 6
**Output**: `data/results/clustered_results.txt`

**Thuáº­t toÃ¡n**:
```python
FOR má»—i giao dá»‹ch:
    distances = []
    FOR má»—i tÃ¢m cá»¥m (5 cá»¥m):
        d = euclidean_distance(giao_dá»‹ch, tÃ¢m_cá»¥m)
        distances.append(d)
    
    cluster_id = argmin(distances)  # Chá»n cá»¥m gáº§n nháº¥t
    ghi_káº¿t_quáº£(giao_dá»‹ch, cluster_id)
```

**Xá»­ lÃ½ batch Ä‘á»ƒ tÄƒng tá»‘c**:
- KhÃ´ng xá»­ lÃ½ tá»«ng giao dá»‹ch
- Xá»­ lÃ½ 1 triá»‡u giao dá»‹ch cÃ¹ng lÃºc
- Sá»­ dá»¥ng NumPy vectorization

#### BÆ¯á»šC 8: PhÃ¢n tÃ­ch káº¿t quáº£ ğŸ“Š
**Má»¥c Ä‘Ã­ch**: TÃ¬m cá»¥m cÃ³ tá»· lá»‡ rá»­a tiá»n cao
**File thá»±c thi**: `scripts/polars/analyze_polars.py`
**Thá»i gian**: ~2 phÃºt
**Input**: `data/results/clustered_results.txt`
**Output**: BÃ¡o cÃ¡o phÃ¢n tÃ­ch

**CÃ¡c phÃ¢n tÃ­ch thá»±c hiá»‡n**:
1. **KÃ­ch thÆ°á»›c cá»¥m**: Má»—i cá»¥m cÃ³ bao nhiÃªu giao dá»‹ch?
2. **Tá»· lá»‡ rá»­a tiá»n**: % rá»­a tiá»n trong tá»«ng cá»¥m
3. **High-risk clusters**: Cá»¥m nÃ o > 10% rá»­a tiá»n?
4. **Feature averages**: Äáº·c Ä‘iá»ƒm trung bÃ¬nh má»—i cá»¥m

**Káº¿t quáº£ tá»« log thá»±c táº¿**:
```
â•”â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Cluster  â•‘ Giao dá»‹ch   â•‘ Rá»­a tiá»n  â•‘ Tá»· lá»‡ (%)       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘    0     â•‘ 40,034,832  â•‘  52,327   â•‘ 0.13%           â•‘
â•‘    1     â•‘ 42,665,746  â•‘  70,450   â•‘ 0.17% â† CAO     â•‘
â•‘    2     â•‘ 24,884,738  â•‘  16,686   â•‘ 0.07%           â•‘
â•‘    3     â•‘ 50,933,651  â•‘  82,943   â•‘ 0.16%           â•‘
â•‘    4     â•‘ 21,183,262  â•‘   3,140   â•‘ 0.01% â† THáº¤P    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ’¡ NHáº¬N XÃ‰T:
- Cluster 1 nghi ngá» nháº¥t (0.17%, cao hÆ¡n trung bÃ¬nh)
- Cluster 4 an toÃ n nháº¥t (0.01%, tháº¥p hÆ¡n nhiá»u)
- KHÃ”NG cÃ³ cá»¥m nÃ o > 10% (good sign)
```

---

<a id="p5"></a>
## PHáº¦N 5: Káº¾T QUáº¢ VÃ€ ÄÃNH GIÃ

### 5.1. Káº¿t quáº£ phÃ¢n cá»¥m

#### Thá»‘ng kÃª tá»•ng quan
- **Tá»•ng giao dá»‹ch xá»­ lÃ½**: 179,702,229
- **Sá»‘ cá»¥m**: 5
- **Sá»‘ vÃ²ng láº·p**: 15
- **Thá»i gian cháº¡y**: 32 phÃºt 56 giÃ¢y
- **Convergence**: Äáº¡t Ä‘Æ°á»£c (shift < 0.01)

#### PhÃ¢n tÃ­ch chi tiáº¿t tá»«ng cá»¥m

**ğŸ”µ Cluster 0 - Cá»¥m Giao Dá»‹ch Vá»«a Pháº£i**
- Sá»‘ lÆ°á»£ng: 40,034,832 (22.28%)
- Rá»­a tiá»n: 52,327 giao dá»‹ch (0.13%)
- Äáº·c Ä‘iá»ƒm:
  - GiÃ¡ trá»‹ trung bÃ¬nh: 5.49M
  - Tá»· lá»‡ received/paid: 1.47
  - ÄÃ¡nh giÃ¡: **Rá»¦I RO TRUNG BÃŒNH**

**ğŸŸ¢ Cluster 1 - Cá»¥m Rá»§i Ro Cao Nháº¥t**
- Sá»‘ lÆ°á»£ng: 42,665,746 (23.74%)
- Rá»­a tiá»n: 70,450 giao dá»‹ch (0.17%) âš ï¸
- Äáº·c Ä‘iá»ƒm:
  - GiÃ¡ trá»‹ trung bÃ¬nh: 6.11M (cao)
  - Tá»· lá»‡ received/paid: 1.59
  - ÄÃ¡nh giÃ¡: **Cáº¦N KIá»‚M TRA Ká»¸**

**ğŸŸ¡ Cluster 2 - Cá»¥m Trao Äá»•i Tiá»n Tá»‡**
- Sá»‘ lÆ°á»£ng: 24,884,738 (13.85%)
- Rá»­a tiá»n: 16,686 giao dá»‹ch (0.07%) âœ“
- Äáº·c Ä‘iá»ƒm:
  - GiÃ¡ trá»‹ trung bÃ¬nh: 5.64M
  - Tá»· lá»‡ received/paid: 1.02 (gáº§n báº±ng 1)
  - ÄÃ¡nh giÃ¡: **Rá»¦I RO THáº¤P** (trao Ä‘á»•i tiá»n tá»‡ bÃ¬nh thÆ°á»ng)

**ğŸ”´ Cluster 3 - Cá»¥m Lá»›n Nháº¥t**
- Sá»‘ lÆ°á»£ng: 50,933,651 (28.34%) â† ÄÃ´ng nháº¥t
- Rá»­a tiá»n: 82,943 giao dá»‹ch (0.16%)
- Äáº·c Ä‘iá»ƒm:
  - GiÃ¡ trá»‹ trung bÃ¬nh: 1.95M (tháº¥p)
  - Tá»· lá»‡ received/paid: 1.00 (báº±ng nhau)
  - ÄÃ¡nh giÃ¡: **GIAO Dá»ŠCH BÃŒNH THÆ¯á»œNG** (sá»‘ lÆ°á»£ng nhá» láº»)

**ğŸŸ£ Cluster 4 - Cá»¥m An ToÃ n Nháº¥t**
- Sá»‘ lÆ°á»£ng: 21,183,262 (11.79%)
- Rá»­a tiá»n: 3,140 giao dá»‹ch (0.01%) âœ“âœ“âœ“
- Äáº·c Ä‘iá»ƒm:
  - GiÃ¡ trá»‹ trung bÃ¬nh: 13.56M (ráº¥t cao)
  - Tá»· lá»‡ received/paid: 6.87
  - ÄÃ¡nh giÃ¡: **Rá»¦I RO Ráº¤T THáº¤P** (cÃ³ thá»ƒ lÃ  Ä‘áº§u tÆ° há»£p phÃ¡p)

### 5.2. Nháº­n xÃ©t vÃ  Insights

#### PhÃ¡t hiá»‡n chÃ­nh
1. **KhÃ´ng cÃ³ cá»¥m nÃ o cÃ³ tá»· lá»‡ rá»­a tiá»n > 10%**
   - ÄÃ¢y lÃ  dáº¥u hiá»‡u Tá»T
   - KhÃ´ng cÃ³ pattern rá»­a tiá»n rÃµ rÃ ng
   - Há»‡ thá»‘ng ngÃ¢n hÃ ng kiá»ƒm soÃ¡t tá»‘t

2. **Cluster 1 cáº§n chÃº Ã½**
   - Tá»· lá»‡ 0.17% cao hÆ¡n trung bÃ¬nh (0.13%)
   - GiÃ¡ trá»‹ giao dá»‹ch cao (6.11M)
   - Khuyáº¿n nghá»‹: Kiá»ƒm tra thá»§ cÃ´ng cÃ¡c giao dá»‹ch trong cá»¥m nÃ y

3. **Cluster 4 ráº¥t an toÃ n**
   - Chá»‰ 0.01% rá»­a tiá»n (tháº¥p nháº¥t)
   - CÃ³ thá»ƒ bá» qua khi kiá»ƒm tra

4. **PhÃ¢n phá»‘i Ä‘á»u**
   - KhÃ´ng cÃ³ cá»¥m quÃ¡ nhá» (<10%)
   - Thuáº­t toÃ¡n phÃ¢n chia tá»‘t

#### So sÃ¡nh vá»›i ngÆ°á»¡ng
```
NgÆ°á»¡ng cáº£nh bÃ¡o: > 10% rá»­a tiá»n

Cluster 0: 0.13% â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” OK
Cluster 1: 0.17% â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” OK (nhÆ°ng cao nháº¥t)
Cluster 2: 0.07% â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” OK
Cluster 3: 0.16% â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” OK
Cluster 4: 0.01% â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” OK (tháº¥p nháº¥t)
```

### 5.3. Hiá»‡u suáº¥t há»‡ thá»‘ng

#### Thá»i gian xá»­ lÃ½ chi tiáº¿t
| BÆ°á»›c | CÃ´ng viá»‡c | Thá»i gian | % Tá»•ng |
|------|-----------|-----------|--------|
| 1 | KhÃ¡m phÃ¡ | 12s | 0.6% |
| 2 | Feature Engineering | 64s | 3.2% |
| 3 | Khá»Ÿi táº¡o Centroids | 23s | 1.2% |
| 4 | Upload HDFS | 42s | 2.1% |
| 5 | **Spark K-means** | **1590s** | **80.5%** |
| 6 | Download | 3s | 0.2% |
| 7 | GÃ¡n nhÃ£n | 194s | 9.8% |
| 8 | PhÃ¢n tÃ­ch | 48s | 2.4% |
| **Tá»”NG** | | **1976s (33 phÃºt)** | **100%** |

**Nháº­n xÃ©t**:
- K-means chiáº¿m 80.5% thá»i gian (Ä‘iá»u nÃ y lÃ  bÃ¬nh thÆ°á»ng)
- CÃ¡c bÆ°á»›c cÃ²n láº¡i ráº¥t nhanh nhá» Polars

#### So sÃ¡nh vá»›i Hadoop MapReduce
| TiÃªu chÃ­ | Hadoop (Legacy) | Spark (Hiá»‡n táº¡i) | Cáº£i thiá»‡n |
|----------|-----------------|------------------|-----------|
| Thá»i gian K-means | 1-2 giá» | 26 phÃºt | **4-8x nhanh hÆ¡n** |
| RAM sá»­ dá»¥ng | Ãt (disk-based) | Nhiá»u (in-memory) | Trade-off |
| Äá»™ phá»©c táº¡p code | Cao (mapper/reducer) | Tháº¥p (PySpark API) | Dá»… maintain |
| Debug | KhÃ³ | Dá»… (local mode) | Developer friendly |

**Káº¿t luáº­n**: Spark lÃ  lá»±a chá»n Ä‘Ãºng Ä‘áº¯n cho K-means iterative!

---

<a id="p6"></a>
## PHáº¦N 6: TUÃ‚N THá»¦ QUY Äá»ŠNH Báº¢O Máº¬T

### 6.1. Quy Ä‘á»‹nh: KHÃ”NG lÆ°u dá»¯ liá»‡u lá»›n á»Ÿ mÃ¡y cá»¥c bá»™

#### LÃ½ do cÃ³ quy Ä‘á»‹nh nÃ y
1. **Báº£o máº­t**: Dá»¯ liá»‡u khÃ¡ch hÃ ng nháº¡y cáº£m
2. **TuÃ¢n thá»§ phÃ¡p luáº­t**: GDPR, CCPA, v.v.
3. **NgÄƒn cháº·n rÃ² rá»‰**: MÃ¡y cÃ¡ nhÃ¢n dá»… bá»‹ hack
4. **Kiá»ƒm soÃ¡t truy cáº­p**: HDFS cÃ³ authentication

### 6.2. CÃ¡ch dá»± Ã¡n tuÃ¢n thá»§

#### âœ… ÄÆ¯á»¢C PHÃ‰P lÆ°u á»Ÿ mÃ¡y cá»¥c bá»™
```
data/
â”œâ”€â”€ raw/
â”‚   â””â”€â”€ HI-Large_Trans.csv     âœ“ (File gá»‘c tá»« giáº£ng viÃªn)
â”‚
â””â”€â”€ results/
    â”œâ”€â”€ final_centroids.txt    âœ“ (Chá»‰ 4KB - káº¿t quáº£ tá»•ng há»£p)
    â””â”€â”€ clustered_results.txt  âœ“ (CÃ³ thá»ƒ táº¡o láº¡i tá»« HDFS)
```

#### âŒ KHÃ”NG ÄÆ¯á»¢C lÆ°u á»Ÿ mÃ¡y cá»¥c bá»™
```
data/
â””â”€â”€ processed/
    â”œâ”€â”€ hadoop_input_temp.txt  âŒ (33GB - Tá»° Äá»˜NG XÃ“A)
    â””â”€â”€ centroids_temp.txt     âŒ (440B - Tá»° Äá»˜NG XÃ“A)
```

#### CÆ¡ cháº¿ tá»± Ä‘á»™ng xÃ³a
**Trong file** `scripts/spark/setup_hdfs.sh`:
```bash
# Upload lÃªn HDFS
hdfs dfs -put data/processed/hadoop_input_temp.txt /user/spark/hi_large/

# XÃ“A NGAY SAU KHI UPLOAD THÃ€NH CÃ”NG
echo "Cleaning up temp files..."
rm -rf "$PROJECT_ROOT/data/processed/"*

echo "âœ… Temp files deleted (data now only on HDFS)"
```

#### Verification (Kiá»ƒm chá»©ng)
**Kiá»ƒm tra trÆ°á»›c khi upload**:
```bash
$ du -sh data/processed/
33G    data/processed/  â† CÃ³ file temp
```

**Kiá»ƒm tra sau khi upload**:
```bash
$ du -sh data/processed/
0      data/processed/  â† ÄÃ£ xÃ³a sáº¡ch! âœ“

$ hdfs dfs -du -h /user/spark/hi_large/
31.0 G  /user/spark/hi_large/input/hadoop_input.txt  â† TrÃªn HDFS
```

### 6.3. Quy trÃ¬nh khÃ´i phá»¥c (náº¿u cáº§n)
Náº¿u cáº§n xem láº¡i dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½:
```bash
# Táº£i vá» tá»« HDFS
hdfs dfs -get /user/spark/hi_large/input/hadoop_input.txt data/processed/

# Sá»­ dá»¥ng
python scripts/polars/analyze_polars.py

# XÃ³a láº¡i sau khi dÃ¹ng xong
rm data/processed/hadoop_input.txt
```

---

<a id="p7"></a>
## PHáº¦N 7: HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG

### 7.1. YÃªu cáº§u há»‡ thá»‘ng

#### Pháº§n cá»©ng tá»‘i thiá»ƒu
- **CPU**: 4 cores (khuyáº¿n nghá»‹ 8+ cores)
- **RAM**: 16GB (khuyáº¿n nghá»‹ 32GB)
- **á»” cá»©ng**: 50GB trá»‘ng (cho HDFS)
- **Máº¡ng**: Náº¿u dÃ¹ng cluster, cáº§n LAN tá»‘c Ä‘á»™ cao

#### Pháº§n má»m
- **Há»‡ Ä‘iá»u hÃ nh**: Linux (Ubuntu, CentOS, Arch)
- **Java**: JDK 11 hoáº·c 17
- **Python**: 3.12+
- **Hadoop**: 3.x (HDFS)
- **Spark**: 4.0.1

#### ThÆ° viá»‡n Python
```bash
polars==0.20.x   # DataFrame library
numpy==1.26.x    # Numerical computing
pyspark==4.0.x   # Spark Python API
```

### 7.2. HÆ°á»›ng dáº«n cÃ i Ä‘áº·t tá»« Ä‘áº§u

#### BÆ°á»›c 1: CÃ i Ä‘áº·t Java
```bash
# Ubuntu/Debian
sudo apt update
sudo apt install openjdk-17-jdk

# Arch Linux
sudo pacman -S jdk17-openjdk

# Kiá»ƒm tra
java -version  # Pháº£i tháº¥y version 17.x.x
```

#### BÆ°á»›c 2: CÃ i Ä‘áº·t Hadoop
```bash
# Download Hadoop
cd /tmp
wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
tar -xzf hadoop-3.3.6.tar.gz
sudo mv hadoop-3.3.6 /opt/hadoop

# Cáº¥u hÃ¬nh biáº¿n mÃ´i trÆ°á»ng (~/.bashrc hoáº·c ~/.zshrc)
export HADOOP_HOME=/opt/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

# Reload
source ~/.zshrc

# Kiá»ƒm tra
hadoop version
```

#### BÆ°á»›c 3: CÃ i Ä‘áº·t Spark (tá»± Ä‘á»™ng)
```bash
cd /home/ultimatebrok/Downloads/Final
./scripts/setup/install_spark.sh

# Script sáº½ tá»± Ä‘á»™ng:
# - Download Spark 4.0.1
# - Giáº£i nÃ©n vÃ o /opt/spark
# - ThÃªm vÃ o PATH
# - Cáº¥u hÃ¬nh SPARK_HOME

# Reload shell
source ~/.zshrc

# Kiá»ƒm tra
spark-submit --version
```

#### BÆ°á»›c 4: CÃ i Ä‘áº·t Python packages
```bash
# Táº¡o virtual environment (khuyáº¿n nghá»‹)
python3 -m venv .venv
source .venv/bin/activate

# CÃ i Ä‘áº·t
pip install polars numpy pyspark

# Kiá»ƒm tra
python -c "import polars; print(polars.__version__)"
```

#### BÆ°á»›c 5: Khá»Ÿi Ä‘á»™ng HDFS
```bash
# Format namenode (CHá»ˆ Láº¦N Äáº¦U)
hdfs namenode -format

# Khá»Ÿi Ä‘á»™ng HDFS
start-dfs.sh

# Kiá»ƒm tra
hdfs dfsadmin -report
# Pháº£i tháº¥y "Live datanodes (1)"
```

### 7.3. Cháº¡y pipeline

#### CÃ¡ch 1: Tá»± Ä‘á»™ng (Khuyáº¿n nghá»‹)
```bash
cd /home/ultimatebrok/Downloads/Final

# Äáº£m báº£o cÃ³ file CSV
ls -lh data/raw/HI-Large_Trans.csv

# Cháº¡y toÃ n bá»™ pipeline
./scripts/pipeline/full_pipeline_spark.sh

# Pipeline sáº½ tá»± Ä‘á»™ng cháº¡y 8 bÆ°á»›c
# Thá»i gian: 40-60 phÃºt
# Log: logs/pipeline_log_YYYYMMDD_HHMMSS.md
```

#### CÃ¡ch 2: Tá»«ng bÆ°á»›c (Debug)
```bash
# BÆ°á»›c 1
python scripts/polars/explore_fast.py

# BÆ°á»›c 2
python scripts/polars/prepare_polars.py

# BÆ°á»›c 3
python scripts/polars/init_centroids.py

# BÆ°á»›c 4
scripts/spark/setup_hdfs.sh

# BÆ°á»›c 5
scripts/spark/run_spark.sh

# BÆ°á»›c 6
scripts/spark/download_from_hdfs.sh

# BÆ°á»›c 7
python scripts/polars/assign_clusters_polars.py

# BÆ°á»›c 8
python scripts/polars/analyze_polars.py
```

### 7.4. Xem káº¿t quáº£

```bash
# Xem log pipeline
cat logs/pipeline_log_*.md

# Xem tÃ¢m cá»¥m cuá»‘i cÃ¹ng
cat data/results/final_centroids.txt

# Xem dá»¯ liá»‡u Ä‘Ã£ gÃ¡n nhÃ£n (10 dÃ²ng Ä‘áº§u)
head data/results/clustered_results.txt
```

---

<a id="p8"></a>
## PHáº¦N 8: Xá»¬ LÃ Sá»° Cá»

### 8.1. Lá»—i thÆ°á»ng gáº·p

#### Lá»—i 1: HDFS khÃ´ng khá»Ÿi Ä‘á»™ng Ä‘Æ°á»£c
**Triá»‡u chá»©ng**:
```
hdfs dfsadmin -report
Connection refused
```

**NguyÃªn nhÃ¢n**: HDFS chÆ°a Ä‘Æ°á»£c khá»Ÿi Ä‘á»™ng
**Giáº£i phÃ¡p**:
```bash
# Kiá»ƒm tra process
jps  # Pháº£i tháº¥y NameNode vÃ  DataNode

# Náº¿u khÃ´ng tháº¥y, khá»Ÿi Ä‘á»™ng láº¡i
stop-dfs.sh
start-dfs.sh

# Äá»£i 10 giÃ¢y rá»“i kiá»ƒm tra
hdfs dfsadmin -report
```

#### Lá»—i 2: Out of Memory trong Spark
**Triá»‡u chá»©ng**:
```
java.lang.OutOfMemoryError: Java heap space
```

**NguyÃªn nhÃ¢n**: RAM khÃ´ng Ä‘á»§ cho executor
**Giáº£i phÃ¡p**: TÄƒng memory trong `scripts/spark/run_spark.sh`
```bash
# TÃ¬m dÃ²ng:
--driver-memory 4g \
--executor-memory 4g \

# Sá»­a thÃ nh (náº¿u cÃ³ Ä‘á»§ RAM):
--driver-memory 8g \
--executor-memory 8g \
```

#### Lá»—i 3: File temp khÃ´ng tá»± Ä‘á»™ng xÃ³a
**Triá»‡u chá»©ng**: Váº«n tháº¥y file trong `data/processed/`
**NguyÃªn nhÃ¢n**: Script bá»‹ lá»—i giá»¯a chá»«ng
**Giáº£i phÃ¡p**: XÃ³a thá»§ cÃ´ng
```bash
rm -rf data/processed/*

# Hoáº·c cháº¡y script cleanup
./scripts/pipeline/clean_spark.sh
```

#### Lá»—i 4: Polars bÃ¡o lá»—i memory
**Triá»‡u chá»©ng**:
```
MemoryError: Unable to allocate array
```

**NguyÃªn nhÃ¢n**: RAM khÃ´ng Ä‘á»§ khi load CSV
**Giáº£i phÃ¡p**: DÃ¹ng streaming mode
```python
# Thay vÃ¬:
df = pl.read_csv('file.csv')

# DÃ¹ng:
df = pl.scan_csv('file.csv')  # Lazy, khÃ´ng load háº¿t vÃ o RAM
df.sink_csv('output.csv')     # Stream ra file
```

### 8.2. Kiá»ƒm tra há»‡ thá»‘ng

#### Checklist trÆ°á»›c khi cháº¡y
```bash
# 1. Java
java -version  # Pháº£i cÃ³ version 11 hoáº·c 17

# 2. HDFS
hdfs dfsadmin -report  # Pháº£i tháº¥y "Live datanodes"

# 3. Spark
spark-submit --version  # Pháº£i cÃ³ version 4.x

# 4. Python packages
python -c "import polars, numpy, pyspark"  # KhÃ´ng lá»—i

# 5. File CSV
ls -lh data/raw/HI-Large_Trans.csv  # Pháº£i ~16GB

# 6. Disk space
df -h  # Pháº£i cÃ²n > 50GB trá»‘ng
```

---

<a id="p9"></a>
## PHáº¦N 9: Káº¾T LUáº¬N VÃ€ HÆ¯á»šNG PHÃT TRIá»‚N

### 9.1. Tá»•ng káº¿t dá»± Ã¡n

#### Nhá»¯ng gÃ¬ Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c
âœ… **Vá» ká»¹ thuáº­t**:
- Xá»­ lÃ½ thÃ nh cÃ´ng 179 triá»‡u giao dá»‹ch (16GB CSV)
- Ãp dá»¥ng K-means trÃªn Apache Spark (distributed)
- Thá»i gian xá»­ lÃ½: 33 phÃºt (nhanh hÆ¡n Hadoop 4-8 láº§n)
- XÃ¢y dá»±ng pipeline tá»± Ä‘á»™ng 8 bÆ°á»›c
- TuÃ¢n thá»§ quy Ä‘á»‹nh báº£o máº­t dá»¯ liá»‡u

âœ… **Vá» há»c mÃ¡y**:
- PhÃ¢n cá»¥m thÃ nh cÃ´ng thÃ nh 5 nhÃ³m
- Thuáº­t toÃ¡n há»™i tá»¥ tá»‘t (shift < 0.01)
- PhÃ¡t hiá»‡n 225,546 giao dá»‹ch nghi ngá»
- XÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c cá»¥m rá»§i ro cao nháº¥t (Cluster 1)

âœ… **Vá» phÃ¡t triá»ƒn pháº§n má»m**:
- Code cÃ³ cáº¥u trÃºc rÃµ rÃ ng (modular)
- TÃ i liá»‡u Ä‘áº§y Ä‘á»§, dá»… hiá»ƒu
- Dá»… báº£o trÃ¬ vÃ  má»Ÿ rá»™ng
- CÃ³ há»‡ thá»‘ng log chi tiáº¿t

#### Háº¡n cháº¿
âš ï¸ **Vá» thuáº­t toÃ¡n**:
- K-means nháº¡y cáº£m vá»›i K ban Ä‘áº§u
- ChÆ°a tá»± Ä‘á»™ng chá»n K tá»‘i Æ°u (hiá»‡n táº¡i fix K=5)
- ChÆ°a xá»­ lÃ½ outliers (Ä‘iá»ƒm ngoáº¡i lai)

âš ï¸ **Vá» infrastructure**:
- Cháº¡y trÃªn single machine (pseudo-distributed)
- ChÆ°a test trÃªn cluster tháº­t
- ChÆ°a cÃ³ monitoring real-time

### 9.2. HÆ°á»›ng phÃ¡t triá»ƒn tÆ°Æ¡ng lai

#### 1. Cáº£i thiá»‡n thuáº­t toÃ¡n
**Tá»± Ä‘á»™ng chá»n K tá»‘i Æ°u**:
- DÃ¹ng Elbow Method
- DÃ¹ng Silhouette Score
- Cháº¡y K-means vá»›i nhiá»u K (3, 5, 7, 10) vÃ  so sÃ¡nh

**Khá»Ÿi táº¡o tá»‘t hÆ¡n**:
- DÃ¹ng K-means++ thay vÃ¬ random
- Giáº£m sá»‘ vÃ²ng láº·p cáº§n thiáº¿t
- TÄƒng Ä‘á»™ á»•n Ä‘á»‹nh

**Xá»­ lÃ½ outliers**:
- PhÃ¡t hiá»‡n vÃ  loáº¡i bá» outliers trÆ°á»›c khi cluster
- DÃ¹ng DBSCAN hoáº·c Isolation Forest

#### 2. Machine Learning nÃ¢ng cao
**Supervised Learning**:
- DÃ¹ng nhÃ£n "Is Laundering" Ä‘á»ƒ train model
- Thá»­ Random Forest, XGBoost
- So sÃ¡nh accuracy, precision, recall

**Deep Learning**:
- Neural Network cho phÃ¡t hiá»‡n anomaly
- Autoencoder Ä‘á»ƒ há»c representation
- LSTM cho time series patterns

**Ensemble Methods**:
- Káº¿t há»£p nhiá»u models
- Voting mechanism
- TÄƒng Ä‘á»™ chÃ­nh xÃ¡c

#### 3. Real-time Processing
**Spark Streaming**:
- Xá»­ lÃ½ giao dá»‹ch real-time khi chÃºng xáº£y ra
- Cáº£nh bÃ¡o tá»©c thÃ¬ khi phÃ¡t hiá»‡n nghi ngá»
- DÃ¹ng Kafka lÃ m message queue

**Dashboard**:
- Visualize clusters báº±ng Plotly
- Real-time monitoring
- Alert system

#### 4. Deployment
**Containerization**:
```dockerfile
# Dockerfile
FROM apache/spark:4.0.1
COPY scripts/ /app/scripts/
COPY data/ /app/data/
CMD ["./scripts/pipeline/full_pipeline_spark.sh"]
```

**Kubernetes**:
- Orchestrate Spark cluster
- Auto-scaling based on load
- High availability

**CI/CD**:
- GitHub Actions cho testing
- Automated deployment
- Version control

#### 5. Báº£o máº­t nÃ¢ng cao
- Encryption at rest (HDFS)
- Encryption in transit (SSL/TLS)
- Role-based access control
- Audit logging

### 9.3. BÃ i há»c kinh nghiá»‡m

#### Vá» ká»¹ thuáº­t
1. **Chá»n cÃ´ng nghá»‡ phÃ¹ há»£p**:
   - Polars cho single-machine processing
   - Spark cho distributed processing
   - HDFS cho storage
   - Má»—i tool cÃ³ strengths riÃªng

2. **Pipeline automation**:
   - Viáº¿t scripts Ä‘á»ƒ tá»± Ä‘á»™ng hÃ³a
   - Sá»­ dá»¥ng checkpoints
   - Logging Ä‘áº§y Ä‘á»§

3. **TuÃ¢n thá»§ quy Ä‘á»‹nh tá»« Ä‘áº§u**:
   - Thiáº¿t káº¿ kiáº¿n trÃºc vá»›i security in mind
   - Tá»± Ä‘á»™ng xÃ³a temp files
   - KhÃ´ng lÆ°u dá»¯ liá»‡u nháº¡y cáº£m local

#### Vá» há»c mÃ¡y
1. **Feature engineering quan trá»ng**:
   - Parse timestamp â†’ temporal features
   - TÃ­nh ratio Ä‘á»ƒ phÃ¡t hiá»‡n báº¥t thÆ°á»ng
   - Normalize Ä‘á»ƒ thuáº­t toÃ¡n hoáº¡t Ä‘á»™ng tá»‘t

2. **K-means cáº§n fine-tuning**:
   - Chá»n K phÃ¹ há»£p
   - Khá»Ÿi táº¡o centroids tá»‘t
   - Kiá»ƒm tra convergence

3. **Validation ráº¥t quan trá»ng**:
   - PhÃ¢n tÃ­ch káº¿t quáº£ sau má»—i run
   - So sÃ¡nh vá»›i ground truth
   - Iterate Ä‘á»ƒ cáº£i thiá»‡n

---

<a id="phu-luc"></a>
## PHá»¤ Lá»¤C

### A. Thuáº­t ngá»¯ vÃ  Giáº£i thÃ­ch

**Big Data**: Dá»¯ liá»‡u cÃ³ quy mÃ´ lá»›n (>1TB), cáº§n cÃ´ng nghá»‡ Ä‘áº·c biá»‡t Ä‘á»ƒ xá»­ lÃ½

**Cluster**: NhÃ³m mÃ¡y tÃ­nh lÃ m viá»‡c cÃ¹ng nhau nhÆ° má»™t há»‡ thá»‘ng

**Distributed Computing**: Xá»­ lÃ½ phÃ¢n tÃ¡n trÃªn nhiá»u mÃ¡y song song

**HDFS**: Hadoop Distributed File System - Há»‡ thá»‘ng file phÃ¢n tÃ¡n

**In-memory Computing**: Xá»­ lÃ½ trong RAM thay vÃ¬ Ä‘á»c/ghi disk liÃªn tá»¥c

**K-means**: Thuáº­t toÃ¡n phÃ¢n cá»¥m khÃ´ng giÃ¡m sÃ¡t

**Polars**: ThÆ° viá»‡n DataFrame nhanh cho Python

**Spark**: Framework xá»­ lÃ½ big data in-memory

**Unsupervised Learning**: Há»c mÃ¡y khÃ´ng cáº§n nhÃ£n (tá»± phÃ¢n nhÃ³m)

**Centroid**: TÃ¢m cá»¥m - Äiá»ƒm trung tÃ¢m cá»§a má»™t nhÃ³m dá»¯ liá»‡u

**Convergence**: Há»™i tá»¥ - Thuáº­t toÃ¡n Ä‘áº¡t tráº¡ng thÃ¡i á»•n Ä‘á»‹nh

**Feature Engineering**: TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« dá»¯ liá»‡u thÃ´

**Normalize**: Chuáº©n hÃ³a - ÄÆ°a dá»¯ liá»‡u vá» cÃ¹ng scale

**Pipeline**: Quy trÃ¬nh tá»± Ä‘á»™ng tá»« input Ä‘áº¿n output

**Replication**: Sao lÆ°u dá»¯ liá»‡u trÃªn nhiá»u mÃ¡y

### B. Cáº¥u trÃºc thÆ° má»¥c Ä‘áº§y Ä‘á»§

```
Final/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ HI-Large_Trans.csv
â”‚   â”œâ”€â”€ processed/              (rá»—ng - files tá»± Ä‘á»™ng xÃ³a)
â”‚   â””â”€â”€ results/
â”‚       â”œâ”€â”€ final_centroids.txt
â”‚       â””â”€â”€ clustered_results.txt
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ polars/
â”‚   â”‚   â”œâ”€â”€ explore_fast.py
â”‚   â”‚   â”œâ”€â”€ prepare_polars.py
â”‚   â”‚   â”œâ”€â”€ init_centroids.py
â”‚   â”‚   â”œâ”€â”€ assign_clusters_polars.py
â”‚   â”‚   â””â”€â”€ analyze_polars.py
â”‚   â”‚
â”‚   â”œâ”€â”€ spark/
â”‚   â”‚   â”œâ”€â”€ setup_hdfs.sh
â”‚   â”‚   â”œâ”€â”€ run_spark.sh
â”‚   â”‚   â”œâ”€â”€ kmeans_spark.py
â”‚   â”‚   â””â”€â”€ download_from_hdfs.sh
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”œâ”€â”€ full_pipeline_spark.sh
â”‚   â”‚   â”œâ”€â”€ clean_spark.sh
â”‚   â”‚   â””â”€â”€ reset_pipeline.sh
â”‚   â”‚
â”‚   â””â”€â”€ setup/
â”‚       â””â”€â”€ install_spark.sh
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ PROJECT_OVERVIEW.md
â”‚   â””â”€â”€ HADOOP_ALTERNATIVES.md
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ pipeline_log_20251028_202850.md
â”‚
â”œâ”€â”€ archive/
â”‚   â””â”€â”€ hadoop/                (legacy code)
â”‚
â”œâ”€â”€ .venv/                     (Python virtual env)
â”œâ”€â”€ .git/                      (Version control)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ CHANGELOG.md
â””â”€â”€ PROJECT_REPORT.md          (BÃ¡o cÃ¡o nÃ y)
```

### C. Thá»‘ng kÃª dá»± Ã¡n

- **Tá»•ng sá»‘ file Python**: 6 files, 442 dÃ²ng code
- **Tá»•ng sá»‘ file Shell**: 7 files, 661 dÃ²ng code
- **Tá»•ng dÃ²ng code**: 1,103 dÃ²ng
- **Thá»i gian phÃ¡t triá»ƒn**: 3 tuáº§n
- **CÃ´ng nghá»‡ sá»­ dá»¥ng**: 5 (Polars, Spark, HDFS, Python, NumPy)
- **Sá»‘ bÆ°á»›c pipeline**: 8
- **Thá»i gian cháº¡y**: 33 phÃºt

---

### D. TÃ i liá»‡u tham kháº£o

1. Apache Spark Documentation: https://spark.apache.org/docs/latest/
2. Polars Guide: https://pola-rs.github.io/polars-book/
3. Hadoop HDFS: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/
4. K-means Algorithm: https://scikit-learn.org/stable/modules/clustering.html#k-means
5. Money Laundering Detection: Research papers on financial crime

---


**Háº¾T BÃO CÃO**


_BÃ¡o cÃ¡o Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng bá»Ÿi `generate_vietnamese_report.py`_  
_NgÃ y: 28/10/2025 22:04:46_
