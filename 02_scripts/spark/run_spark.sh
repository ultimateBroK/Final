#!/bin/bash
# run_spark.sh - Ch·∫°y thu·∫≠t to√°n K-means v·ªõi PySpark tr√™n HDFS

echo "=== PYSPARK K-MEANS V·ªöI HDFS üöÄ ==="

# X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c
SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
ROOT_DIR="$(cd "$SCRIPT_DIR/../.." && pwd)"

# Thi·∫øt l·∫≠p SPARK_HOME cho Spark ƒë∆∞·ª£c c√†i ƒë·∫∑t qua pip
export SPARK_HOME="$ROOT_DIR/.venv/lib/python3.12/site-packages/pyspark"
export PATH="$SPARK_HOME/bin:$PATH"

# C·∫•u h√¨nh HDFS
HDFS_URI="hdfs://localhost:9000"
HDFS_BASE="/user/spark/hi_large"
HDFS_INPUT="$HDFS_URI$HDFS_BASE/input/hadoop_input.txt"
HDFS_OUTPUT="$HDFS_URI$HDFS_BASE/output_centroids"

# Ki·ªÉm tra HDFS c√≥ ƒëang ch·∫°y kh√¥ng
if ! hdfs dfs -test -e / 2>/dev/null; then
    echo "HDFS kh√¥ng th·ªÉ truy c·∫≠p. Vui l√≤ng kh·ªüi ƒë·ªông HDFS tr∆∞·ªõc."
    echo "   Ch·∫°y: start-dfs.sh"
    exit 1
fi

# Ki·ªÉm tra file ƒë·∫ßu v√†o c√≥ t·ªìn t·∫°i trong HDFS kh√¥ng
if ! hdfs dfs -test -e "$HDFS_BASE/input/hadoop_input.txt" 2>/dev/null; then
    echo "Kh√¥ng t√¨m th·∫•y t·ªáp ƒë·∫ßu v√†o trong HDFS: $HDFS_BASE/input/hadoop_input.txt"
    echo "   Vui l√≤ng ch·∫°y: ./02_scripts/spark/setup_hdfs.sh"
    exit 1
fi

# L·∫•y s·ªë CPU cores ƒë·ªÉ t·ªëi ∆∞u h√≥a
CPU_CORES=$(nproc)
EXECUTOR_CORES=4
EXECUTOR_INSTANCES=4
DRIVER_MEMORY="8g"
EXECUTOR_MEMORY="8g"

echo "C·∫•u h√¨nh:"
echo "  CPU cores: $CPU_CORES"
echo "  S·ªë executor: $EXECUTOR_INSTANCES"
echo "  Executor cores: $EXECUTOR_CORES"
echo "  Executor memory: $EXECUTOR_MEMORY"
echo "  Driver memory: $DRIVER_MEMORY"
echo ""
echo "S·ª≠ d·ª•ng MLlib - nhanh h∆°n 30-50%"
echo "   - Catalyst optimizer"
echo "   - Tungsten execution engine"
echo "   - Kh·ªüi t·∫°o k-means++"
echo "   - Adaptive query execution"
echo ""
echo "ƒê∆∞·ªùng d·∫´n HDFS:"
echo "  ƒê·∫ßu v√†o: $HDFS_INPUT"
echo "  ƒê·∫ßu ra: $HDFS_OUTPUT"
echo ""

# D·ªçn d·∫πp k·∫øt qu·∫£ c≈© trong HDFS
echo "ƒêang d·ªçn d·∫πp k·∫øt qu·∫£ c≈©..."
hdfs dfs -rm -r -f "$HDFS_BASE/output_centroids" 2>/dev/null

# Ch·∫°y K-means v·ªõi MLlib
# M·∫∑c ƒë·ªãnh
MAX_ITER=15
K=5
SEED=42
TOL=0.0001

# Parse named arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --k)
            K="$2"
            shift 2
            ;;
        --max-iter)
            MAX_ITER="$2"
            shift 2
            ;;
        --seed)
            SEED="$2"
            shift 2
            ;;
        --tol)
            TOL="$2"
            shift 2
            ;;
        *)
            # Fallback: positional args for backward compatibility
            # $1=K, $2=MAX_ITER, $3=SEED, $4=TOL
            if [ -n "${1:-}" ] && [[ "$1" =~ ^[0-9]+$ ]]; then
                K="$1"
                [ -n "${2:-}" ] && MAX_ITER="$2"
                [ -n "${3:-}" ] && SEED="$3"
                [ -n "${4:-}" ] && TOL="$4"
            fi
            break
            ;;
    esac
done

echo "ƒêang ch·∫°y K-means v·ªõi Spark MLlib..."
echo "S·ªë c·ª•m: $K"
echo "S·ªë l·∫ßn l·∫∑p t·ªëi ƒëa: $MAX_ITER"
echo "Seed: $SEED"
echo "Tol: $TOL"
echo ""

# Ch·∫°y v·ªõi YARN (ho·∫∑c standalone/local-cluster ƒë·ªÉ test)
# Thay ƒë·ªïi --master th√†nh:
#   - yarn: cho YARN cluster
#   - spark://master:7077: cho Spark standalone
#   - local[*]: cho ch·∫ø ƒë·ªô local v·ªõi t·∫•t c·∫£ cores
#   - local-cluster[N,C,M]: N workers, C cores m·ªói worker, M MB memory (Spark 4.x y√™u c·∫ßu MB l√† s·ªë nguy√™n)

# Chuy·ªÉn ƒë·ªïi memory sang MB cho local-cluster (v√≠ d·ª•: 4g -> 4096)
EXECUTOR_MEMORY_MB=$(echo "$EXECUTOR_MEMORY" | sed 's/g$//' | awk '{print $1*1024}')

spark-submit \
    --master local-cluster[${EXECUTOR_INSTANCES},${EXECUTOR_CORES},${EXECUTOR_MEMORY_MB}] \
    --deploy-mode client \
    --driver-memory "$DRIVER_MEMORY" \
    --executor-memory "$EXECUTOR_MEMORY" \
    --executor-cores $EXECUTOR_CORES \
    --num-executors $EXECUTOR_INSTANCES \
    --conf spark.sql.shuffle.partitions=800 \
    --conf spark.default.parallelism=800 \
    --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
    --conf spark.kryoserializer.buffer.max=512m \
    --conf spark.driver.maxResultSize=2g \
    --conf spark.memory.offHeap.enabled=true \
    --conf spark.memory.offHeap.size=4g \
    --conf spark.sql.adaptive.enabled=true \
    --conf spark.sql.adaptive.coalescePartitions.enabled=true \
    --conf spark.hadoop.fs.defaultFS="$HDFS_URI" \
    "$SCRIPT_DIR/kmeans_spark.py" \
    "$HDFS_INPUT" \
    "$HDFS_OUTPUT" \
    "$K" \
    "$MAX_ITER" \
    "$SEED" \
    "$TOL"

if [ $? -ne 0 ]; then
    echo "PySpark job th·∫•t b·∫°i"
    exit 1
fi

echo ""
echo "K-means MLlib ho√†n th√†nh!"
echo "T√¢m c·ª•m ƒë√£ l∆∞u v√†o: $HDFS_OUTPUT"
echo ""
echo "ƒê·ªÉ xem k·∫øt qu·∫£:"
echo "  hdfs dfs -cat $HDFS_BASE/output_centroids/part-*"
