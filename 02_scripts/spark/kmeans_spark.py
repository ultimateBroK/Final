#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BÆ¯á»šC 4: THUáº¬T TOÃN K-MEANS Vá»šI SPARK MLLIB (Tá»I Æ¯U HÆ N)

Pipeline 7 bÆ°á»›c hiá»‡n táº¡i:
  1. KhÃ¡m phÃ¡ dá»¯ liá»‡u
  2. Chuáº©n bá»‹ Ä‘áº·c trÆ°ng
  3. Upload lÃªn HDFS
  4. K-means MLlib <- BÆ¯á»šC NÃ€Y
  5. Táº£i káº¿t quáº£
  6. GÃ¡n nhÃ£n cá»¥m
  7. PhÃ¢n tÃ­ch

Má»¥c Ä‘Ã­ch:
- Cháº¡y thuáº­t toÃ¡n K-means clustering trÃªn dá»¯ liá»‡u lá»›n (179M dÃ²ng)
- Sá»­ dá»¥ng Spark MLlib - tá»‘i Æ°u hÃ³a bá»Ÿi Catalyst optimizer + Tungsten
- K-means++ initialization (k-means||) - há»™i tá»¥ nhanh hÆ¡n
- LÆ°u trá»¯ dá»¯ liá»‡u trÃªn HDFS Ä‘á»ƒ tuÃ¢n thá»§ quy Ä‘á»‹nh báº£o máº­t

Thá»i gian cháº¡y: ~10-15 phÃºt (nhanh hÆ¡n 30-50% so vá»›i custom RDD)
Input:
  - HDFS: /user/spark/hi_large/input/hadoop_input.txt (33GB)
Output: HDFS: /user/spark/hi_large/output_centroids (5 tÃ¢m cá»¥m cuá»‘i cÃ¹ng)

Ká»¹ thuáº­t: Spark MLlib KMeans, k-means++ init, adaptive execution
"""

import sys
import time
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.ml.clustering import KMeans
from pyspark.ml.feature import VectorAssembler
from pyspark import StorageLevel

def run_kmeans(input_path, output_path, k=5, max_iterations=15, seed=42, tol=1e-4, use_hdfs=True):
    """
    Cháº¡y thuáº­t toÃ¡n K-means clustering vá»›i Spark MLlib
    
    Args:
        input_path: ÄÆ°á»ng dáº«n HDFS Ä‘áº¿n dá»¯ liá»‡u Ä‘áº§u vÃ o (CSV khÃ´ng cÃ³ header)
        output_path: ÄÆ°á»ng dáº«n HDFS Ä‘á»ƒ lÆ°u tÃ¢m cá»¥m cuá»‘i cÃ¹ng
        k: Sá»‘ cá»¥m (default=5)
        max_iterations: Sá»‘ láº§n láº·p tá»‘i Ä‘a (default=15)
        use_hdfs: CÃ³ sá»­ dá»¥ng HDFS hay khÃ´ng (máº·c Ä‘á»‹nh True)
    """
    
    # Khá»Ÿi táº¡o Spark vá»›i MLlib optimization
    spark = SparkSession.builder \
        .appName("K-means MLlib - Optimized") \
        .config("spark.driver.memory", "8g") \
        .config("spark.executor.memory", "8g") \
        .config("spark.executor.instances", "4") \
        .config("spark.executor.cores", "4") \
        .config("spark.sql.shuffle.partitions", "800") \
        .config("spark.default.parallelism", "800") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .config("spark.kryoserializer.buffer.max", "512m") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.hadoop.fs.defaultFS", "hdfs://localhost:9000") \
        .getOrCreate()
    
    sc = spark.sparkContext
    sc.setLogLevel("WARN")
    
    # Helper function for timestamps
    def log_with_time(msg, step_start=None):
        current_time = datetime.now().strftime("%H:%M:%S")
        if step_start:
            elapsed = time.time() - step_start
            print(f"[{current_time}] {msg} (â±ï¸  {elapsed:.1f}s)")
        else:
            print(f"[{current_time}] {msg}")
    
    pipeline_start = time.time()
    
    print("=" * 70)
    print("ğŸš€ SPARK MLLIB K-MEANS - TÄ‚NG Tá»C EDITION")
    print("=" * 70)
    log_with_time(f"ğŸ“… Äáº§u vÃ o: {input_path}")
    log_with_time(f"ğŸ“Š Sá»‘ cá»¥m: {k}")
    log_with_time(f"ğŸ”„ Sá»‘ láº§n láº·p tá»‘i Ä‘a: {max_iterations}")
    log_with_time("âœ… K-means++ initialization")
    log_with_time("âœ… Catalyst optimizer + Tungsten")
    print()
    
    # Äá»c dá»¯ liá»‡u tá»« HDFS
    step_start = time.time()
    log_with_time("ğŸ“‚ BÆ¯á»šC 1/5: Äang Ä‘á»c dá»¯ liá»‡u tá»« HDFS...")
    df = spark.read.csv(
        input_path,
        header=False,
        inferSchema=True
    )
    
    # Äá»•i tÃªn cá»™t
    num_cols = len(df.columns)
    df = df.toDF(*[f'f{i}' for i in range(num_cols)])
    
    data_count = df.count()
    log_with_time(f"âœ… ÄÃ£ load {data_count:,} Ä‘iá»ƒm dá»¯ liá»‡u", step_start)
    print()
    
    # Chuyá»ƒn Ä‘á»•i sang vector features
    step_start = time.time()
    log_with_time("ğŸ”§ BÆ¯á»šC 2/5: Chuyá»ƒn Ä‘á»•i sang feature vectors...")
    assembler = VectorAssembler(
        inputCols=df.columns,
        outputCol="features"
    )
    vector_df = assembler.transform(df).select("features")
    
    # Cache Ä‘á»ƒ tÄƒng tá»‘c
    log_with_time("   ğŸ’¾ Caching data vÃ o memory/disk...")
    vector_df = vector_df.persist(StorageLevel.MEMORY_AND_DISK)
    
    vec_count = vector_df.count()
    log_with_time(f"âœ… ÄÃ£ táº¡o {vec_count:,} feature vectors", step_start)
    print()
    
    # Khá»Ÿi táº¡o K-means vá»›i k-means++ (k-means||)
    step_start = time.time()
    log_with_time("ğŸ¯ BÆ¯á»šC 3/5: Khá»Ÿi táº¡o K-means vá»›i k-means++ initialization...")
    kmeans = KMeans() \
        .setK(k) \
        .setMaxIter(max_iterations) \
        .setInitMode("k-means||") \
        .setFeaturesCol("features") \
        .setPredictionCol("cluster") \
        .setSeed(seed) \
        .setTol(tol)
    
    log_with_time(f"   âœ… Model configured: K={k}, MaxIter={max_iterations}, Seed={seed}, Tol={tol}", step_start)
    print()
    
    # Train model
    train_start = time.time()
    log_with_time(f"ğŸš€ BÆ¯á»šC 4/5: Äang train K-means (tá»‘i Ä‘a {max_iterations} iterations)...")
    log_with_time("   ğŸ’» Sá»­ dá»¥ng Catalyst optimizer + Tungsten execution engine")
    log_with_time("   âŒ› Vui lÃ²ng Ä‘á»£i 10-15 phÃºt (phá»¥ thuá»™c pháº§n cá»©ng)...")
    print()
    
    model = kmeans.fit(vector_df)
    
    log_with_time("âœ… K-means training hoÃ n thÃ nh!", train_start)
    
    # Láº¥y centroids
    centroids = model.clusterCenters()
    
    print()
    print("=" * 70)
    log_with_time("âœ… HOÃ€N THÃ€NH K-MEANS CLUSTERING!")
    print("=" * 70)
    print()
    
    # LÆ°u centroids vÃ o HDFS
    step_start = time.time()
    log_with_time(f"ğŸ’¾ BÆ¯á»šC 5/5: LÆ°u {len(centroids)} tÃ¢m cá»¥m vÃ o HDFS...")
    log_with_time(f"   ğŸ“ Path: {output_path}")
    
    # Chuyá»ƒn centroids thÃ nh text format
    centroids_lines = [','.join(f'{x:.6f}' for x in row) for row in centroids]
    centroids_rdd = sc.parallelize(centroids_lines, 1)
    centroids_rdd.saveAsTextFile(output_path)
    
    log_with_time("âœ… ÄÃ£ lÆ°u tÃ¢m cá»¥m", step_start)
    print()
    
    # TÃ­nh toÃ¡n phÃ¢n cá»¥m cuá»‘i cÃ¹ng
    step_start = time.time()
    log_with_time("ğŸ“Š PHÃ‚N TÃCH Káº¾T QUáº¢...")
    predictions = model.transform(vector_df)
    
    # Äáº¿m sá»‘ Ä‘iá»ƒm má»—i cá»¥m
    cluster_counts = predictions.groupBy("cluster").count().collect()
    cluster_counts = sorted(cluster_counts, key=lambda r: r.cluster)
    
    log_with_time("âœ… ÄÃ£ phÃ¢n tÃ­ch xong", step_start)
    print()
    print("ğŸ“Š PHÃ‚N PHá»I Cá»¤M:")
    for row in cluster_counts:
        count = row['count']
        percentage = (count / data_count) * 100
        print(f"   Cá»¥m {row.cluster}: {count:,} Ä‘iá»ƒm ({percentage:.2f}%)")
    
    # TÃ­nh Within Set Sum of Squared Errors (WSSSE)
    wssse = model.summary.trainingCost
    print()
    print(f"ğŸ“ˆ WSSSE (Within-cluster SSE): {wssse:.2f}")
    print("   (GiÃ¡ trá»‹ tháº¥p hÆ¡n = cá»¥m tá»‘t hÆ¡n)")
    
    # Tá»•ng káº¿t thá»i gian
    total_time = time.time() - pipeline_start
    print()
    print("=" * 70)
    log_with_time("âœ… HOÃ€N THÃ€NH TOÃ€N Bá»˜ PIPELINE!")
    log_with_time(f"â±ï¸  Tá»•ng thá»i gian: {total_time/60:.1f} phÃºt ({total_time:.1f}s)")
    print("=" * 70)
    print()
    
    # Unpersist
    vector_df.unpersist()
    
    spark.stop()

if __name__ == "__main__":
    if len(sys.argv) < 3:
        print("CÃ¡ch sá»­ dá»¥ng: spark-submit kmeans_spark.py <Ä‘Æ°á»ng_dáº«n_hdfs_input> <Ä‘Æ°á»ng_dáº«n_hdfs_output> [k] [max_iter] [seed] [tol]")
        print("VÃ­ dá»¥: spark-submit kmeans_spark.py hdfs://localhost:9000/user/spark/input.txt hdfs://localhost:9000/user/spark/output 5 15 42 1e-4")
        sys.exit(1)

    input_path = sys.argv[1]
    output_path = sys.argv[2]

    # Defaults
    k = 5
    max_iterations = 15
    seed = 42
    tol = 0.0001

    # Helper parsers that won't crash if order is flexible
    def try_parse_int(value):
        try:
            return int(value)
        except Exception:
            return None

    def try_parse_float(value):
        try:
            return float(value)
        except Exception:
            return None

    # Parse optional args with resilience to misordered inputs
    if len(sys.argv) > 3:
        k_candidate = try_parse_int(sys.argv[3])
        if k_candidate is not None and k_candidate > 0:
            k = k_candidate

    if len(sys.argv) > 4:
        # Prefer interpreting as max_iter if it is an int, otherwise maybe tol
        mi_candidate = try_parse_int(sys.argv[4])
        if mi_candidate is not None and mi_candidate > 0:
            max_iterations = mi_candidate
        else:
            tol_candidate = try_parse_float(sys.argv[4])
            if tol_candidate is not None and tol_candidate > 0:
                tol = tol_candidate

    if len(sys.argv) > 5:
        # Prefer seed (int); if not int, may be tol
        seed_candidate = try_parse_int(sys.argv[5])
        if seed_candidate is not None:
            seed = seed_candidate
        else:
            tol_candidate = try_parse_float(sys.argv[5])
            if tol_candidate is not None and tol_candidate > 0:
                tol = tol_candidate

    if len(sys.argv) > 6:
        tol_candidate = try_parse_float(sys.argv[6])
        if tol_candidate is not None and tol_candidate > 0:
            tol = tol_candidate

    print(f"HDFS Äáº§u vÃ o: {input_path}")
    print(f"HDFS Äáº§u ra: {output_path}")
    print(f"Sá»‘ cá»¥m K: {k}")
    print(f"Max iterations: {max_iterations}")
    print(f"Seed: {seed}")
    print(f"Tol: {tol}")
    print()

    run_kmeans(input_path, output_path, k, max_iterations, seed, tol, use_hdfs=True)
