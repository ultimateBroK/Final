#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BÆ¯á»šC 4: THUáº¬T TOÃN K-MEANS Vá»šI SPARK MLlib (Tá»I Æ¯U)

Pipeline 7 bÆ°á»›c:
  1) KhÃ¡m phÃ¡ dá»¯ liá»‡u
  2) Chuáº©n bá»‹ Ä‘áº·c trÆ°ng
  3) Táº£i lÃªn HDFS
  4) K-means MLlib <- BÆ°á»›c nÃ y
  5) Táº£i káº¿t quáº£
  6) GÃ¡n nhÃ£n cá»¥m
  7) PhÃ¢n tÃ­ch

Má»¥c Ä‘Ã­ch:
- Cháº¡y K-means trÃªn dá»¯ liá»‡u lá»›n (179M dÃ²ng)
- DÃ¹ng Spark MLlib (Catalyst + Tungsten), khá»Ÿi táº¡o k-means++ (k-means||)
- LÆ°u trÃªn HDFS Ä‘á»ƒ tuÃ¢n thá»§ báº£o máº­t
"""

import sys
import time
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.ml.clustering import KMeans
from pyspark.ml.feature import VectorAssembler
from pyspark import StorageLevel
import numpy as np

def run_kmeans(input_path: str, output_path: str, k: int = 5, max_iter: int = 15, seed: int = 42, tol: float = 1e-4) -> None:
    """Cháº¡y K-means vá»›i Spark MLlib trÃªn dá»¯ liá»‡u CSV khÃ´ng cÃ³ header (HDFS)."""
    
    # Khá»Ÿi táº¡o Spark vá»›i MLlib optimization
    spark = SparkSession.builder \
        .appName("K-means MLlib - Optimized") \
        .config("spark.driver.memory", "8g") \
        .config("spark.executor.memory", "8g") \
        .config("spark.executor.instances", "4") \
        .config("spark.executor.cores", "4") \
        .config("spark.sql.shuffle.partitions", "800") \
        .config("spark.default.parallelism", "800") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .config("spark.kryoserializer.buffer.max", "512m") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.hadoop.fs.defaultFS", "hdfs://localhost:9000") \
        .getOrCreate()
    
    sc = spark.sparkContext
    # Thiáº¿t láº­p log level WARN Ä‘á»ƒ táº¯t TaskSetManager, executor details
    sc.setLogLevel("WARN")
    
    # Gá»£i Ã½ giáº£m á»“n log má»©c INFO (tÃ¹y phiÃªn báº£n Spark)
    log4j = sc._jvm.org.apache.log4j
    log4j.LogManager.getLogger("org.apache.spark").setLevel(log4j.Level.WARN)
    
    # Helper function for timestamps
    def log_with_time(msg, step_start=None):
        current_time = datetime.now().strftime("%H:%M:%S")
        if step_start:
            elapsed = time.time() - step_start
            print(f"[{current_time}] {msg} (máº¥t {elapsed:.1f}s)")
        else:
            print(f"[{current_time}] {msg}")
    
    pipeline_start = time.time()
    
    print("=" * 70)
    print("SPARK MLlib K-means - Cháº¿ Ä‘á»™ tá»‘i Æ°u")
    print("=" * 70)
    log_with_time(f"Äáº§u vÃ o: {input_path}")
    log_with_time(f"Sá»‘ cá»¥m: {k}")
    log_with_time(f"Sá»‘ láº§n láº·p tá»‘i Ä‘a: {max_iter}")
    log_with_time("Khá»Ÿi táº¡o k-means++")
    log_with_time("Tá»‘i Æ°u hÃ³a Catalyst + Tungsten")
    print()
    
    # Äá»c dá»¯ liá»‡u tá»« HDFS
    step_start = time.time()
    log_with_time("BÆ°á»›c 1/5: Äá»c dá»¯ liá»‡u tá»« HDFS...")
    df = spark.read.csv(
        input_path,
        header=False,
        inferSchema=True
    )
    
    # Äá»•i tÃªn cá»™t
    num_cols = len(df.columns)
    df = df.toDF(*[f'f{i}' for i in range(num_cols)])
    
    data_count = df.count()
    log_with_time(f"ÄÃ£ táº£i {data_count:,} báº£n ghi", step_start)
    print()
    
    # Chuyá»ƒn Ä‘á»•i sang vector
    step_start = time.time()
    log_with_time("BÆ°á»›c 2/5: Táº¡o vector Ä‘áº·c trÆ°ng...")
    assembler = VectorAssembler(
        inputCols=[f for f in df.columns],
        outputCol="features"
    )
    vector_df = assembler.transform(df).select("features")
    
    # Cache Ä‘á»ƒ tÄƒng tá»‘c
    log_with_time("   LÆ°u Ä‘á»‡m vÃ o bá»™ nhá»›/Ä‘Ä©a...")
    vector_df = vector_df.persist(StorageLevel.MEMORY_AND_DISK)
    
    vec_count = vector_df.count()
    log_with_time(f"ÄÃ£ táº¡o {vec_count:,} vector Ä‘áº·c trÆ°ng", step_start)
    print()
    
    # Khá»Ÿi táº¡o K-means vá»›i k-means++ (k-means||)
    step_start = time.time()
    log_with_time("BÆ°á»›c 3/5: Cáº¥u hÃ¬nh K-means (k-means++)...")
    kmeans = KMeans() \
        .setK(k) \
        .setMaxIter(max_iter) \
        .setInitMode("k-means||") \
        .setFeaturesCol("features") \
        .setPredictionCol("cluster") \
        .setSeed(seed) \
        .setTol(tol)

    log_with_time(f"   ÄÃ£ cáº¥u hÃ¬nh: K={k}, MaxIter={max_iter}, Seed={seed}, Tol={tol}", step_start)
    print()
    
    # Huáº¥n luyá»‡n
    train_start = time.time()
    log_with_time("BÆ°á»›c 4/5: Äang huáº¥n luyá»‡n K-means...")
    log_with_time("   Sá»­ dá»¥ng Catalyst + Tungsten; vui lÃ²ng Ä‘á»£i...")
    print()
    
    # Huáº¥n luyá»‡n mÃ´ hÃ¬nh
    model = kmeans.fit(vector_df)

    log_with_time("HoÃ n thÃ nh huáº¥n luyá»‡n K-means! âœ…", train_start)
    
    # Láº¥y centroids
    centroids = model.clusterCenters()
    
    # Thá»‘ng kÃª vá» quÃ¡ trÃ¬nh training
    summary = getattr(model, 'summary', None)
    num_iterations = getattr(summary, 'numIter', "N/A")
    training_cost = getattr(summary, 'trainingCost', None)
    
    print()
    print("=" * 70)
    log_with_time("HoÃ n thÃ nh phÃ¢n cá»¥m K-means! âœ…")
    print("=" * 70)
    print()
    print("ThÃ´ng tin huáº¥n luyá»‡n:")
    print(f"   - Sá»‘ vÃ²ng láº·p thá»±c táº¿: {num_iterations}")
    print(f"   - Sá»‘ vÃ²ng láº·p tá»‘i Ä‘a: {max_iter}")
    if training_cost is not None:
        print(f"   - WSSSE (Within-Set SSE): {training_cost:.2f}")
    print()
    
    # LÆ°u tÃ¢m cá»¥m vÃ o HDFS
    step_start = time.time()
    log_with_time(f"BÆ°á»›c 5/5: LÆ°u {len(centroids)} tÃ¢m cá»¥m vÃ o HDFS...")
    log_with_time(f"   ÄÆ°á»ng dáº«n: {output_path}")
    
    # Chuyá»ƒn centroids thÃ nh text format
    centroids_lines = [','.join(f'{x:.6f}' for x in row) for row in centroids]
    centroids_rdd = sc.parallelize(centroids_lines, 1)
    # Ghi Ä‘Ã¨ thÆ° má»¥c káº¿t quáº£ náº¿u tá»“n táº¡i
    try:
        sc._jsc.hadoopConfiguration().set("dfs.client.block.write.replace-datanode-on-failure.policy", "ALWAYS")
        sc._jsc.hadoopConfiguration().set("dfs.client.block.write.replace-datanode-on-failure.enable", "true")
        # XÃ³a thÆ° má»¥c cÅ© náº¿u cÃ³
        sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration()).delete(sc._jvm.org.apache.hadoop.fs.Path(output_path), True)
    except Exception:
        pass
    centroids_rdd.saveAsTextFile(output_path)
    
    log_with_time("ÄÃ£ lÆ°u tÃ¢m cá»¥m", step_start)
    print()
    
    # TÃ­nh toÃ¡n phÃ¢n cá»¥m cuá»‘i cÃ¹ng
    step_start = time.time()
    log_with_time("PhÃ¢n tÃ­ch káº¿t quáº£...")
    predictions = model.transform(vector_df)
    
    # Äáº¿m sá»‘ Ä‘iá»ƒm má»—i cá»¥m
    cluster_counts = predictions.groupBy("cluster").count().collect()
    
    log_with_time("ÄÃ£ phÃ¢n tÃ­ch xong", step_start)
    print()
    print("ğŸ“Š PhÃ¢n phá»‘i cá»¥m:")
    total_points = data_count
    for row in sorted(cluster_counts, key=lambda r: r["cluster"]):
        cid = row["cluster"]
        count = row["count"]
        percentage = (count / total_points) * 100 if total_points else 0.0
        bar = 'â–ˆ' * max(1, int(percentage / 2))
        print(f"   - Cá»¥m {cid}: {count:,} Ä‘iá»ƒm ({percentage:.2f}%) {bar}")
    
    if training_cost is not None and total_points:
        print()
        print("Chá»‰ sá»‘ cháº¥t lÆ°á»£ng cá»¥m:")
        print(f"   - WSSSE: {training_cost:.2f}")
        print(f"   - Trung bÃ¬nh SSE/Ä‘iá»ƒm: {training_cost/total_points:.6f}")
    
    # Tá»•ng káº¿t thá»i gian
    total_time = time.time() - pipeline_start
    print()
    print("=" * 70)
    log_with_time("HoÃ n thÃ nh toÃ n bá»™ tiáº¿n trÃ¬nh!")
    log_with_time(f"Tá»•ng thá»i gian: {total_time/60:.1f} phÃºt ({total_time:.1f}s)")
    print("=" * 70)
    print()
    
    # Unpersist
    vector_df.unpersist()
    
    spark.stop()

if __name__ == "__main__":
    if len(sys.argv) < 3:
        print("CÃ¡ch dÃ¹ng: spark-submit kmeans_spark.py <hdfs_input> <hdfs_output> [k] [max_iter] [seed] [tol]")
        print("VÃ­ dá»¥: spark-submit kmeans_spark.py hdfs://localhost:9000/user/spark/hi_large/input/hadoop_input.txt hdfs://localhost:9000/user/spark/hi_large/output_centroids 5 15 42 1e-4")
        sys.exit(1)

    input_path = sys.argv[1]
    output_path = sys.argv[2]

    # Máº·c Ä‘á»‹nh
    k = int(sys.argv[3]) if len(sys.argv) > 3 else 5
    max_iterations = int(sys.argv[4]) if len(sys.argv) > 4 else 15
    seed = int(sys.argv[5]) if len(sys.argv) > 5 else 42
    tol = float(sys.argv[6]) if len(sys.argv) > 6 else 1e-4

    print(f"ğŸ“¥ HDFS Ä‘áº§u vÃ o: {input_path}")
    print(f"ğŸ“¤ HDFS Ä‘áº§u ra: {output_path}")
    print(f"ğŸ”¢ Sá»‘ cá»¥m K: {k}")
    print(f"ğŸ” Sá»‘ láº§n láº·p tá»‘i Ä‘a: {max_iterations}")
    print(f"ğŸŒ± Seed: {seed}")
    print(f"ğŸ¯ Tol: {tol}")
    print()

    run_kmeans(input_path, output_path, k, max_iterations, seed, tol)
