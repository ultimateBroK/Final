# Output mẫu của Bước 4 - K-means MLlib

Khi chạy bước 4 (K-means training), bạn sẽ thấy output chi tiết theo từng bước với timestamp như sau:

```
======================================================================
SPARK MLlib K-means - Chế độ tối ưu
======================================================================
[14:32:15] Đầu vào: hdfs://localhost:9000/user/spark/hi_large/input/hadoop_input.txt
[14:32:15] Số cụm: 5
[14:32:15] Số lần lặp tối đa: 15
[14:32:15] Seed: 42
[14:32:15] Tol: 1e-4
[14:32:15] K-means++ initialization
[14:32:15] Catalyst optimizer + Tungsten

[14:32:16] BƯỚC 1/5: Đọc dữ liệu từ HDFS...
[14:34:42] Đã load 179,702,229 điểm dữ liệu (146.3s)

[14:34:43] BƯỚC 2/5: Tạo feature vectors...
[14:34:43]    Caching data vào memory/disk...
[14:37:18] Đã tạo 179,702,229 feature vectors (155.2s)

[14:37:19] BƯỚC 3/5: Cấu hình K-means (k-means++)...
[14:37:19]    Đã cấu hình: K=5, MaxIter=15, Seed=42, Tol=1e-4 (0.2s)

[14:37:19] BƯỚC 4/5: Đang huấn luyện K-means...
[14:37:19]    Sử dụng Catalyst + Tungsten; vui lòng đợi...

[14:49:34] Huấn luyện K-means hoàn thành (735.4s)

======================================================================
[14:49:34] HOÀN THÀNH K-MEANS CLUSTERING
======================================================================

[14:49:35] BƯỚC 5/5: Lưu 5 tâm cụm vào HDFS...
[14:49:35]    Đường dẫn: hdfs://localhost:9000/user/spark/hi_large/output_centroids
[14:49:36] Đã lưu tâm cụm (1.2s)

[14:49:37] PHÂN TÍCH KẾT QUẢ...
[14:51:02] Đã phân tích xong (85.1s)

PHÂN PHỐI CỤM:
   Cụm 0: 40,034,832 điểm (22.28%)
   Cụm 1: 42,665,746 điểm (23.74%)
   Cụm 2: 24,884,738 điểm (13.85%)
   Cụm 3: 50,933,651 điểm (28.34%)
   Cụm 4: 21,183,262 điểm (11.79%)

WSSSE (Within-cluster SSE): 1234567.89
   (Giá trị thấp hơn = cụm tốt hơn)

======================================================================
[14:51:02] HOÀN THÀNH TOÀN BỘ PIPELINE
[14:51:02] Tổng thời gian: 18.8 phút (1127.3s)
======================================================================
```

## Giải thích các bước

### Bước 1: Đọc dữ liệu từ HDFS (~2-3 phút)
- Load 179M dòng dữ liệu từ HDFS
- Parse CSV và infer schema
- Đây là bước I/O-intensive

### Bước 2: Chuyển đổi feature vectors (~2-3 phút)
- Chuyển DataFrame thành ML Vector format
- Cache vào memory/disk để tăng tốc các bước sau
- Trigger materialization với `.count()`

### Bước 3: Khởi tạo model (<1 giây)
- Cấu hình K-means model
- Chỉ là setup, không tính toán

### Bước 4: Training K-means (10-15 phút - bước chính)
- **Đây là bước mất thời gian nhất!**
- K-means++ initialization (thông minh hơn random)
- Iterative algorithm (tối đa 15 iterations)
- Catalyst optimizer tự động tối ưu execution plan
- Tungsten engine xử lý trong memory

### Bước 5: Lưu kết quả (~1-2 giây)
- Lưu 5 tâm cụm cuối cùng lên HDFS
- File rất nhỏ (~500 bytes)

### Phân tích kết quả (~1-2 phút)
- Transform toàn bộ dataset với model
- Đếm số điểm mỗi cụm
- Tính WSSSE (quality metric)

## Gợi ý

1. **Theo dõi tiến trình**: Mỗi bước có timestamp `[HH:MM:SS]` để bạn biết đang ở đâu

2. **Thời gian chính**: Bước 4 (training) chiếm ~70-80% tổng thời gian

3. **Nếu quá lâu**: 
   - Kiểm tra Spark UI: `http://localhost:4040`
   - Xem resource utilization (CPU, memory)
   - Có thể tăng memory trong `run_spark.sh`

4. **Progress bar**: Spark UI có visual progress bar chi tiết hơn

## Cải thiện hiệu năng

Nếu muốn nhanh hơn:
- Tăng `executor.memory` trong script
- Tăng số `shuffle.partitions`
- Sử dụng SSD cho HDFS
- Thêm executor nodes (nếu có cluster)
