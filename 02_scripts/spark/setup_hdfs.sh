#!/bin/bash
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“Š Dá»° ÃN: PhÃ¢n TÃ­ch Rá»­a Tiá»n â€” K-means (Polars + Spark)
# BÆ¯á»šC 3/7: THIáº¾T Láº¬P HDFS & UPLOAD Dá»® LIá»†U
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Má»¥c tiÃªu: Táº¡o thÆ° má»¥c HDFS, dá»n dáº¹p dá»¯ liá»‡u cÅ©, upload dá»¯ liá»‡u chuáº©n hoÃ¡
#            (temp local) lÃªn HDFS vÃ  (máº·c Ä‘á»‹nh) xoÃ¡ temp local.
# I/O:
#   - Input(local táº¡m): 01_data/processed/hadoop_input_temp.txt (~33GB)
#   - Output (HDFS)   : /user/spark/hi_large/input/hadoop_input.txt
# CÃ¡ch cháº¡y nhanh:
#   bash 02_scripts/spark/setup_hdfs.sh [--no-delete]
# Tham sá»‘:
#   --input <path>     : Ä‘Æ°á»ng dáº«n file táº¡m local
#   --hdfs-base <dir>  : thÆ° má»¥c gá»‘c HDFS (máº·c Ä‘á»‹nh /user/spark/hi_large)
#   --no-delete        : KHÃ”NG xoÃ¡ file táº¡m sau khi upload

set -euo pipefail

echo "=== BÆ¯á»šC 3: THIáº¾T Láº¬P HDFS CHO PYSPARK K-MEANS ==="

usage() {
  echo "CÃ¡ch dÃ¹ng: $0 [--input <local_temp_path>] [--hdfs-base <hdfs_base_dir>] [--no-delete]"
  echo "  --input       ÄÆ°á»ng dáº«n file táº¡m local (máº·c Ä‘á»‹nh: 01_data/processed/hadoop_input_temp.txt)"
  echo "  --hdfs-base   ThÆ° má»¥c gá»‘c trÃªn HDFS (máº·c Ä‘á»‹nh: /user/spark/hi_large)"
  echo "  --no-delete   KhÃ´ng xÃ³a file táº¡m local sau khi upload"
}

# XÃ¡c Ä‘á»‹nh Ä‘Æ°á»ng dáº«n thÆ° má»¥c
SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
ROOT_DIR="$(cd "$SCRIPT_DIR/../.." && pwd)"
DATA_PROCESSED="$ROOT_DIR/01_data/processed"

# Cáº¥u hÃ¬nh HDFS (cÃ³ thá»ƒ override qua CLI)
HDFS_BASE="/user/spark/hi_large"

# Kiá»ƒm tra HDFS cÃ³ Ä‘ang cháº¡y khÃ´ng
if ! hdfs dfs -test -e / 2>/dev/null; then
    echo "HDFS khÃ´ng thá»ƒ truy cáº­p. Vui lÃ²ng khá»Ÿi Ä‘á»™ng HDFS trÆ°á»›c."
    echo ""
    echo "Äá»ƒ khá»Ÿi Ä‘á»™ng HDFS:"
    echo "  1. Format namenode (chá»‰ láº§n Ä‘áº§u): hdfs namenode -format"
    echo "  2. Khá»Ÿi Ä‘á»™ng HDFS: start-dfs.sh"
    echo "  3. Kiá»ƒm tra tráº¡ng thÃ¡i: jps"
    exit 1
fi

echo "HDFS cÃ³ thá»ƒ truy cáº­p"
echo ""

# Parse CLI
INPUT_TEMP="$DATA_PROCESSED/hadoop_input_temp.txt"
DELETE_LOCAL=true

while [[ $# -gt 0 ]]; do
  case $1 in
    --input)
      INPUT_TEMP="$2"; shift 2 ;;
    --hdfs-base)
      HDFS_BASE="$2"; shift 2 ;;
    --no-delete)
      DELETE_LOCAL=false; shift ;;
    --help|-h)
      usage; exit 0 ;;
    *)
      echo "Tham sá»‘ khÃ´ng há»£p lá»‡: $1"; usage; exit 1 ;;
  esac
done

# Kiá»ƒm tra file dá»¯ liá»‡u táº¡m cÃ³ tá»“n táº¡i khÃ´ng

if [ ! -f "$INPUT_TEMP" ]; then
    echo "KhÃ´ng tÃ¬m tháº¥y tá»‡p Ä‘áº§u vÃ o táº¡m: $INPUT_TEMP"
    echo "   Vui lÃ²ng cháº¡y chuáº©n bá»‹ dá»¯ liá»‡u trÆ°á»›c:"
    echo "   python $ROOT_DIR/02_scripts/polars/02_prepare_polars.py"
    exit 1
fi

echo "ÄÃ£ tÃ¬m tháº¥y tá»‡p dá»¯ liá»‡u táº¡m"
echo ""

# Táº¡o thÆ° má»¥c HDFS
echo "Äang táº¡o thÆ° má»¥c HDFS..."
hdfs dfs -mkdir -p "$HDFS_BASE/input"
hdfs dfs -mkdir -p "$HDFS_BASE/output"

# Dá»n dáº¹p dá»¯ liá»‡u cÅ©
echo "Äang dá»n dáº¹p dá»¯ liá»‡u cÅ© trong HDFS..."
hdfs dfs -rm -f "$HDFS_BASE/input/*" 2>/dev/null
hdfs dfs -rm -r -f "$HDFS_BASE/output_centroids" 2>/dev/null

# Upload dá»¯ liá»‡u Ä‘áº§u vÃ o
echo ""
echo "Äang táº£i dá»¯ liá»‡u Ä‘áº§u vÃ o lÃªn HDFS..."
echo "  Nguá»“n: $INPUT_TEMP"
echo "  ÄÃ­ch: $HDFS_BASE/input/hadoop_input.txt"
hdfs dfs -put "$INPUT_TEMP" "$HDFS_BASE/input/hadoop_input.txt"

if [ $? -ne 0 ]; then
    echo "Tháº¥t báº¡i khi táº£i dá»¯ liá»‡u Ä‘áº§u vÃ o"
    exit 1
fi

# XÃ³a file táº¡m sau khi upload thÃ nh cÃ´ng (tuá»³ chá»n)
echo ""
if [ "$DELETE_LOCAL" = true ]; then
  echo "Äang dá»n dáº¹p tá»‡p táº¡m..."
  rm -f "$INPUT_TEMP"
  echo "ÄÃ£ xÃ³a tá»‡p táº¡m (dá»¯ liá»‡u chá»‰ cÃ²n trÃªn HDFS)"
else
  echo "Giá»¯ láº¡i tá»‡p táº¡m local theo yÃªu cáº§u (--no-delete)"
fi
echo "MLlib sáº½ tá»± Ä‘á»™ng khá»Ÿi táº¡o tÃ¢m cá»¥m vá»›i k-means++"

# XÃ¡c minh upload
echo ""
echo "Äang xÃ¡c minh upload..."
INPUT_SIZE=$(hdfs dfs -du -h "$HDFS_BASE/input/hadoop_input.txt" | awk '{print $1 " " $2}')

echo "  Dá»¯ liá»‡u Ä‘áº§u vÃ o: $INPUT_SIZE"

# Hiá»ƒn thá»‹ cáº¥u trÃºc thÆ° má»¥c HDFS
echo ""
echo "Cáº¥u trÃºc thÆ° má»¥c HDFS:"
hdfs dfs -ls -R "$HDFS_BASE"

echo ""
echo "==================================="
echo "HOÃ€N Táº¤T THIáº¾T Láº¬P HDFS!"
echo "==================================="
echo ""
echo "ÄÆ°á»ng dáº«n HDFS:"
echo "  Äáº§u vÃ o: hdfs://localhost:9000$HDFS_BASE/input/hadoop_input.txt"
echo "  Äáº§u ra: hdfs://localhost:9000$HDFS_BASE/output_centroids"
echo ""
echo "BÆ°á»›c tiáº¿p theo (BÆ°á»›c 4): Cháº¡y K-means vá»›i MLlib"
echo "  bash ./02_scripts/spark/run_spark.sh"
echo ""
echo "Ghi chÃº: MLlib dÃ¹ng k-means++ Ä‘á»ƒ khá»Ÿi táº¡o centroids tá»± Ä‘á»™ng"
