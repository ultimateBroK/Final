#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BƯỚC 3: KHỚI TẠO TÂM CỤM BAN ĐẦU (CENTROID INITIALIZATION)

Mục đích:
- Chọn ngẫu nhiên K=5 điểm làm tâm cụm ban đầu
- Tâm cụm = điểm trung tâm của mỗi nhóm giao dịch
- Thuật toán K-means cần điểm khởi tạo để bắt đầu

Thời gian chạy: ~30 giây
Input: data/processed/hadoop_input_temp.txt (từ bước 2)
Output: 01_data/processed/centroids_temp.txt (TẠM THỜI)

⚠️  LƯU Ý: File output sẽ BỊ XÓA TỰ ĐỘNG sau khi upload HDFS!
"""

import numpy as np
import polars as pl
import os

# ==================== CẤU HÌNH ĐƯỜNG DẪN ====================
ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
DATA_PROCESSED = os.path.join(ROOT_DIR, '01_data', 'processed')

print("="*70)
print("🎯 BƯỚC 3: KHỚI TẠO TÂM CỤM BAN ĐẦU")
print("="*70)
print()

# ==================== KIỂM TRA FILE INPUT ====================
temp_file = os.path.join(DATA_PROCESSED, 'hadoop_input_temp.txt')

print(f"Kiểm tra file input: {temp_file}")

if not os.path.exists(temp_file):
    print(f"❌ LỖI: Không tìm thấy file tạm!")
    print(f"   Đường dẫn: {temp_file}")
    print()
    print("💡 HƯỚNG DẪN:")
    print("   1. Chạy bước 2 trước:")
    print("      python 02_scripts/polars/02_prepare_polars.py")
    print("   2. Rồi chạy lại script này")
    print()
    exit(1)

print("✅ File input tồn tại!\n")

# ==================== ĐỌC DỮ LIỆU ====================
print("📊 ĐỌC DỮ LIỆU ĐÃ CHUẨN HÓA...")

# Đọc file không có header, đặt tên cột là f0, f1, ..., f8
# (9 đặc trưng: amount_received, amount_paid, amount_ratio, hour, ...)
df = pl.read_csv(
    temp_file,
    has_header=False,  # File không có header
    new_columns=[f'f{i}' for i in range(9)]  # Đặt tên cột f0-f8
)

print(f"✅ Đã load {len(df):,} dòng với {len(df.columns)} đặc trưng\n")

# ==================== LẤY MẪU NGẪU NHIÊN ====================
print("🎲 LẤY MẪU NGẪU NHIÊN ĐỂ KHỚI TẠO...")

# Lấy mẫu 100,000 dòng (hoặc ít hơn nếu file nhỏ)
n_samples = min(100000, len(df))
print(f"   Số mẫu: {n_samples:,} dòng")

# Sample = chọn ngẫu nhiên n dòng
sample = df.sample(n=n_samples)

# Chuyển sang NumPy array để tính toán nhanh hơn
data = sample.to_numpy()

print(f"✅ Đã lấy mẫu {len(data):,} điểm\n")

# ==================== CHỌ8N TÂM CỤM BAN ĐẦU ====================
k = 5  # Số cụm (clusters)

print(f"🎯 KHỚI TẠO {k} TÂM CỤM BAN ĐẦU...")
print(f"   Thuật toán: Chọn ngẫu nhiên {k} điểm từ {len(data):,} mẫu")
print()

# Chọn ngẫu nhiên k chỉ số (không lặp lại)
np.random.seed(42)  # Set seed để kết quả nhất quán
indices = np.random.choice(len(data), k, replace=False)

# Lấy các điểm tại chỉ số đã chọn
centroids = data[indices]

print(f"✅ Đã chọn {k} tâm cụm ban đầu")
print(f"   Mỗi tâm cụm có {centroids.shape[1]} đặc trưng\n")

# ==================== LƯU FILE TẠM THỜI ====================
print("💾 LƯU TÂM CỤM VÀO FILE TẠM THỜI...")
print("⚠️  LƯU Ý: File này sẽ tự động xóa sau khi upload HDFS!\n")

temp_centroids = os.path.join(DATA_PROCESSED, 'centroids_temp.txt')
print(f"   Đang ghi: {temp_centroids}")

# Lưu với định dạng CSV
# Mỗi dòng = 1 tâm cụm với 9 giá trị (cách nhau bởi dấu phẩy)
np.savetxt(
    temp_centroids,
    centroids,
    delimiter=',',  # Phân cách bằng dấu phẩy
    fmt='%.6f'      # Định dạng số: 6 chữ số thập phân
)

file_size = os.path.getsize(temp_centroids)
print("="*70)
print("✅ HOÀN TẤT KHỚI TẠO TÂM CỤM!")
print("="*70)
print(f"📄 File tạm: {temp_centroids}")
print(f"📊 Kích thước: {file_size} bytes (~{file_size/1024:.1f} KB)")
print(f"📊 Số cụm: {k}")
print(f"📊 Số đặc trưng mỗi cụm: {centroids.shape[1]}")
print()
print("📄 NỘI DUNG FILE (preview):")
print("-" * 70)
for i, centroid in enumerate(centroids):
    print(f"Cluster {i}: [{', '.join(f'{v:.3f}' for v in centroid[:5])}...]")
print()
print("⚠️  QUAN TRỌNG:")
print("   Cả 2 file tạm (hadoop_input_temp.txt và centroids_temp.txt)")
print("   sẽ BỊ XÓA sau khi upload lên HDFS (Bước 4)")
print("   Dữ liệu chỉ lưu trên HDFS để tuân thủ quy định bảo mật")
print()
print("💡 GỢI Ý TIẾP THEO:")
print("   Chạy bước 4: ./02_scripts/spark/setup_hdfs.sh")
print()
