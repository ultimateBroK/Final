#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BÆ¯á»šC 3: KHá»šI Táº O TÃ‚M Cá»¤M BAN Äáº¦U (CENTROID INITIALIZATION)

Má»¥c Ä‘Ã­ch:
- Chá»n ngáº«u nhiÃªn K=5 Ä‘iá»ƒm lÃ m tÃ¢m cá»¥m ban Ä‘áº§u
- TÃ¢m cá»¥m = Ä‘iá»ƒm trung tÃ¢m cá»§a má»—i nhÃ³m giao dá»‹ch
- Thuáº­t toÃ¡n K-means cáº§n Ä‘iá»ƒm khá»Ÿi táº¡o Ä‘á»ƒ báº¯t Ä‘áº§u

Thá»i gian cháº¡y: ~30 giÃ¢y
Input: data/processed/hadoop_input_temp.txt (tá»« bÆ°á»›c 2)
Output: 01_data/processed/centroids_temp.txt (Táº M THá»œI)

âš ï¸  LÆ¯U Ã: File output sáº½ Bá»Š XÃ“A Tá»° Äá»˜NG sau khi upload HDFS!
"""

import numpy as np
import polars as pl
import os

# ==================== Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN ====================
ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
DATA_PROCESSED = os.path.join(ROOT_DIR, '01_data', 'processed')

print("="*70)
print("ğŸ¯ BÆ¯á»šC 3: KHá»šI Táº O TÃ‚M Cá»¤M BAN Äáº¦U")
print("="*70)
print()

# ==================== KIá»‚M TRA FILE INPUT ====================
temp_file = os.path.join(DATA_PROCESSED, 'hadoop_input_temp.txt')

print(f"Kiá»ƒm tra file input: {temp_file}")

if not os.path.exists(temp_file):
    print(f"âŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y file táº¡m!")
    print(f"   ÄÆ°á»ng dáº«n: {temp_file}")
    print()
    print("ğŸ’¡ HÆ¯á»šNG DáºªN:")
    print("   1. Cháº¡y bÆ°á»›c 2 trÆ°á»›c:")
    print("      python 02_scripts/polars/02_prepare_polars.py")
    print("   2. Rá»“i cháº¡y láº¡i script nÃ y")
    print()
    exit(1)

print("âœ… File input tá»“n táº¡i!\n")

# ==================== Äá»ŒC Dá»® LIá»†U ====================
print("ğŸ“Š Äá»ŒC Dá»® LIá»†U ÄÃƒ CHUáº¨N HÃ“A...")

# Äá»c file khÃ´ng cÃ³ header, Ä‘áº·t tÃªn cá»™t lÃ  f0, f1, ..., f8
# (9 Ä‘áº·c trÆ°ng: amount_received, amount_paid, amount_ratio, hour, ...)
df = pl.read_csv(
    temp_file,
    has_header=False,  # File khÃ´ng cÃ³ header
    new_columns=[f'f{i}' for i in range(9)]  # Äáº·t tÃªn cá»™t f0-f8
)

print(f"âœ… ÄÃ£ load {len(df):,} dÃ²ng vá»›i {len(df.columns)} Ä‘áº·c trÆ°ng\n")

# ==================== Láº¤Y MáºªU NGáºªU NHIÃŠN ====================
print("ğŸ² Láº¤Y MáºªU NGáºªU NHIÃŠN Äá»‚ KHá»šI Táº O...")

# Láº¥y máº«u 100,000 dÃ²ng (hoáº·c Ã­t hÆ¡n náº¿u file nhá»)
n_samples = min(100000, len(df))
print(f"   Sá»‘ máº«u: {n_samples:,} dÃ²ng")

# Sample = chá»n ngáº«u nhiÃªn n dÃ²ng
sample = df.sample(n=n_samples)

# Chuyá»ƒn sang NumPy array Ä‘á»ƒ tÃ­nh toÃ¡n nhanh hÆ¡n
data = sample.to_numpy()

print(f"âœ… ÄÃ£ láº¥y máº«u {len(data):,} Ä‘iá»ƒm\n")

# ==================== CHá»Œ8N TÃ‚M Cá»¤M BAN Äáº¦U ====================
k = 5  # Sá»‘ cá»¥m (clusters)

print(f"ğŸ¯ KHá»šI Táº O {k} TÃ‚M Cá»¤M BAN Äáº¦U...")
print(f"   Thuáº­t toÃ¡n: Chá»n ngáº«u nhiÃªn {k} Ä‘iá»ƒm tá»« {len(data):,} máº«u")
print()

# Chá»n ngáº«u nhiÃªn k chá»‰ sá»‘ (khÃ´ng láº·p láº¡i)
np.random.seed(42)  # Set seed Ä‘á»ƒ káº¿t quáº£ nháº¥t quÃ¡n
indices = np.random.choice(len(data), k, replace=False)

# Láº¥y cÃ¡c Ä‘iá»ƒm táº¡i chá»‰ sá»‘ Ä‘Ã£ chá»n
centroids = data[indices]

print(f"âœ… ÄÃ£ chá»n {k} tÃ¢m cá»¥m ban Ä‘áº§u")
print(f"   Má»—i tÃ¢m cá»¥m cÃ³ {centroids.shape[1]} Ä‘áº·c trÆ°ng\n")

# ==================== LÆ¯U FILE Táº M THá»œI ====================
print("ğŸ’¾ LÆ¯U TÃ‚M Cá»¤M VÃ€O FILE Táº M THá»œI...")
print("âš ï¸  LÆ¯U Ã: File nÃ y sáº½ tá»± Ä‘á»™ng xÃ³a sau khi upload HDFS!\n")

temp_centroids = os.path.join(DATA_PROCESSED, 'centroids_temp.txt')
print(f"   Äang ghi: {temp_centroids}")

# LÆ°u vá»›i Ä‘á»‹nh dáº¡ng CSV
# Má»—i dÃ²ng = 1 tÃ¢m cá»¥m vá»›i 9 giÃ¡ trá»‹ (cÃ¡ch nhau bá»Ÿi dáº¥u pháº©y)
np.savetxt(
    temp_centroids,
    centroids,
    delimiter=',',  # PhÃ¢n cÃ¡ch báº±ng dáº¥u pháº©y
    fmt='%.6f'      # Äá»‹nh dáº¡ng sá»‘: 6 chá»¯ sá»‘ tháº­p phÃ¢n
)

file_size = os.path.getsize(temp_centroids)
print("="*70)
print("âœ… HOÃ€N Táº¤T KHá»šI Táº O TÃ‚M Cá»¤M!")
print("="*70)
print(f"ğŸ“„ File táº¡m: {temp_centroids}")
print(f"ğŸ“Š KÃ­ch thÆ°á»›c: {file_size} bytes (~{file_size/1024:.1f} KB)")
print(f"ğŸ“Š Sá»‘ cá»¥m: {k}")
print(f"ğŸ“Š Sá»‘ Ä‘áº·c trÆ°ng má»—i cá»¥m: {centroids.shape[1]}")
print()
print("ğŸ“„ Ná»˜I DUNG FILE (preview):")
print("-" * 70)
for i, centroid in enumerate(centroids):
    print(f"Cluster {i}: [{', '.join(f'{v:.3f}' for v in centroid[:5])}...]")
print()
print("âš ï¸  QUAN TRá»ŒNG:")
print("   Cáº£ 2 file táº¡m (hadoop_input_temp.txt vÃ  centroids_temp.txt)")
print("   sáº½ Bá»Š XÃ“A sau khi upload lÃªn HDFS (BÆ°á»›c 4)")
print("   Dá»¯ liá»‡u chá»‰ lÆ°u trÃªn HDFS Ä‘á»ƒ tuÃ¢n thá»§ quy Ä‘á»‹nh báº£o máº­t")
print()
print("ğŸ’¡ Gá»¢I Ã TIáº¾P THEO:")
print("   Cháº¡y bÆ°á»›c 4: ./02_scripts/spark/setup_hdfs.sh")
print()
