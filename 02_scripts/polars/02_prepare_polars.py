#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
B∆Ø·ªöC 2: X·ª¨ L√ù V√Ä TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG (FEATURE ENGINEERING)

M·ª•c ƒë√≠ch:
- Chuy·ªÉn d·ªØ li·ªáu th√¥ th√†nh d·∫°ng s·ªë ƒë·ªÉ thu·∫≠t to√°n K-means x·ª≠ l√Ω
- Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ timestamp (gi·ªù, ng√†y trong tu·∫ßn)
- T√≠nh to√°n c√°c t·ª∑ l·ªá (amount_ratio)
- M√£ h√≥a bi·∫øn ph√¢n lo·∫°i (categorical encoding)
- Chu·∫©n h√≥a t·∫•t c·∫£ features v·ªÅ [0, 1]

Th·ªùi gian ch·∫°y: ~10 ph√∫t
Input: 01_data/raw/HI-Large_Trans.csv (16GB, 179M d√≤ng)
Output: 01_data/processed/hadoop_input_temp.txt (33GB, T·∫†M TH·ªúI)

‚ö†Ô∏è  L∆ØU √ù: File output s·∫Ω B·ªä X√ìA T·ª∞ ƒê·ªòNG sau khi upload l√™n HDFS!
"""

import polars as pl
import numpy as np
import os
import time
from datetime import datetime

# ==================== C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N ====================
ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
DATA_RAW = os.path.join(ROOT_DIR, '01_data', 'raw', 'HI-Large_Trans.csv')
DATA_PROCESSED = os.path.join(ROOT_DIR, '01_data', 'processed')

# T·∫°o th∆∞ m·ª•c processed n·∫øu ch∆∞a c√≥
os.makedirs(DATA_PROCESSED, exist_ok=True)

def log_with_time(msg, step_start=None):
    """In message v·ªõi timestamp v√† th·ªùi gian elapsed n·∫øu c√≥"""
    current_time = datetime.now().strftime("%H:%M:%S")
    if step_start:
        elapsed = time.time() - step_start
        print(f"[{current_time}] {msg} (m·∫•t {elapsed:.1f}s)")
    else:
        print(f"[{current_time}] {msg}")

PIPELINE_START = time.time()

print("="*70)
print("B∆Ø·ªöC 2: X·ª¨ L√ù V√Ä TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG üîß")
print("="*70)
log_with_time(f"ƒê·ªçc t·ªáp: {DATA_RAW}")
log_with_time("Vui l√≤ng ƒë·ª£i (c√≥ th·ªÉ m·∫•t 5-10 ph√∫t)...")
print()

# ==================== ƒê·ªåC D·ªÆ LI·ªÜU ====================
step_start = time.time()
log_with_time("B∆Ø·ªöC 2.1/6: ƒêang thi·∫øt l·∫≠p ƒë·ªçc tr√¨ ho√£n (kh√¥ng t·∫£i to√†n b·ªô)...")
# Scan_csv = Lazy loading (kh√¥ng load h·∫øt v√†o RAM)
# Polars s·∫Ω x·ª≠ l√Ω t·ª´ng batch v√† streaming ra disk
df = pl.scan_csv(DATA_RAW)

log_with_time("ƒê√£ thi·∫øt l·∫≠p ƒë·ªçc tr√¨ ho√£n (kh√¥ng t·ªën RAM)", step_start)
print()

# ==================== TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG ====================
step_start = time.time()
log_with_time("B∆Ø·ªöC 2.2/6: ƒêang tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ d·ªØ li·ªáu th√¥...")
log_with_time("   ‚îú‚îÄ Timestamp ‚Üí Gi·ªù, Ng√†y trong tu·∫ßn")
log_with_time("   ‚îú‚îÄ Amount ‚Üí Received, Paid, Ratio")
log_with_time("   ‚îú‚îÄ Route ‚Üí Hash(From Bank ‚äï To Bank)")
log_with_time("   ‚îî‚îÄ Currencies ‚Üí Label encoding")

# Ch·ªçn v√† t·∫°o c√°c ƒë·∫∑c tr∆∞ng m·ªõi
df_features = df.select([
    # === ƒê·∫∂c tr∆∞ng th·ªùi gian ===
    # Parse timestamp t·ª´ chu·ªói sang datetime
    pl.col('Timestamp').str.strptime(pl.Datetime, format='%Y/%m/%d %H:%M').alias('datetime'),
    
    # === ƒê·∫∑c tr∆∞ng c∆° b·∫£n ===
    # S·ªë ti·ªÅn nh·∫≠n ƒë∆∞·ª£c
    pl.col('Amount Received').alias('amount_received'),
    # S·ªë ti·ªÅn tr·∫£
    pl.col('Amount Paid').alias('amount_paid'),
    
    # === ƒê·∫∑c tr∆∞ng t√≠nh to√°n ===
    # T·ª∑ l·ªá ti·ªÅn nh·∫≠n / ti·ªÅn tr·∫£ (+ 1e-6 ƒë·ªÉ tr√°nh chia cho 0)
    # N·∫øu ratio b·∫•t th∆∞·ªùng (v√≠ d·ª• >>1) c√≥ th·ªÉ l√† r·ª≠a ti·ªÅn
    (pl.col('Amount Received') / (pl.col('Amount Paid') + 1e-6)).alias('amount_ratio'),
    
    # === ƒê·∫∑c tr∆∞ng th·ªùi gian chi ti·∫øt ===
    # Gi·ªù trong ng√†y (0-23)
    # R·ª≠a ti·ªÅn th∆∞·ªùng x·∫£y ra v√†o gi·ªù kh√¥ng b√¨nh th∆∞·ªùng
    pl.col('Timestamp').str.strptime(pl.Datetime, format='%Y/%m/%d %H:%M').dt.hour().alias('hour'),
    # Ng√†y trong tu·∫ßn (0=Th·ª© 2, 6=Ch·ªß nh·∫≠t)
    pl.col('Timestamp').str.strptime(pl.Datetime, format='%Y/%m/%d %H:%M').dt.weekday().alias('day_of_week'),
    
    # === ƒê·∫∑c tr∆∞ng tuy·∫øn ƒë∆∞·ªùng ===
    # Hash(From Bank XOR To Bank) = m√£ h√≥a tuy·∫øn chuy·ªÉn ti·ªÅn
    # Ph√°t hi·ªán c√°c tuy·∫øn l·∫∑p l·∫°i nghi ng·ªù
    (pl.col('From Bank').hash() ^ pl.col('To Bank').hash()).alias('route_hash'),
    
    # === Bi·∫øn ph√¢n lo·∫°i (ch∆∞a m√£ h√≥a) ===
    pl.col('Receiving Currency').alias('recv_curr'),
    pl.col('Payment Currency').alias('payment_curr'),
    pl.col('Payment Format').alias('payment_format'),
])

log_with_time(f"ƒê√£ tr√≠ch xu·∫•t {len(df_features.collect_schema().names())} ƒë·∫∑c tr∆∞ng", step_start)
print()

# ==================== M√É H√ìA BI·∫æN PH√ÇN LO·∫†I ====================
step_start = time.time()
log_with_time("B∆Ø·ªöC 2.3/6: ƒêang m√£ h√≥a bi·∫øn ph√¢n lo·∫°i...")
log_with_time("   ‚îú‚îÄ Receiving Currency ‚Üí S·ªë")
log_with_time("   ‚îú‚îÄ Payment Currency ‚Üí S·ªë")
log_with_time("   ‚îî‚îÄ Payment Format ‚Üí S·ªë")

# Label Encoding: Chuy·ªÉn chu·ªói th√†nh s·ªë
# V√≠ d·ª•: "US Dollar" -> 0, "Yuan" -> 1, "Euro" -> 2, ...
df_features = df_features.with_columns([
    pl.col('recv_curr').cast(pl.Categorical).to_physical().alias('recv_curr_encoded'),
    pl.col('payment_curr').cast(pl.Categorical).to_physical().alias('payment_curr_encoded'),
    pl.col('payment_format').cast(pl.Categorical).to_physical().alias('payment_format_encoded'),
])

log_with_time("ƒê√£ m√£ h√≥a th√†nh s·ªë", step_start)
print()

# ==================== CH·ªåN ƒê·∫∂C TR∆ØNG S·ªê ====================
step_start = time.time()
log_with_time("B∆Ø·ªöC 2.4/6: ƒêang ch·ªçn c√°c ƒë·∫∑c tr∆∞ng s·ªë...")
# Ch·ªâ gi·ªØ l·∫°i 9 ƒë·∫∑c tr∆∞ng s·ªë (lo·∫°i b·ªè chu·ªói)
df_numeric = df_features.select([
    'amount_received',      # S·ªë ti·ªÅn nh·∫≠n
    'amount_paid',          # S·ªë ti·ªÅn tr·∫£
    'amount_ratio',         # T·ª∑ l·ªá
    'hour',                 # Gi·ªù
    'day_of_week',          # Ng√†y trong tu·∫ßn
    'route_hash',           # M√£ tuy·∫øn
    'recv_curr_encoded',    # Lo·∫°i ti·ªÅn nh·∫≠n (s·ªë)
    'payment_curr_encoded', # Lo·∫°i ti·ªÅn tr·∫£ (s·ªë)
    'payment_format_encoded', # H√¨nh th·ª©c thanh to√°n (s·ªë)
])

log_with_time(f"‚úÖ ƒê√£ ch·ªçn {len(df_numeric.collect_schema().names())} ƒë·∫∑c tr∆∞ng s·ªë cho K-means", step_start)
feature_list = df_numeric.collect_schema().names()
for i, feat in enumerate(feature_list, 1):
    print(f"   {i}. {feat}")
print()

# ==================== CHU·∫®N H√ìA (NORMALIZATION) ====================
step_start = time.time()
log_with_time("B∆Ø·ªöC 2.5/6: ƒêang chu·∫©n h√≥a d·ªØ li·ªáu (chu·∫©n Z-score)...")
log_with_time("   C√¥ng th·ª©c: (x - mean) / std")
log_with_time("   M·ª•c ƒë√≠ch: ƒê∆∞a t·∫•t c·∫£ features v·ªÅ c√πng scale")

# Chu·∫©n h√≥a: ƒê∆∞a t·∫•t c·∫£ v·ªÅ kho·∫£ng [0, 1]
# C√¥ng th·ª©c: (x - mean) / std
# L√Ω do: C√°c ƒë·∫∑c tr∆∞ng c√≥ scale kh√°c nhau (ti·ªÅn c√≥ th·ªÉ h√†ng tri·ªáu, gi·ªù ch·ªâ 0-23)
# N·∫øu kh√¥ng chu·∫©n h√≥a, K-means s·∫Ω b·ªã ·∫£nh h∆∞·ªüng b·ªüi ƒë·∫∑c tr∆∞ng c√≥ gi√° tr·ªã l·ªõn
df_normalized = df_numeric.select([
    ((pl.col(c) - pl.col(c).mean()) / pl.col(c).std()).alias(c)
    for c in df_numeric.collect_schema().names()
])

log_with_time(f"ƒê√£ chu·∫©n h√≥a {len(df_normalized.collect_schema().names())} ƒë·∫∑c tr∆∞ng", step_start)
print()

# ==================== L∆ØU FILE T·∫†M TH·ªúI ====================
step_start = time.time()
log_with_time("B∆Ø·ªöC 2.6/6: ƒêang l∆∞u t·ªáp t·∫°m th·ªùi cho HDFS...")
log_with_time("L∆∞u √Ω: T·ªáp n√†y s·∫Ω t·ª± ƒë·ªông x√≥a sau khi t·∫£i l√™n HDFS!")

temp_output = os.path.join(DATA_PROCESSED, 'hadoop_input_temp.txt')
log_with_time(f"   ƒêang ghi: {temp_output}")
log_with_time("   Vui l√≤ng ƒë·ª£i (c√≥ th·ªÉ m·∫•t 3-5 ph√∫t)...")
log_with_time("   Polars ƒëang ghi lu·ªìng d·ªØ li·ªáu xu·ªëng ƒëƒ©a (kh√¥ng t·ªën RAM)")
print()

# Ghi file kh√¥ng c√≥ header (ch·ªâ c√≥ s·ªë)
# Sink = streaming write (kh√¥ng t·ªën RAM)
df_normalized.sink_csv(temp_output, include_header=False)

log_with_time("ƒê√£ ghi xong t·ªáp", step_start)

file_size_mb = os.path.getsize(temp_output) / (1024 * 1024 * 1024)

total_time = time.time() - PIPELINE_START

print()
print("="*70)
log_with_time("HO√ÄN T·∫§T X·ª¨ L√ù D·ªÆ LI·ªÜU!")
print("="*70)
print()
print("TH·ªêNG K√ä K·∫æT QU·∫¢:")
print(f"   T·ªáp t·∫°m: {temp_output}")
print(f"   K√≠ch th∆∞·ªõc: {file_size_mb:.2f} GB")
print(f"   S·ªë d√≤ng: [ghi lu·ªìng - kh√¥ng ƒë·∫øm]")
print(f"   S·ªë ƒë·∫∑c tr∆∞ng: {len(df_normalized.collect_schema().names())}")
print(f"   Danh s√°ch ƒë·∫∑c tr∆∞ng:")
for i, feat in enumerate(df_normalized.collect_schema().names(), 1):
    print(f"      {i}. {feat}")
print()
log_with_time(f"T·ªïng th·ªùi gian b∆∞·ªõc 2: {total_time/60:.1f} ph√∫t ({total_time:.1f}s)")
print()
print("L∆ØU √ù QUAN TR·ªåNG:")
print("   ‚îú‚îÄ File n√†y ch·ªâ t·ªìn t·∫°i T·∫†M TH·ªúI!")
print("   ‚îú‚îÄ N√≥ s·∫Ω B·ªä X√ìA sau khi upload l√™n HDFS (B∆∞·ªõc 3)")
print("   ‚îî‚îÄ D·ªØ li·ªáu ch·ªâ l∆∞u tr√™n HDFS ƒë·ªÉ tu√¢n th·ªß quy ƒë·ªãnh b·∫£o m·∫≠t")
print()
print("G·ª¢I √ù TI·∫æP THEO:")
print("   Ch·∫°y b∆∞·ªõc 3: bash 02_scripts/spark/setup_hdfs.sh")
print("   (MLlib s·∫Ω t·ª± ƒë·ªông d√πng k-means++ initialization)")
print()
