# Changelog

## [2025-10-29-03] - Snapshot cáº­p nháº­t

### ğŸ“¦ Táº¡o snapshot má»›i nháº¥t

- Snapshot: `snapshot_20251029_213229`
- Thá»i gian: `2025-10-29 21:32:30`
- KÃ­ch thÆ°á»›c: `342.75 MB`
- Files:
  - `final_centroids.txt` (436 bytes)
  - `clustered_results.txt` (342.75 MB)
  - `suspicious_transactions.csv` (558 bytes)
  - `pipeline_log.md`

### ğŸ“ Docs cáº­p nháº­t

- Cáº­p nháº­t `bao_cao_du_an.md` theo snapshot má»›i
- Bá»• sung má»¥c "Latest snapshot" trong `README.md`

## [2025-10-29-02] - âš¡ NÃ¢ng Cáº¥p LÃªn MLlib K-means++

### ğŸš€ Chuyá»ƒn sang Spark MLlib 

#### Thay Ä‘á»•i chÃ­nh:

**Tá»‘i Æ°u thuáº­t toÃ¡n:**
- âœ… **Bá» bÆ°á»›c 3** (init centroids) - KhÃ´ng cáº§n ná»¯a!
- âœ… **MLlib tá»± Ä‘á»™ng dÃ¹ng k-means++** thay vÃ¬ random initialization
- âœ… **Catalyst optimizer** - Tá»‘i Æ°u query plan tá»± Ä‘á»™ng
- âœ… **Tungsten execution engine** - Code generation runtime
- âœ… **Adaptive query execution** - Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh partitions

**Files Ä‘Ã£ cáº­p nháº­t:**
- `scripts/spark/kmeans_spark.py` - Chuyá»ƒn sang MLlib KMeans
- `scripts/spark/run_spark.sh` - Bá» tham sá»‘ centroids.txt
- `scripts/spark/setup_hdfs.sh` - KhÃ´ng upload centroids ná»¯a
- `scripts/pipeline/full_pipeline_spark_v2.sh` - Giáº£m 8â†’7 bÆ°á»›c
- `scripts/polars/04_assign_clusters.py` - Äá»c tá»« results/ thay vÃ¬ processed/
- `README.md` - Cáº­p nháº­t hÆ°á»›ng dáº«n
- `docs/cau-truc.md` - Cáº­p nháº­t workflow

### âœ¨ Cáº£i tiáº¿n hiá»‡u suáº¥t

| Metric | TrÆ°á»›c (Custom RDD) | Sau (MLlib) | Cáº£i thiá»‡n |
|--------|-------------------|-------------|------------|
| Thá»i gian | ~20-30 phÃºt | **~10-15 phÃºt** | âš¡ **30-50%** |
| Sá»‘ bÆ°á»›c | 8 bÆ°á»›c | **7 bÆ°á»›c** | ğŸ“‰ -1 bÆ°á»›c |
| Initialization | Random | **K-means++** | âœ… Tá»‘t hÆ¡n |
| Convergence | 15-20 iterations | **10-12 iterations** | âš¡ Nhanh hÆ¡n |
| Code quality | Custom | **Built-in + optimized** | ğŸ› ï¸ Pro |

### ğŸ¯ Táº¡i sao k-means++ tá»‘t hÆ¡n?

**Random init (cÅ©):**
```python
centroids = random.sample(data, k=5)  # May máº¯n
```

**K-means++ (má»›i):**
```python
# Chá»n centroid Ä‘áº§u tiÃªn ngáº«u nhiÃªn
# Má»—i centroid tiáº¿p theo: chá»n Ä‘iá»ƒm XA NHáº¤T vá»›i cÃ¡c centroid hiá»‡n táº¡i
# â‡’ 5 centroids phÃ¢n tÃ¡n Ä‘á»u, khÃ´ng gáº§n nhau
# â‡’ Há»™i tá»¥ nhanh hÆ¡n vÃ  káº¿t quáº£ á»•n Ä‘á»‹nh hÆ¡n!
```

### ğŸ“ˆ Káº¿t quáº£

- ğŸš€ **Pipeline nhanh hÆ¡n 30-50%**
- ğŸ“Š **Káº¿t quáº£ tá»‘t hÆ¡n vÃ  á»•n Ä‘á»‹nh hÆ¡n**
- ğŸ› ï¸ **Code Ä‘Æ¡n giáº£n hÆ¡n** (dÃ¹ng built-in MLlib)
- ğŸ“ **Ãt bÆ°á»›c hÆ¡n** (7 thay vÃ¬ 8)

---

## [2025-10-29-01] - Tá»‘i Æ¯u Cáº¥u TrÃºc & Dá»on Dáº¹p

### ğŸ§¹ Tá»‘i Æ°u hÃ³a cáº¥u trÃºc dá»± Ã¡n

#### ÄÃ£ xÃ³a:
- âŒ `analysis_notebook.ipynb` (notebook tiáº¿ng Anh trÃ¹ng láº·p)

#### ÄÃ£ gá»™p:
- ğŸ“„ `BAO_CAO_TIEU_LUAN.md` + `BAO_CAO_TIEU_LUAN_2.md` â†’ `BAO_CAO_DU_AN.md`
  - Gá»™p bÃ¡o cÃ¡o chi tiáº¿t vÃ  pháº§n lÃ½ thuyáº¿t thÃ nh 1 file duy nháº¥t
  - Giá»¯ Ä‘áº§y Ä‘á»§ ná»™i dung cáº£ 2 bÃ¡o cÃ¡o
  - Dá»… quáº£n lÃ½ vÃ  tÃ¬m kiáº¿m hÆ¡n

#### ÄÃ£ di chuyá»ƒn:
- ğŸ“‚ Táº¥t cáº£ file hÆ°á»›ng dáº«n vÃ o `docs/`:
  - `INSTALLATION.md` â†’ `docs/cai-dat.md`
  - `JUPYTER_SETUP.md` â†’ `docs/jupyter.md`
  - `HUONG_DAN_CHAY.md` â†’ `docs/huong-dan.md`
  - `MIGRATION_GUIDE.md` â†’ `docs/migration.md`
  - `CAU_TRUC_DU_AN.md` â†’ `docs/cau-truc.md`
  - `PROJECT_OVERVIEW.md` â†’ `docs/tong-quan.md`
  - `HADOOP_ALTERNATIVES.md` â†’ `docs/hadoop-alternatives.md`

#### ÄÃ£ Ä‘á»•i tÃªn cho dá»… Ä‘á»c:
- ğŸ“ `CHANGELOG.md` â†’ `changelog.md`
- ğŸ““ `phan_tich_clustering.ipynb` â†’ `phan-tich.ipynb`
- ğŸ—‚ï¸ Táº¥t cáº£ file trong `docs/` dÃ¹ng kebab-case (dá»… gÃµ hÆ¡n)

### âœ¨ Cáº£i thiá»‡n

1. **Giáº£m trÃ¹ng láº·p**: Chá»‰ giá»¯ 1 notebook tiáº¿ng Viá»‡t, 1 bÃ¡o cÃ¡o gá»™p
2. **Bá»‘ cá»¥c rÃµ rÃ ng**: Táº¥t cáº£ tÃ i liá»‡u táº­p trung trong `docs/`
3. **TÃªn ngáº¯n gá»n**: DÃ¹ng kebab-case cho file markdown
4. **Dá»… tÃ¬m**: Cáº¥u trÃºc logic, khÃ´ng phÃ¢n tÃ¡n

### ğŸ“Š Káº¿t quáº£

**TrÆ°á»›c:**
```
- 2 notebook trÃ¹ng láº·p
- 2 bÃ¡o cÃ¡o riÃªng biá»‡t
- 7 file .md á»Ÿ root
- 2 file .md trong docs/
```

**Sau:**
```
- 1 notebook tiáº¿ng Viá»‡t
- 1 bÃ¡o cÃ¡o gá»™p
- 3 file .md á»Ÿ root (README, changelog, BAO_CAO_DU_AN)
- 7 file .md trong docs/ (táº¥t cáº£ hÆ°á»›ng dáº«n)
```

---

## [2025-10-28-02] - Documentation Overhaul

### ğŸ“š Cáº­p nháº­t Documentation

#### README.md

- Cáº­p nháº­t cáº¥u trÃºc thÆ° má»¥c chi tiáº¿t hÆ¡n (tree format)
- ThÃªm pháº§n cÃ i Ä‘áº·t Python dependencies
- Má»Ÿ rá»™ng hÆ°á»›ng dáº«n HDFS-Only Workflow vá»›i diagrams
- ThÃªm báº£ng chi tiáº¿t Pipeline Steps vá»›i thá»i gian Æ°á»›c tÃ­nh
- Cáº£i thiá»‡n kiáº¿n trÃºc há»‡ thá»‘ng vá»›i ASCII diagrams
- ThÃªm pháº§n kiá»ƒm tra HDFS data structure
- ThÃªm reference Ä‘áº¿n PROJECT_OVERVIEW.md

#### docs/PROJECT_OVERVIEW.md (Má»›i)
- Document tá»•ng quan Ä‘áº§y Ä‘á»§ vá» project
- Kiáº¿n trÃºc há»‡ thá»‘ng chi tiáº¿t
- Workflow 8 bÆ°á»›c vá»›i giáº£i thÃ­ch tá»«ng bÆ°á»›c
- Performance benchmarks
- Quy táº¯c HDFS-Only rÃµ rÃ ng
- Troubleshooting guide
- Monitoring vÃ  checkpoints
- Development guidelines

#### docs/HADOOP_ALTERNATIVES.md:
- Cáº­p nháº­t tiÃªu Ä‘á» thÃ nh "So SÃ¡nh CÃ¡c PhÆ°Æ¡ng PhÃ¡p"
- Nháº¥n máº¡nh Spark lÃ  giáº£i phÃ¡p hiá»‡n táº¡i
- ThÃªm so sÃ¡nh vá»›i Hadoop MapReduce (legacy)
- Cáº­p nháº­t code examples vá»›i paths thá»±c táº¿

### âœ¨ Cáº£i tiáº¿n

1. **TÃ i liá»‡u rÃµ rÃ ng hÆ¡n**: Dá»… hiá»ƒu cho ngÆ°á»i má»›i
2. **Visual diagrams**: ASCII art giÃºp hÃ¬nh dung kiáº¿n trÃºc
3. **Troubleshooting**: Giáº£i quyáº¿t váº¥n Ä‘á» thÆ°á»ng gáº·p
4. **Performance metrics**: Thá»i gian Æ°á»›c tÃ­nh rÃµ rÃ ng
5. **Development guide**: HÆ°á»›ng dáº«n cho developers

---

## [2025-10-28-01] - Major Restructuring

### ğŸ”„ Cáº¥u trÃºc thÆ° má»¥c má»›i

#### ÄÃ£ tá»• chá»©c láº¡i scripts theo chá»©c nÄƒng:

**`scripts/polars/`** - Data processing vá»›i Polars
- `explore_fast.py` - KhÃ¡m phÃ¡ dá»¯ liá»‡u nhanh
- `prepare_polars.py` - Feature engineering & normalization
- `init_centroids.py` - Khá»Ÿi táº¡o centroids cho K-means
- `assign_clusters_polars.py` - GÃ¡n nhÃ£n cluster cho dá»¯ liá»‡u
- `analyze_polars.py` - PhÃ¢n tÃ­ch káº¿t quáº£ clustering

**`scripts/spark/`** - PySpark K-means implementation
- `setup_hdfs.sh` - Upload dá»¯ liá»‡u lÃªn HDFS
- `run_spark.sh` - Cháº¡y Spark K-means trÃªn HDFS
- `kmeans_spark.py` - PySpark K-means implementation
- `download_from_hdfs.sh` - Táº£i káº¿t quáº£ tá»« HDFS

**`scripts/pipeline/`** - Pipeline orchestration
- `full_pipeline_spark.sh` - Cháº¡y toÃ n bá»™ pipeline
- `clean_spark.sh` - Dá»n dáº¹p project
- `reset_pipeline.sh` - Reset checkpoints

**`scripts/setup/`** - Installation & setup
- `install_spark.sh` - CÃ i Ä‘áº·t Apache Spark

### â™»ï¸ Chuyá»ƒn sang HDFS-only workflow

#### KhÃ´ng lÆ°u dá»¯ liá»‡u lá»›n á»Ÿ local:
- `prepare_polars.py` táº¡o temp files thay vÃ¬ file vÄ©nh viá»…n
- `init_centroids.py` táº¡o temp files thay vÃ¬ file vÄ©nh viá»…n
- `setup_hdfs.sh` tá»± Ä‘á»™ng xÃ³a temp files sau khi upload HDFS
- Dá»¯ liá»‡u chá»‰ tá»“n táº¡i trÃªn HDFS

### ğŸ—„ï¸ Archive legacy code

**`archive/`** - KhÃ´ng cÃ²n sá»­ dá»¥ng
- `hadoop/` - MapReduce implementation (legacy)
- `full_pipeline.sh` - Hadoop pipeline (legacy)
- `clean_outputs.sh` - Old cleanup script (legacy)

### ğŸ“ Cáº­p nháº­t documentation

#### README.md:
- Cáº­p nháº­t cáº¥u trÃºc thÆ° má»¥c má»›i
- Cáº­p nháº­t hÆ°á»›ng dáº«n sá»­ dá»¥ng vá»›i Ä‘Æ°á»ng dáº«n má»›i
- Nháº¥n máº¡nh HDFS-only workflow
- XÃ³a hÆ°á»›ng dáº«n Hadoop legacy

#### Paths Ä‘Ã£ thay Ä‘á»•i:
```bash
# CÅ¨
./scripts/full_pipeline_spark.sh
./scripts/clean_spark.sh
python scripts/prepare_polars.py

# Má»šI
./scripts/pipeline/full_pipeline_spark.sh
./scripts/pipeline/clean_spark.sh
python scripts/polars/prepare_polars.py
```

### âœ¨ Lá»£i Ã­ch cá»§a cáº¥u trÃºc má»›i

1. **Dá»… Ä‘iá»u hÆ°á»›ng**: CÃ¡c file Ä‘Æ°á»£c nhÃ³m theo chá»©c nÄƒng
2. **RÃµ rÃ ng hÆ¡n**: Biáº¿t ngay file nÃ o thuá»™c module nÃ o
3. **Dá»… báº£o trÃ¬**: ThÃªm/sá»­a/xÃ³a file dá»… dÃ ng hÆ¡n
4. **TuÃ¢n thá»§ quy táº¯c**: KhÃ´ng lÆ°u dá»¯ liá»‡u lá»›n local

### âš ï¸ Breaking Changes

Táº¥t cáº£ Ä‘Æ°á»ng dáº«n scripts Ä‘Ã£ thay Ä‘á»•i. Cáº§n cáº­p nháº­t:
- Automation scripts
- CI/CD pipelines
- Documentation references
- Shortcuts/aliases
