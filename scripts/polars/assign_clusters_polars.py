#!/usr/bin/env python3
# ==============================================================================
# File: assign_clusters_polars.py
# ==============================================================================
"""
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“Š Dá»° ÃN: PhÃ¢n TÃ­ch Rá»­a Tiá»n â€” K-means Clustering (Polars + Spark)
BÆ¯á»šC 6/7: GÃN NHÃƒN Cá»¤M CHO Tá»ªNG GIAO Dá»ŠCH
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

TÃ“M Táº®T
- Má»¥c tiÃªu: TÃ­nh khoáº£ng cÃ¡ch Euclidean tá»›i cÃ¡c tÃ¢m cá»¥m cuá»‘i cÃ¹ng vÃ  gÃ¡n nhÃ£n
  cá»¥m (0..K-1) cho tá»«ng giao dá»‹ch theo batch Ä‘á»ƒ tiáº¿t kiá»‡m RAM.
- CÃ´ng nghá»‡: Polars (Ä‘á»c stream tá»« HDFS) + NumPy (vectorized distance).

I/O & THá»œI GIAN
- Input : data/results/final_centroids.txt (centroids táº£i tá»« HDFS)
- Input : HDFS /user/spark/hi_large/input/hadoop_input.txt (179M dÃ²ng)
- Output: data/results/clustered_results.txt (ID cá»¥m má»—i giao dá»‹ch)
- Thá»i gian cháº¡y: ~10 phÃºt (tÃ¹y mÃ¡y)

CÃCH CHáº Y NHANH
  python scripts/polars/assign_clusters_polars.py \
    --centroids data/results/final_centroids.txt \
    --hdfs-path /user/spark/hi_large/input/hadoop_input.txt

THAM Sá» CLI
- --centroids <path> : ÄÆ°á»ng dáº«n file tÃ¢m cá»¥m (máº·c Ä‘á»‹nh data/results/final_centroids.txt)
- --hdfs-path <path> : ÄÆ°á»ng dáº«n HDFS tá»›i hadoop_input.txt

GHI CHÃš
- Xá»­ lÃ½ theo batch 1,000,000 báº£n ghi/lÆ°á»£t Ä‘á»ƒ á»•n Ä‘á»‹nh bá»™ nhá»›.
"""

import polars as pl
import numpy as np
import os
import subprocess
import argparse

# ==================== Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN ====================
ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
DATA_PROCESSED = os.path.join(ROOT_DIR, 'data', 'processed')
DATA_RESULTS = os.path.join(ROOT_DIR, 'data', 'results')

# Táº¡o thÆ° má»¥c results náº¿u chÆ°a cÃ³
os.makedirs(DATA_RESULTS, exist_ok=True)

print("="*70)
print("ğŸ·ï¸  BÆ¯á»šC 6: GÃN NHÃƒN Cá»¤M CHO Tá»ªNG GIAO Dá»ŠCH")
print("="*70)
print()

# ==================== Äá»ŒC TÃ‚M Cá»¤M CUá»I CÃ™NG ====================
print("ğŸ¯ Äá»ŒC TÃ‚M Cá»¤M CUá»I CÃ™NG Tá»ª BÆ¯á»šC 5...")

centroids_file = os.path.join(DATA_RESULTS, 'final_centroids.txt')
print(f"   File: {centroids_file}")

try:
    # Load táº­p tin chá»©a 5 tÃ¢m cá»¥m (má»—i dÃ²ng = 1 tÃ¢m cá»¥m vá»›i 9 giÃ¡ trá»‹)
    centroids = np.loadtxt(centroids_file, delimiter=',')
    
    # Náº¿u chá»‰ cÃ³ 1 tÃ¢m cá»¥m, reshape thÃ nh matrix 1x9
    if centroids.ndim == 1:
        centroids = centroids.reshape(1, -1)
    
    # Kiá»ƒm tra file rá»—ng
    if centroids.size == 0:
        raise ValueError("File tÃ¢m cá»¥m rá»—ng!")
    
    print(f"\nâœ… ÄÃ£ load {centroids.shape[0]} tÃ¢m cá»¥m")
    print(f"   Má»—i tÃ¢m cá»¥m cÃ³ {centroids.shape[1]} Ä‘áº·c trÆ°ng")
    print()
    
except (ValueError, OSError) as e:
    print(f"\nâŒ Lá»I: KhÃ´ng Ä‘á»c Ä‘Æ°á»£c file tÃ¢m cá»¥m!")
    print(f"   Chi tiáº¿t: {e}")
    print(f"   ÄÆ°á»ng dáº«n: {centroids_file}")
    print()
    print("ğŸ’¡ HÆ¯á»šNG DáºªN:")
    print("   1. Cháº¡y bÆ°á»›c 6 trÆ°á»›c:")
    print("      ./scripts/spark/download_from_hdfs.sh")
    print("   2. Rá»“i cháº¡y láº¡i script nÃ y")
    print()
    exit(1)

# ==================== Äá»ŒC Dá»® LIá»†U Tá»ª HDFS ====================
print("ğŸ“‚ Äá»ŒC Dá»® LIá»†U Tá»ª HDFS...")
print("   File HDFS: /user/spark/hi_large/input/hadoop_input.txt")
print("   (179M dÃ²ng, 33GB - Ä‘Ã£ chuáº©n hÃ³a)\n")

# Streaming tá»« HDFS vÃ  Ä‘á»c trá»±c tiáº¿p báº±ng Polars
try:
    print("ğŸ”„ Streaming tá»« HDFS (cÃ³ thá»ƒ máº¥t vÃ i phÃºt)...")
    
    # DÃ¹ng hdfs dfs -cat Ä‘á»ƒ stream dá»¯ liá»‡u
    # Giá»‘ng nhÆ° cat file nhÆ°ng tá»« HDFS
    process = subprocess.Popen(
        ["hdfs", "dfs", "-cat", "/user/spark/hi_large/input/hadoop_input.txt"],
        stdout=subprocess.PIPE,  # Láº¥y output
        stderr=subprocess.PIPE,  # Láº¥y error messages
    )
    
    # Äá»c trá»±c tiáº¿p tá»« stream (khÃ´ng cáº§n lÆ°u file táº¡m)
    print("ğŸ“Š Äang Ä‘á»c CSV tá»« HDFS stream...")
    stdout = process.stdout
    
    if stdout is None:
        raise Exception("HDFS process khÃ´ng cÃ³ stdout stream")
    
    # Polars Ä‘á»c trá»±c tiáº¿p tá»« stream
    # has_header=False vÃ¬ file chá»‰ chá»©a sá»‘
    df = pl.read_csv(
        stdout,
        has_header=False,
        new_columns=[f'f{i}' for i in range(9)]  # 9 Ä‘áº·c trÆ°ng
    )
    
    # Äá»£i process hoÃ n thÃ nh
    returncode = process.wait()
    
    if returncode != 0:
        # Náº¿u cÃ³ lá»—i, Ä‘á»c stderr
        stderr_data = process.stderr.read().decode() if process.stderr is not None else ""
        raise Exception(f"HDFS read tháº¥t báº¡i vá»›i code {returncode}: {stderr_data}")
    
    print(f"\nâœ… ÄÃ£ load {len(df):,} báº£n ghi tá»« HDFS")
    print()
    
except Exception as e:
    print(f"\nâŒ Lá»I: KhÃ´ng Ä‘á»c Ä‘Æ°á»£c dá»¯ liá»‡u tá»« HDFS!")
    print(f"   Chi tiáº¿t: {e}")
    print()
    print("ğŸ’¡ HÆ¯á»šNG DáºªN:")
    print("   1. Kiá»ƒm tra HDFS Ä‘ang cháº¡y:")
    print("      hdfs dfsadmin -report")
    print("   2. Kiá»ƒm tra file tá»“n táº¡i:")
    print("      hdfs dfs -ls /user/spark/hi_large/input/")
    print("   3. Náº¿u khÃ´ng cÃ³, cháº¡y láº¡i bÆ°á»›c 4:")
    print("      ./scripts/spark/setup_hdfs.sh")
    print()
    exit(1)

# ==================== TÃNH TOÃN KHOáº¢NG CÃCH VÃ€ GÃN Cá»¤M ====================
print("ğŸ“Š CHUYá»‚N SANG NUMPY VÃ€ TÃNH KHOáº¢NG CÃCH...")

# Chuyá»ƒn Polars DataFrame sang NumPy array Ä‘á»ƒ tÃ­nh toÃ¡n nhanh hÆ¡n
data = df.to_numpy()

print(f"   Dá»¯ liá»‡u: {data.shape[0]:,} dÃ²ng x {data.shape[1]} cá»™t")
print(f"   TÃ¢m cá»¥m: {centroids.shape[0]} cá»¥m x {centroids.shape[1]} Ä‘áº·c trÆ°ng")
print()

print("ğŸ”¢ TÃNH KHOáº¢NG CÃCH EUCLIDEAN (Batch Processing)...")
print("   Xá»­ lÃ½ 1 triá»‡u giao dá»‹ch má»—i láº§n Ä‘á»ƒ tiáº¿t kiá»‡m RAM\n")

# KÃ­ch thÆ°á»›c batch (sá»‘ dÃ²ng xá»­ lÃ½ cÃ¹ng lÃºc)
chunk_size = 1000000  # 1M giao dá»‹ch/batch

# Máº£ng chá»©a káº¿t quáº£: má»—i giao dá»‹ch thuá»™c cá»¥m nÃ o (0-4)
clusters = np.zeros(len(data), dtype=np.int32)

# Xá»­ lÃ½ tá»«ng batch
for i in range(0, len(data), chunk_size):
    end_idx = min(i + chunk_size, len(data))
    chunk = data[i:end_idx]  # Láº¥y 1M dÃ²ng
    
    # TÃ­nh khoáº£ng cÃ¡ch Euclidean: sqrt(sum((x - y)^2))
    # chunk[:, None, :] = (batch_size, 1, 9)
    # centroids[None, :, :] = (1, 5, 9)
    # Káº¿t quáº£: (batch_size, 5) = khoáº£ng cÃ¡ch Ä‘áº¿n 5 tÃ¢m cá»¥m
    distances = np.sqrt(((chunk[:, None, :] - centroids[None, :, :]) ** 2).sum(axis=2))
    
    # Chá»n cá»¥m cÃ³ khoáº£ng cÃ¡ch nhá» nháº¥t
    clusters[i:end_idx] = np.argmin(distances, axis=1)
    
    # In tiáº¿n Ä‘á»™ má»—i 10 batch
    if (i // chunk_size) % 10 == 0 or end_idx == len(data):
        percent = (end_idx / len(data)) * 100
        print(f"    ÄÃ£ xá»­ lÃ½ {end_idx:,}/{len(data):,} giao dá»‹ch ({percent:.1f}%)")

print(f"\nâœ… ÄÃ£ hoÃ n thÃ nh tÃ­nh toÃ¡n cho {len(data):,} giao dá»‹ch\n")

# ==================== LÆ¯U Káº¾T QUáº¢ ====================
print("ğŸ’¾ LÆ¯U Káº¾T QUáº¢...")

output_file = os.path.join(DATA_RESULTS, 'clustered_results.txt')
print(f"   File: {output_file}")

# LÆ°u máº£ng clusters vÃ o file (má»—i dÃ²ng = 1 cluster ID)
np.savetxt(output_file, clusters, fmt='%d')

# Thá»‘ng kÃª phÃ¢n phá»‘i
cluster_counts = np.bincount(clusters)

file_size_mb = os.path.getsize(output_file) / (1024 * 1024)
print("="*70)
print("âœ… HOÃ€N Táº¤T GÃN NHÃƒN Cá»¤M!")
print("="*70)
print(f"ğŸ“„ File káº¿t quáº£: {output_file}")
print(f"ğŸ“Š KÃ­ch thÆ°á»›c: {file_size_mb:.2f} MB")
print(f"ğŸ“Š Tá»•ng giao dá»‹ch: {len(clusters):,}")
print()
print("ğŸ“Š PHÃ‚N PHá»I Cá»¤M:")
for cluster_id, count in enumerate(cluster_counts):
    percent = (count / len(clusters)) * 100
    print(f"   Cluster {cluster_id}: {count:,} giao dá»‹ch ({percent:.2f}%)")
print()
print("ğŸ’¡ Gá»¢I Ã TIáº¾P THEO:")
print("   Cháº¡y bÆ°á»›c 7: python scripts/polars/analyze.py")
print()


