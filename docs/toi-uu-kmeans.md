# ğŸš€ Tá»‘i Æ¯u Thá»i Gian Cháº¡y K-means (KHÃ”NG áº¢nh HÆ°á»Ÿng Accuracy)

## ğŸ“Š PhÃ¢n TÃ­ch Hiá»‡n Táº¡i

**Thá»i gian:** ~20-30 phÃºt cho 179M Ä‘iá»ƒm  
**Bottlenecks:** Shuffle operations, I/O HDFS, Memory churn  
**YÃªu cáº§u:** Tá»‘i Æ°u tá»‘c Ä‘á»™ **KHÃ”NG lÃ m giáº£m accuracy**

---

## ğŸ¯ 8 CÃ¡ch Tá»‘i Æ¯u (100% Accuracy)

### 1ï¸âƒ£ **TÄƒng Sá»‘ Partitions** (Dá»… - Hiá»‡u quáº£ cao)

**Váº¥n Ä‘á» hiá»‡n táº¡i:**
```python
.config("spark.sql.shuffle.partitions", "200")  # QuÃ¡ Ã­t cho 179M rows!
.config("spark.default.parallelism", "200")
```

**Tá»‘i Æ°u:**
```python
# Rule of thumb: 2-4 partitions per CPU core
# Vá»›i 179M rows, nÃªn dÃ¹ng 400-800 partitions
.config("spark.sql.shuffle.partitions", "800") 
.config("spark.default.parallelism", "800")
```

**Táº¡i sao:** 
- 200 partitions = má»—i partition ~900K rows (quÃ¡ lá»›n!)
- 800 partitions = má»—i partition ~224K rows (tá»‘i Æ°u cho memory)

**Cáº£i thiá»‡n:** â¬‡ï¸ 20-30% thá»i gian

---

### 2ï¸âƒ£ **Persistent Caching vá»›i MEMORY_AND_DISK** (Dá»… - Hiá»‡u quáº£ cao)

**Váº¥n Ä‘á» hiá»‡n táº¡i:**
```python
data_rdd = sc.textFile(input_path).map(parse_point).cache()
```

**Tá»‘i Æ°u:**
```python
from pyspark import StorageLevel

data_rdd = sc.textFile(input_path) \
    .map(parse_point) \
    .persist(StorageLevel.MEMORY_AND_DISK_SER)  # Serialized + spillable
```

**Táº¡i sao:**
- `.cache()` = MEMORY_ONLY â†’ OOM náº¿u khÃ´ng Ä‘á»§ RAM
- `MEMORY_AND_DISK_SER` = serialize + spill to disk náº¿u cáº§n
- Serialized tiáº¿t kiá»‡m RAM ~3-5x

**Cáº£i thiá»‡n:** â¬‡ï¸ 15-20% thá»i gian, trÃ¡nh OOM

---

### 3ï¸âƒ£ **K-means++ Initialization** âœ… **ÄÃƒ ÃP Dá»¤NG Tá»° Äá»˜NG Bá»œI MLLIB**

âœ… **THÃ”NG TIN QUAN TRá»ŒNG:** MLlib K-means Ä‘Ã£ **Tá»° Äá»˜NG** sá»­ dá»¥ng **k-means++** (gá»i lÃ  "k-means||") trong ná»™i bá»™!

**Tráº¡ng thÃ¡i hiá»‡n táº¡i:**
- Dá»± Ã¡n Ä‘Ã£ chuyá»ƒn sang MLlib K-means
- MLlib tá»± Ä‘á»™ng dÃ¹ng thuáº­t toÃ¡n k-means|| (k-means++ song song hÃ³a)
- KhÃ´ng cáº§n file `03_init_centroids.py` ná»¯a â†’ ÄÃ£ loáº¡i bá»!

**Lá»£i Ã­ch:**
- MLlib k-means|| **tá»‘t hÆ¡n** random init (khá»Ÿi táº¡o thÃ´ng minh)
- Tá»± Ä‘á»™ng phÃ¢n tÃ¡n khá»Ÿi táº¡o trÃªn cluster
- Ãt iteration hÆ¡n (thÆ°á»ng 10-12 thay vÃ¬ 15)
- TrÃ¡nh local minima hiá»‡u quáº£

**CÃ¡ch MLlib thá»±c hiá»‡n:**
```python
# Trong code hiá»‡n táº¡i (kmeans_spark.py hoáº·c kmeans_mllib.py)
from pyspark.ml.clustering import KMeans

kmeans = KMeans() \
    .setK(5) \
    .setMaxIter(15) \
    .setInitMode("k-means||")  # â† Tá»° Äá»˜NG dÃ¹ng k-means++

model = kmeans.fit(vector_df)
```

**Káº¿t quáº£:** âœ… ÄÃ£ tá»‘i Æ°u! KhÃ´ng cáº§n lÃ m gÃ¬ thÃªm.

**Cáº£i thiá»‡n Ä‘áº¡t Ä‘Æ°á»£c:** â¬‡ï¸ 20-30% iterations (tá»« 15 â†’ 10-12)

---

### 4ï¸âƒ£ **Vectorized Operations vá»›i NumPy** (ÄÃ£ cÃ³ - Tá»‘i Æ°u thÃªm)

**Hiá»‡n táº¡i Ä‘Ã£ tá»‘t:**
```python
distances = np.linalg.norm(centroids - point_arr, axis=1)
```

**Tá»‘i Æ°u thÃªm vá»›i numba JIT:**
```python
from numba import jit

@jit(nopython=True, fastmath=True)
def closest_centroid_fast(point, centroids):
    min_dist = float('inf')
    min_idx = 0
    for i in range(len(centroids)):
        dist = 0.0
        for j in range(len(point)):
            diff = point[j] - centroids[i, j]
            dist += diff * diff
        if dist < min_dist:
            min_dist = dist
            min_idx = i
    return min_idx
```

**Táº¡i sao:** 
- Numba JIT compile â†’ 10-100x nhanh hÆ¡n Python
- KhÃ´ng cáº§n sqrt (so sÃ¡nh squared distance Ä‘á»§)

**Cáº£i thiá»‡n:** â¬‡ï¸ 10-15% thá»i gian map operations

---

### 5ï¸âƒ£ **TÄƒng Memory Allocation** (Dá»… - Náº¿u cÃ³ RAM)

**Hiá»‡n táº¡i:**
```bash
DRIVER_MEMORY="4g"
EXECUTOR_MEMORY="4g"
```

**Tá»‘i Æ°u (náº¿u cÃ³ 32GB+ RAM):**
```bash
DRIVER_MEMORY="8g"
EXECUTOR_MEMORY="8g"  # hoáº·c 12g, 16g

# VÃ  tÄƒng off-heap memory
--conf spark.memory.offHeap.enabled=true
--conf spark.memory.offHeap.size=4g
```

**Táº¡i sao:**
- Giáº£m spill to disk
- Giáº£m GC overhead
- Cache nhiá»u data hÆ¡n

**Cáº£i thiá»‡n:** â¬‡ï¸ 20-30% náº¿u cÃ³ Ä‘á»§ RAM

---

### 6ï¸âƒ£ **Repartition Data Evenly** (Trung bÃ¬nh)

**ThÃªm trÆ°á»›c cache:**
```python
data_rdd = sc.textFile(input_path) \
    .map(parse_point) \
    .repartition(800)  # Äáº£m báº£o phÃ¢n phá»‘i Ä‘á»u
    .persist(StorageLevel.MEMORY_AND_DISK_SER)
```

**Táº¡i sao:**
- HDFS blocks cÃ³ thá»ƒ khÃ´ng Ä‘á»u â†’ data skew
- Repartition Ä‘áº£m báº£o má»—i partition ~same size

**Cáº£i thiá»‡n:** â¬‡ï¸ 10-15% báº±ng cÃ¡ch trÃ¡nh stragglers

---

### 7ï¸âƒ£ **Early Stopping (Convergence-based)** (Dá»…)

âš ï¸ **QUAN TRá»ŒNG:** Chá»‰ dÃ¹ng convergence check dá»±a trÃªn **full data**, khÃ´ng sample!

**Hiá»‡n táº¡i:**
```python
if centroid_shift < tolerance:
    break
```

**Tá»‘i Æ°u (KHÃ”NG máº¥t accuracy):**
```python
# DÃ¹ng relative change thay vÃ¬ absolute
relative_change = centroid_shift / np.linalg.norm(centroids)
if relative_change < tolerance:
    print(f"âœ… Há»™i tá»¥: relative change = {relative_change:.6f}")
    break

# Hoáº·c track stability (Ä‘áº£m báº£o thá»±c sá»± há»™i tá»¥)
shifts = []
shifts.append(centroid_shift)
if len(shifts) >= 3:
    # Náº¿u 3 iterations liÃªn tiáº¿p Ä‘á»u < tolerance â†’ thá»±c sá»± há»™i tá»¥
    if all(s < tolerance for s in shifts[-3:]):
        print("âœ… Há»™i tá»¥ á»•n Ä‘á»‹nh qua 3 iterations")
        break
```

**Táº¡i sao an toÃ n:**
- Váº«n dÃ¹ng full data, khÃ´ng sample
- Chá»‰ dá»«ng khi THá»°C Sá»° há»™i tá»¥
- KhÃ´ng lÃ m giáº£m accuracy

**Cáº£i thiá»‡n:** â¬‡ï¸ 5-15% thá»i gian (náº¿u há»™i tá»¥ sá»›m)

---

### 8ï¸âƒ£ **Use DataFrames thay vÃ¬ RDDs** (KhÃ³ - Hiá»‡u quáº£ cao)

**Hiá»‡n táº¡i (RDD-based):**
```python
data_rdd = sc.textFile(input_path).map(parse_point)
```

**Tá»‘i Æ°u (DataFrame-based vá»›i MLlib):**
```python
from pyspark.ml.clustering import KMeans as MLlibKMeans
from pyspark.ml.feature import VectorAssembler

# Äá»c as DataFrame
df = spark.read.csv(input_path, header=False, inferSchema=True)

# Convert to vector column
assembler = VectorAssembler(
    inputCols=df.columns,
    outputCol="features"
)
vector_df = assembler.transform(df)

# Use MLlib K-means (tá»‘i Æ°u sáºµn)
kmeans = MLlibKMeans() \
    .setK(5) \
    .setMaxIter(15) \
    .setFeaturesCol("features") \
    .setPredictionCol("cluster") \
    .setInitMode("k-means||")  # K-means++

model = kmeans.fit(vector_df)
centroids = model.clusterCenters()
```

**Táº¡i sao:**
- Catalyst optimizer tá»± Ä‘á»™ng tá»‘i Æ°u
- Tungsten execution (off-heap, columnar)
- MLlib Ä‘Ã£ implement nhiá»u optimizations

**Cáº£i thiá»‡n:** â¬‡ï¸ 30-50% thá»i gian vá»›i built-in optimizations

---

## ğŸ“ˆ Tá»•ng Há»£p Cáº£i Thiá»‡n (100% Accuracy)

| Optimization | Äá»™ KhÃ³ | Cáº£i Thiá»‡n | Accuracy | Æ¯u TiÃªn |
|--------------|---------|-----------|----------|----------|
| 1. TÄƒng partitions (800) | â­ Dá»… | 20-30% | âœ… 100% | ğŸ”¥ CAO |
| 2. MEMORY_AND_DISK_SER | â­ Dá»… | 15-20% | âœ… 100% | ğŸ”¥ CAO |
| 5. TÄƒng memory (náº¿u cÃ³) | â­ Dá»… | 20-30% | âœ… 100% | ğŸ”¥ CAO |
| 8. Use DataFrame/MLlib | â­â­â­ KhÃ³ | 30-50% | âœ… 100% | ğŸ”¥ CAO |
| 3. K-means++ init | âœ… ÄÃƒ ÃP Dá»¤NG | 20-30% iter | âœ… Better! | âœ… HOÃ€N THÃ€NH |
| 6. Repartition evenly | â­â­ TB | 10-15% | âœ… 100% | ğŸŸ¡ TRUNG |
| 4. Numba JIT | â­â­ TB | 10-15% | âœ… 100% | ğŸŸ¢ THáº¤P |
| 7. Early stopping | â­ Dá»… | 5-15% | âœ… 100% | ğŸŸ¢ THáº¤P |

**âŒ LOáº I Bá» (áº¢nh hÆ°á»Ÿng accuracy):**
- ~~Mini-batch K-means~~ â†’ Giáº£m 2-5% accuracy
- ~~Sampling convergence~~ â†’ CÃ³ thá»ƒ dá»«ng sá»›m khi chÆ°a há»™i tá»¥ tháº­t

**Tá»•ng cáº£i thiá»‡n cÃ³ thá»ƒ:** â¬‡ï¸ **50-70%** thá»i gian (tá»« 30 phÃºt â†’ 9-15 phÃºt)

**âœ… Äáº£m báº£o:** Accuracy giá»‘ng há»‡t thuáº­t toÃ¡n gá»‘c!

---

## ğŸ› ï¸ Quick Wins (LÃ m Ngay - 15 PhÃºt)

### File: `scripts/spark/run_spark.sh`

```bash
# Thay Ä‘á»•i dÃ²ng 89-90
--conf spark.sql.shuffle.partitions=800 \  # 200 â†’ 800
--conf spark.default.parallelism=800 \     # 200 â†’ 800
```

### File: `scripts/spark/kmeans_spark.py`

```python
# Thay dÃ²ng 75-77
from pyspark import StorageLevel
data_rdd = sc.textFile(input_path) \
    .map(parse_point) \
    .repartition(800) \  # ThÃªm dÃ²ng nÃ y
    .persist(StorageLevel.MEMORY_AND_DISK_SER)  # cache() â†’ persist()
```

**Cháº¡y láº¡i vÃ  so sÃ¡nh thá»i gian!**

---

## ğŸš€ Advanced: Chuyá»ƒn Sang MLlib (30-60 PhÃºt)

Táº¡o file má»›i: `scripts/spark/kmeans_mllib.py`

```python
#!/usr/bin/env python3
from pyspark.sql import SparkSession
from pyspark.ml.clustering import KMeans
from pyspark.ml.feature import VectorAssembler
import sys

def run_kmeans_mllib(input_path, output_path, k=5, max_iter=15):
    spark = SparkSession.builder \
        .appName("K-means MLlib") \
        .config("spark.sql.shuffle.partitions", "800") \
        .getOrCreate()
    
    # Äá»c CSV
    df = spark.read.csv(input_path, header=False, inferSchema=True)
    
    # Convert to feature vector
    assembler = VectorAssembler(
        inputCols=df.columns,
        outputCol="features"
    )
    vector_df = assembler.transform(df).select("features")
    
    # K-means vá»›i k-means++ init
    kmeans = KMeans() \
        .setK(k) \
        .setMaxIter(max_iter) \
        .setInitMode("k-means||") \
        .setFeaturesCol("features")
    
    # Train
    model = kmeans.fit(vector_df)
    
    # Save centroids
    centroids = model.clusterCenters()
    # ... save logic
    
    spark.stop()

if __name__ == "__main__":
    run_kmeans_mllib(sys.argv[1], sys.argv[2])
```

---

## ğŸ“Š Monitoring & Profiling

### 1. Spark UI (Quan trá»ng!)

```bash
# Khi cháº¡y, truy cáº­p:
http://localhost:4040

# Xem:
# - Stages: thá»i gian má»—i stage
# - Storage: memory usage
# - Executors: CPU/memory per executor
# - SQL: query plans (náº¿u dÃ¹ng DataFrame)
```

### 2. Track Metrics

ThÃªm vÃ o `kmeans_spark.py`:

```python
import time

iteration_times = []
for iteration in range(1, max_iterations + 1):
    iter_start = time.time()
    
    # ... K-means logic ...
    
    iter_time = time.time() - iter_start
    iteration_times.append(iter_time)
    print(f"â±ï¸  Iteration {iteration} took {iter_time:.2f}s")

print(f"\nğŸ“Š Stats:")
print(f"   Avg iteration time: {np.mean(iteration_times):.2f}s")
print(f"   Total time: {sum(iteration_times):.2f}s")
```

---

## ğŸ¯ Káº¿t Luáº­n

**LÃ m ngay (5-15 phÃºt) - 100% Accuracy:**
1. âœ… TÄƒng partitions lÃªn 800
2. âœ… DÃ¹ng `persist(MEMORY_AND_DISK_SER)`
3. âœ… TÄƒng memory náº¿u cÃ³ RAM
4. âœ… Add .repartition(800)

**Expected:** â¬‡ï¸ 30-50% thá»i gian (30 phÃºt â†’ 15-20 phÃºt)

**LÃ m sau (náº¿u cáº§n nhanh hÆ¡n) - 100% Accuracy:**
1. âœ… Chuyá»ƒn sang MLlib K-means (cÃ³ Catalyst optimizer)
2. âœ… K-means++ initialization (tháº­m chÃ­ tá»‘t hÆ¡n!)
3. âœ… Numba JIT compilation
4. âœ… Repartition evenly

**Expected:** â¬‡ï¸ 50-70% thá»i gian (30 phÃºt â†’ 9-15 phÃºt)

**âŒ KHÃ”NG lÃ m (máº¥t accuracy):**
- âŒ Mini-batch K-means
- âŒ Sampling cho convergence check

---

**Good luck optimizing! ğŸš€**
