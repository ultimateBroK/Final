# üöÄ Gi·∫£i Ph√°p Thay Th·∫ø Hadoop Cho B∆∞·ªõc 4 (K-means Clustering)

## üìä T·ªïng Quan

B∆∞·ªõc 4 hi·ªán t·∫°i s·ª≠ d·ª•ng Hadoop MapReduce ƒë·ªÉ ch·∫°y K-means clustering tr√™n 180M rows (~33GB hadoop_input.txt). ƒê√¢y l√† bottleneck l·ªõn nh·∫•t trong pipeline.

**V·∫•n ƒë·ªÅ v·ªõi Hadoop:**
- Overhead cao t·ª´ HDFS I/O (upload/download qua network)
- Serialize/deserialize d·ªØ li·ªáu m·ªói iteration
- Shuffle phase ch·∫≠m gi·ªØa mapper v√† reducer
- Kh√¥ng t·∫≠n d·ª•ng t·ªëi ƒëa RAM c·ªßa single machine
- Th·ªùi gian x·ª≠ l√Ω: **1-2 gi·ªù** cho 15 iterations

---

## üî• Gi·∫£i Ph√°p 1: MiniBatch K-Means (Scikit-learn)

**Nhanh nh·∫•t v√† ƒë∆°n gi·∫£n nh·∫•t - KHUY·∫æN NGH·ªä cho h·∫ßu h·∫øt tr∆∞·ªùng h·ª£p**

### T·∫°i sao nhanh h∆°n?
- Ch·ªâ load m·ªôt ph·∫ßn d·ªØ li·ªáu v√†o RAM m·ªói l·∫ßn (streaming)
- T·∫≠n d·ª•ng optimized C/Cython code
- Kh√¥ng c√≥ network overhead
- Converge nhanh h∆°n (~5-8 iterations thay v√¨ 15)

### Implementation

```python path=null start=null
# step4_minibatch_kmeans.py
import polars as pl
import numpy as np
from sklearn.cluster import MiniBatchKMeans
from time import time

print("Step 4: MiniBatch K-Means (Scikit-learn)")
start_time = time()

# Load d·ªØ li·ªáu ƒë√£ chu·∫©n b·ªã
print("Loading data...")
df = pl.read_csv('hadoop_input.txt', has_header=False, 
                 new_columns=['Amount_Received', 'Amount_Paid', 'amount_ratio', 'hour'])

# Chuy·ªÉn sang numpy array
X = df.to_numpy()
print(f"Data shape: {X.shape}")

# MiniBatch K-Means
print("\nTraining MiniBatch K-Means...")
kmeans = MiniBatchKMeans(
    n_clusters=5,
    batch_size=100000,      # Process 100K rows at a time
    max_iter=100,           # Max iterations
    random_state=42,
    verbose=1,
    compute_labels=False,   # Ch·ªâ train centroids, kh√¥ng assign labels
    init='k-means++',
    n_init=3                # S·ªë l·∫ßn kh·ªüi t·∫°o
)

# C√≥ th·ªÉ train tr√™n chunks n·∫øu RAM kh√¥ng ƒë·ªß
# Uncomment block n√†y n·∫øu RAM < 16GB:
# chunk_size = 10_000_000
# for i in range(0, len(X), chunk_size):
#     print(f"Processing chunk {i//chunk_size + 1}...")
#     kmeans.partial_fit(X[i:i+chunk_size])

# Train to√†n b·ªô (n·∫øu RAM ƒë·ªß)
kmeans.fit(X)

# L∆∞u centroids
centroids = kmeans.cluster_centers_
np.savetxt('final_centroids.txt', centroids, delimiter='\t', fmt='%.6f')

elapsed_time = time() - start_time
print(f"\n‚úÖ Completed in {elapsed_time/60:.2f} minutes")
print(f"Centroids saved to final_centroids.txt")
print(f"Inertia: {kmeans.inertia_:.2f}")
```

### K·∫øt qu·∫£ d·ª± ki·∫øn
- **Th·ªùi gian**: 8-15 ph√∫t (thay v√¨ 1-2 gi·ªù)
- **T·ªëc ƒë·ªô tƒÉng**: 8-12x nhanh h∆°n
- **RAM c·∫ßn**: 8-16GB
- **Accuracy**: ~95-98% so v·ªõi standard K-Means

### Ch·∫°y pipeline m·ªõi

```bash
# S·ª≠a file full_pipeline.sh, thay d√≤ng 132:
# bash run_hadoop_optimized.sh 2>&1 | tee -a "$LOG_FILE"
# Th√†nh:
python step4_minibatch_kmeans.py 2>&1 | tee -a "$LOG_FILE"
```

---

## ‚ö° Gi·∫£i Ph√°p 2: Dask-ML K-Means (Parallel + Out-of-Core)

**T·ªët nh·∫•t khi c√≥ nhi·ªÅu CPU cores v√† mu·ªën 100% accuracy**

### T·∫°i sao nhanh h∆°n?
- Parallel processing tr√™n t·∫•t c·∫£ CPU cores
- Out-of-core: x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn h∆°n RAM
- Thu·∫≠t to√°n gi·ªëng sklearn nh∆∞ng distributed

### Implementation

```python path=null start=null
# step4_dask_kmeans.py
import dask.dataframe as dd
import dask.array as da
from dask_ml.cluster import KMeans
from time import time
import numpy as np

print("Step 4: Dask-ML K-Means (Parallel)")
start_time = time()

# Load v·ªõi Dask (lazy)
print("Loading data with Dask...")
df = dd.read_csv('hadoop_input.txt', header=None, 
                 names=['Amount_Received', 'Amount_Paid', 'amount_ratio', 'hour'],
                 blocksize='256MB')  # Process in 256MB chunks

# Convert to dask array
X = df.to_dask_array(lengths=True)
print(f"Data shape: {X.shape}")

# Dask K-Means
print("\nTraining Dask K-Means...")
kmeans = KMeans(
    n_clusters=5,
    init='k-means++',
    max_iter=100,
    random_state=42
)

kmeans.fit(X)

# L∆∞u centroids
centroids = kmeans.cluster_centers_
np.savetxt('final_centroids.txt', centroids, delimiter='\t', fmt='%.6f')

elapsed_time = time() - start_time
print(f"\n‚úÖ Completed in {elapsed_time/60:.2f} minutes")
print(f"Centroids saved to final_centroids.txt")
```

### C√†i ƒë·∫∑t

```bash
pip install dask[complete] dask-ml
```

### K·∫øt qu·∫£ d·ª± ki·∫øn
- **Th·ªùi gian**: 15-25 ph√∫t
- **T·ªëc ƒë·ªô tƒÉng**: 4-6x nhanh h∆°n
- **RAM c·∫ßn**: C√≥ th·ªÉ x·ª≠ l√Ω data > RAM
- **Accuracy**: 100% gi·ªëng standard K-Means

---

## üî¨ Gi·∫£i Ph√°p 3: FAISS K-Means (GPU-accelerated)

**Nhanh nh·∫•t n·∫øu c√≥ GPU**

### T·∫°i sao nhanh h∆°n?
- Ch·∫°y tr√™n GPU (h√†ng ngh√¨n cores)
- Optimized cho high-dimensional data
- Vector operations si√™u nhanh

### Implementation

```python path=null start=null
# step4_faiss_kmeans.py
import polars as pl
import numpy as np
import faiss
from time import time

print("Step 4: FAISS K-Means (GPU-accelerated)")
start_time = time()

# Load data
print("Loading data...")
df = pl.read_csv('hadoop_input.txt', has_header=False,
                 new_columns=['Amount_Received', 'Amount_Paid', 'amount_ratio', 'hour'])
X = df.to_numpy().astype('float32')  # FAISS requires float32

print(f"Data shape: {X.shape}")
n_samples, n_features = X.shape

# FAISS K-Means
print("\nTraining FAISS K-Means...")
n_clusters = 5
kmeans = faiss.Kmeans(
    d=n_features,           # dimensionality
    k=n_clusters,           # number of clusters
    niter=100,              # max iterations
    verbose=True,
    gpu=True,               # Use GPU (set False if no GPU)
    nredo=3,                # s·ªë l·∫ßn kh·ªüi t·∫°o
    seed=42
)

# Train
kmeans.train(X)

# L∆∞u centroids
centroids = kmeans.centroids
np.savetxt('final_centroids.txt', centroids, delimiter='\t', fmt='%.6f')

elapsed_time = time() - start_time
print(f"\n‚úÖ Completed in {elapsed_time/60:.2f} minutes")
print(f"Centroids saved to final_centroids.txt")
print(f"Final objective: {kmeans.obj[-1]:.2f}")
```

### C√†i ƒë·∫∑t

```bash
# CPU version
pip install faiss-cpu

# GPU version (n·∫øu c√≥ CUDA)
pip install faiss-gpu
```

### K·∫øt qu·∫£ d·ª± ki·∫øn
- **Th·ªùi gian GPU**: 3-5 ph√∫t (30-40x nhanh h∆°n!)
- **Th·ªùi gian CPU**: 10-15 ph√∫t (8-12x nhanh h∆°n)
- **RAM/VRAM c·∫ßn**: 8-16GB
- **Accuracy**: 100%

---

## üî• Gi·∫£i Ph√°p 4: Apache Spark MLlib

**T·ªët nh·∫•t cho distributed computing v·ªõi API d·ªÖ d√πng h∆°n Hadoop**

### T·∫°i sao t·ªët h∆°n Hadoop?
- API high-level d·ªÖ d√πng h∆°n MapReduce
- In-memory processing (nhanh h∆°n 10-100x so v·ªõi Hadoop)
- Kh√¥ng c·∫ßn vi·∫øt mapper/reducer th·ªß c√¥ng
- MLlib c√≥ s·∫µn K-Means optimized
- V·∫´n distributed nh∆∞ Hadoop nh∆∞ng hi·ªán ƒë·∫°i h∆°n

### Implementation

```python path=null start=null
# step4_spark_kmeans.py
from pyspark.sql import SparkSession
from pyspark.ml.clustering import KMeans
from pyspark.ml.feature import VectorAssembler
from time import time
import numpy as np

print("Step 4: Apache Spark K-Means")
start_time = time()

# Kh·ªüi t·∫°o Spark
spark = SparkSession.builder \
    .appName("HI-Large K-Means") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .config("spark.sql.shuffle.partitions", "200") \
    .config("spark.default.parallelism", "200") \
    .getOrCreate()

print("Loading data...")
# Load d·ªØ li·ªáu
df = spark.read.csv('hadoop_input.txt', inferSchema=True, header=False)
df = df.toDF('Amount_Received', 'Amount_Paid', 'amount_ratio', 'hour')

print(f"Data loaded: {df.count()} rows")

# Vector assembly (Spark MLlib requires vector format)
assembler = VectorAssembler(
    inputCols=['Amount_Received', 'Amount_Paid', 'amount_ratio', 'hour'],
    outputCol='features'
)
df_features = assembler.transform(df)

# K-Means
print("\nTraining Spark K-Means...")
kmeans = KMeans(
    k=5,
    maxIter=100,
    seed=42,
    featuresCol='features',
    predictionCol='cluster'
)

# Train
model = kmeans.fit(df_features)

# L·∫•y centroids
centroids = model.clusterCenters()
centroids_array = np.array(centroids)

# L∆∞u centroids
np.savetxt('final_centroids.txt', centroids_array, delimiter='\t', fmt='%.6f')

# Metrics
wssse = model.summary.trainingCost
print(f"\nWithin Set Sum of Squared Errors: {wssse:.2f}")
print(f"Number of iterations: {model.summary.numIter}")

elapsed_time = time() - start_time
print(f"\n‚úÖ Completed in {elapsed_time/60:.2f} minutes")
print(f"Centroids saved to final_centroids.txt")

# Cleanup
spark.stop()
```

### C√†i ƒë·∫∑t

```bash
# C√†i PySpark
pip install pyspark

# Ho·∫∑c v·ªõi conda
conda install pyspark
```

### K·∫øt qu·∫£ d·ª± ki·∫øn
- **Th·ªùi gian (local mode)**: 20-30 ph√∫t
- **Th·ªùi gian (cluster mode)**: 10-15 ph√∫t
- **T·ªëc ƒë·ªô tƒÉng**: 3-6x nhanh h∆°n Hadoop
- **RAM c·∫ßn**: 8-16GB (local), scalable (cluster)
- **Accuracy**: 100%

### Spark vs Hadoop

| Ti√™u ch√≠ | Hadoop MapReduce | Apache Spark |
|----------|------------------|-------------|
| X·ª≠ l√Ω | Disk-based | In-memory |
| API | Low-level (mapper/reducer) | High-level (DataFrame, ML) |
| T·ªëc ƒë·ªô | Baseline | 10-100x nhanh h∆°n |
| Code | Nhi·ªÅu, ph·ª©c t·∫°p | √çt, d·ªÖ hi·ªÉu |
| Iterative algorithms | Ch·∫≠m (write to disk) | Nhanh (keep in memory) |
| Setup | ƒê∆°n gi·∫£n | Medium |

### Spark Standalone (Local Mode)

```python path=null start=null
# step4_spark_kmeans_local.py - Simplified cho single machine
from pyspark.sql import SparkSession
from pyspark.ml.clustering import KMeans
from pyspark.ml.feature import VectorAssembler
import numpy as np

# Spark local mode - d√πng t·∫•t c·∫£ cores
spark = SparkSession.builder \
    .appName("HI-Large K-Means Local") \
    .master("local[*]") \
    .config("spark.driver.memory", "8g") \
    .config("spark.driver.maxResultSize", "4g") \
    .getOrCreate()

print("Loading data with Spark (local mode)...")
df = spark.read.csv('hadoop_input.txt', inferSchema=True, header=False) \
    .toDF('f0', 'f1', 'f2', 'f3')

# Cache ƒë·ªÉ tƒÉng t·ªëc
df.cache()
print(f"Total rows: {df.count():,}")

# Vector assembly
assembler = VectorAssembler(inputCols=['f0', 'f1', 'f2', 'f3'], outputCol='features')
df_vec = assembler.transform(df).select('features')

# K-Means v·ªõi tuning
kmeans = KMeans(k=5, seed=42, maxIter=100, initMode='k-means||')
model = kmeans.fit(df_vec)

# L∆∞u k·∫øt qu·∫£
centroids = np.array(model.clusterCenters())
np.savetxt('final_centroids.txt', centroids, delimiter='\t', fmt='%.6f')

print(f"‚úÖ WSSSE: {model.summary.trainingCost:.2f}")
print(f"Iterations: {model.summary.numIter}")

spark.stop()
```

### Ch·∫°y v·ªõi Spark Submit (Production)

```bash
# submit_spark_job.sh
spark-submit \
  --master local[*] \
  --driver-memory 8g \
  --executor-memory 8g \
  --conf spark.sql.shuffle.partitions=200 \
  --conf spark.default.parallelism=200 \
  step4_spark_kmeans.py
```

### ∆Øu ƒëi·ªÉm Spark so v·ªõi c√°c gi·∫£i ph√°p kh√°c

‚úÖ **vs Hadoop MapReduce**:
- Nhanh h∆°n 10-30x (in-memory)
- Code ng·∫Øn g·ªçn h∆°n r·∫•t nhi·ªÅu
- API hi·ªán ƒë·∫°i, d·ªÖ maintain

‚úÖ **vs Scikit-learn/FAISS**:
- Scalable ra nhi·ªÅu m√°y (horizontal scaling)
- X·ª≠ l√Ω ƒë∆∞·ª£c data > RAM c·ªßa 1 m√°y
- Fault tolerant (production-ready)

‚ùå **vs Scikit-learn MiniBatch**:
- Ch·∫≠m h∆°n tr√™n single machine
- Setup ph·ª©c t·∫°p h∆°n
- Overhead t·ª´ Spark framework

### Khi n√†o n√™n d√πng Spark?

‚úÖ **N√äN d√πng khi**:
- C√≥ cluster nhi·ªÅu m√°y
- D·ªØ li·ªáu > 100GB
- C·∫ßn fault tolerance cao
- Pipeline c√≥ nhi·ªÅu b∆∞·ªõc ML kh√°c
- Team ƒë√£ quen v·ªõi Spark ecosystem

‚ùå **KH√îNG N√äN d√πng khi**:
- Single machine ƒë·ªß RAM (d√πng MiniBatch/FAISS nhanh h∆°n)
- Ch·ªâ c·∫ßn POC/prototype
- D·ªØ li·ªáu < 50GB (overhead kh√¥ng ƒë√°ng)

---

## üêª Gi·∫£i Ph√°p 5: Polars + Custom K-Means

**Best balance gi·ªØa t·ªëc ƒë·ªô v√† control**

### Implementation

```python path=null start=null
# step4_polars_kmeans.py
import polars as pl
import numpy as np
from time import time

def kmeans_iteration(df, centroids):
    """M·ªôt iteration c·ªßa K-means using Polars"""
    # T√≠nh kho·∫£ng c√°ch ƒë·∫øn t·∫•t c·∫£ centroids
    distances = []
    for i, centroid in enumerate(centroids):
        dist_expr = pl.lit(0.0)
        for j, col in enumerate(df.columns):
            dist_expr = dist_expr + (pl.col(col) - centroid[j]) ** 2
        distances.append(dist_expr.sqrt().alias(f'dist_{i}'))
    
    # Assign clusters
    df_with_dist = df.with_columns(distances)
    dist_cols = [f'dist_{i}' for i in range(len(centroids))]
    df_clustered = df_with_dist.with_columns(
        pl.concat_list(dist_cols).list.arg_min().alias('cluster')
    )
    
    # T√≠nh centroids m·ªõi
    new_centroids = []
    for i in range(len(centroids)):
        cluster_data = df_clustered.filter(pl.col('cluster') == i)
        if len(cluster_data) > 0:
            centroid = cluster_data.select(df.columns).mean().to_numpy()[0]
            new_centroids.append(centroid)
        else:
            new_centroids.append(centroids[i])  # Keep old centroid
    
    return np.array(new_centroids), df_clustered['cluster']

print("Step 4: Custom K-Means with Polars")
start_time = time()

# Load data
print("Loading data...")
df = pl.read_csv('hadoop_input.txt', has_header=False,
                 new_columns=['f0', 'f1', 'f2', 'f3'])

# Load initial centroids
centroids = np.loadtxt('centroids.txt', delimiter='\t')
print(f"Data shape: {df.shape}")
print(f"Initial centroids shape: {centroids.shape}")

# K-Means iterations
max_iter = 100
tolerance = 1e-4

for iteration in range(max_iter):
    print(f"\nIteration {iteration + 1}/{max_iter}")
    
    # M·ªôt iteration
    new_centroids, labels = kmeans_iteration(df, centroids)
    
    # Check convergence
    centroid_shift = np.linalg.norm(new_centroids - centroids)
    print(f"Centroid shift: {centroid_shift:.6f}")
    
    if centroid_shift < tolerance:
        print(f"‚úÖ Converged at iteration {iteration + 1}")
        break
    
    centroids = new_centroids

# L∆∞u centroids
np.savetxt('final_centroids.txt', centroids, delimiter='\t', fmt='%.6f')

elapsed_time = time() - start_time
print(f"\n‚úÖ Completed in {elapsed_time/60:.2f} minutes")
```

### K·∫øt qu·∫£ d·ª± ki·∫øn
- **Th·ªùi gian**: 20-30 ph√∫t
- **T·ªëc ƒë·ªô tƒÉng**: 3-5x nhanh h∆°n
- **RAM c·∫ßn**: 16-32GB
- **Accuracy**: 100%

---

## üìä So S√°nh C√°c Gi·∫£i Ph√°p

| Ph∆∞∆°ng Ph√°p | Th·ªùi Gian | T·ªëc ƒê·ªô TƒÉng | RAM C·∫ßn | Accuracy | ƒê·ªô Ph·ª©c T·∫°p | Khuy·∫øn Ngh·ªã |
|-------------|-----------|-------------|---------|----------|-------------|-------------|
| **Hadoop MapReduce** (hi·ªán t·∫°i) | 1-2 gi·ªù | 1x | 4-8GB | 100% | Medium | ‚ùå Ch·∫≠m nh·∫•t |
| **MiniBatch K-Means** | 8-15 ph√∫t | 8-12x | 8-16GB | 95-98% | Easy | ‚úÖ **BEST** |
| **Dask-ML K-Means** | 15-25 ph√∫t | 4-6x | >RAM OK | 100% | Easy | ‚úÖ RAM th·∫•p |
| **FAISS (GPU)** | 3-5 ph√∫t | 30-40x | 8GB VRAM | 100% | Medium | ‚úÖ C√≥ GPU |
| **FAISS (CPU)** | 10-15 ph√∫t | 8-12x | 8-16GB | 100% | Medium | ‚úÖ Nhanh |
| **Spark (local)** | 20-30 ph√∫t | 3-6x | 8-16GB | 100% | Medium | ‚úÖ Modern |
| **Spark (cluster)** | 10-15 ph√∫t | 6-12x | Scalable | 100% | Medium | ‚úÖ Production |
| **Polars Custom** | 20-30 ph√∫t | 3-5x | 16-32GB | 100% | Hard | ‚ö†Ô∏è Advanced |

---

## üéØ Khuy·∫øn Ngh·ªã C·ª• Th·ªÉ

### Tr∆∞·ªùng h·ª£p 1: Development/Testing (∆∞u ti√™n t·ªëc ƒë·ªô)
```bash
# D√πng MiniBatch K-Means
pip install scikit-learn
python step4_minibatch_kmeans.py
```
**K·∫øt qu·∫£**: 8-15 ph√∫t, accuracy 95-98%

### Tr∆∞·ªùng h·ª£p 2: Production (∆∞u ti√™n accuracy)
```bash
# D√πng FAISS CPU
pip install faiss-cpu
python step4_faiss_kmeans.py
```
**K·∫øt qu·∫£**: 10-15 ph√∫t, accuracy 100%

### Tr∆∞·ªùng h·ª£p 3: C√≥ GPU
```bash
# D√πng FAISS GPU
pip install faiss-gpu
python step4_faiss_kmeans.py
```
**K·∫øt qu·∫£**: 3-5 ph√∫t, accuracy 100%

### Tr∆∞·ªùng h·ª£p 4: RAM th·∫•p (<8GB)
```bash
# D√πng Dask-ML (out-of-core)
pip install dask[complete] dask-ml
python step4_dask_kmeans.py
```
**K·∫øt qu·∫£**: 15-25 ph√∫t, accuracy 100%

### Tr∆∞·ªùng h·ª£p 5: Mu·ªën thay Hadoop b·∫±ng framework hi·ªán ƒë·∫°i h∆°n
```bash
# D√πng Apache Spark (API d·ªÖ h∆°n, nhanh h∆°n Hadoop)
pip install pyspark
python step4_spark_kmeans.py
```
**K·∫øt qu·∫£**: 20-30 ph√∫t (local), 10-15 ph√∫t (cluster), accuracy 100%

---

## üîß Migration Guide

### B∆∞·ªõc 1: Ch·ªçn gi·∫£i ph√°p
D·ª±a v√†o b·∫£ng so s√°nh v√† khuy·∫øn ngh·ªã tr√™n

### B∆∞·ªõc 2: T·∫°o file m·ªõi
T·∫°o file script t∆∞∆°ng ·ª©ng (v√≠ d·ª•: `step4_minibatch_kmeans.py`)

### B∆∞·ªõc 3: S·ª≠a pipeline
S·ª≠a file `full_pipeline.sh`:

```bash
# T√¨m d√≤ng 132:
bash run_hadoop_optimized.sh 2>&1 | tee -a "$LOG_FILE"

# Thay th√†nh (v√≠ d·ª• v·ªõi MiniBatch):
python step4_minibatch_kmeans.py 2>&1 | tee -a "$LOG_FILE"
```

### B∆∞·ªõc 4: Test
```bash
# Reset pipeline
./reset_pipeline.sh

# Ch·∫°y l·∫°i
./full_pipeline.sh
```

### B∆∞·ªõc 5: Verify k·∫øt qu·∫£
```bash
# So s√°nh centroids
cat final_centroids.txt

# Ch·∫°y b∆∞·ªõc 5-6 nh∆∞ b√¨nh th∆∞·ªùng
python assign_clusters_polars.py
python analyze_polars.py
```

---

## ‚ö†Ô∏è L∆∞u √ù Quan Tr·ªçng

### 1. Compatibility v·ªõi pipeline hi·ªán t·∫°i
- T·∫•t c·∫£ c√°c gi·∫£i ph√°p ƒë·ªÅu output `final_centroids.txt` c√πng format
- B∆∞·ªõc 5 v√† 6 kh√¥ng c·∫ßn thay ƒë·ªïi
- Ch·ªâ c·∫ßn thay th·∫ø b∆∞·ªõc 4

### 2. Trade-offs
- **MiniBatch**: M·∫•t m·ªôt ch√∫t accuracy (~2-5%) nh∆∞ng c·ª±c nhanh
- **Dask/FAISS**: 100% accuracy nh∆∞ng c·∫ßn c√†i th√™m dependencies
- **GPU**: C·∫ßn CUDA/NVIDIA GPU

### 3. Khi n√†o v·∫´n n√™n d√πng Hadoop?
- D·ªØ li·ªáu > 100GB v√† kh√¥ng fit v√†o RAM
- C√≥ cluster nhi·ªÅu m√°y
- C·∫ßn fault tolerance cao (production critical)

### 4. Optimization tips
- Gi·∫£m s·ªë features n·∫øu c√≥ th·ªÉ (step 2)
- Sample data cho testing
- Tune `batch_size` v√† `max_iter`

---

## üìà ∆Ø·ªõc T√≠nh Hi·ªáu Su·∫•t

### Pipeline hi·ªán t·∫°i
```
Step 1: 1 ph√∫t
Step 2: 10 ph√∫t  
Step 3: <1 ph√∫t
Step 4: 90 ph√∫t (HADOOP) ‚è∞
Step 5: 15 ph√∫t
Step 6: 5 ph√∫t
--------------------
TOTAL: ~122 ph√∫t (~2 gi·ªù)
```

### Pipeline m·ªõi (v·ªõi MiniBatch K-Means)
```
Step 1: 1 ph√∫t
Step 2: 10 ph√∫t  
Step 3: <1 ph√∫t
Step 4: 10 ph√∫t (MiniBatch) ‚ö°
Step 5: 15 ph√∫t
Step 6: 5 ph√∫t
--------------------
TOTAL: ~42 ph√∫t
```

**TƒÉng t·ªëc t·ªïng th·ªÉ**: **~3x nhanh h∆°n** (2 gi·ªù ‚Üí 42 ph√∫t)

---

## üöÄ Quick Start Commands

```bash
# Gi·∫£i ph√°p khuy·∫øn ngh·ªã (MiniBatch K-Means)
pip install scikit-learn

# T·∫°o file step4_minibatch_kmeans.py (copy code t·ª´ Gi·∫£i ph√°p 1)

# S·ª≠a full_pipeline.sh d√≤ng 132
# T·ª´: bash run_hadoop_optimized.sh 2>&1 | tee -a "$LOG_FILE"
# Th√†nh: python step4_minibatch_kmeans.py 2>&1 | tee -a "$LOG_FILE"

# Reset v√† ch·∫°y
./reset_pipeline.sh
./full_pipeline.sh
```

**Expected result**: Pipeline ho√†n th√†nh trong 40-50 ph√∫t thay v√¨ 2 gi·ªù! üéâ

---

## üìû Troubleshooting

### RAM kh√¥ng ƒë·ªß
```python
# Trong step4_minibatch_kmeans.py, uncomment ph·∫ßn train theo chunks:
chunk_size = 10_000_000
for i in range(0, len(X), chunk_size):
    kmeans.partial_fit(X[i:i+chunk_size])
```

### Accuracy kh√¥ng ƒë·ªß cao
```python
# TƒÉng batch_size v√† s·ªë iterations
kmeans = MiniBatchKMeans(
    n_clusters=5,
    batch_size=200000,    # TƒÉng t·ª´ 100K
    max_iter=200,         # TƒÉng t·ª´ 100
    n_init=5              # TƒÉng s·ªë l·∫ßn kh·ªüi t·∫°o
)
```

### Mu·ªën gi·ªØ 100% accuracy
‚Üí D√πng FAISS ho·∫∑c Dask-ML thay v√¨ MiniBatch

---

## üéì K·∫øt Lu·∫≠n

**TL;DR**: Thay Hadoop b·∫±ng **MiniBatch K-Means** (scikit-learn) ƒë·ªÉ:
- ‚ö° Gi·∫£m th·ªùi gian t·ª´ 90 ph√∫t ‚Üí 10 ph√∫t (9x nhanh h∆°n)
- üéØ Pipeline t·ªïng th·ªÉ: 2 gi·ªù ‚Üí 40 ph√∫t (3x nhanh h∆°n)
- üíª ƒê∆°n gi·∫£n h∆°n, kh√¥ng c·∫ßn HDFS
- üìä Accuracy v·∫´n r·∫•t t·ªët (95-98%)

**Khi n√†o d√πng c√°c gi·∫£i ph√°p kh√°c**:
- C√≥ GPU ‚Üí **FAISS GPU** (3-5 ph√∫t)
- C·∫ßn 100% accuracy ‚Üí **FAISS CPU** ho·∫∑c **Dask-ML**
- RAM th·∫•p ‚Üí **Dask-ML** (out-of-core)
- Mu·ªën upgrade t·ª´ Hadoop ‚Üí **Apache Spark** (API hi·ªán ƒë·∫°i, nhanh h∆°n 10-30x)
- C√≥ cluster ‚Üí **Spark cluster mode** (scalable, production-ready)

### üí° Spark vs Other Solutions

**Apache Spark l√† l·ª±a ch·ªçn t·ªët n·∫øu**:
- ‚úÖ B·∫°n ƒë√£ c√≥ Hadoop cluster (d·ªÖ migrate)
- ‚úÖ Team quen v·ªõi distributed systems
- ‚úÖ D·ªØ li·ªáu s·∫Ω ti·∫øp t·ª•c tƒÉng l√™n
- ‚úÖ C·∫ßn pipeline ML ph·ª©c t·∫°p h∆°n trong t∆∞∆°ng lai

**MiniBatch/FAISS t·ªët h∆°n Spark n·∫øu**:
- ‚úÖ Single machine ƒë·ªß m·∫°nh (RAM ‚â• 16GB)
- ‚úÖ ∆Øu ti√™n t·ªëc ƒë·ªô t·ªëi ƒëa
- ‚úÖ Kh√¥ng c·∫ßn distributed
- ‚úÖ Setup ƒë∆°n gi·∫£n
