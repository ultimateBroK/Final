# ‚ö° Polars + Hadoop cho HI-Large_Trans.csv (T·ªëi ∆Øu)

**File**: 16GB, 179M rows  
**Th·ªùi gian**: ~1.5 gi·ªù (thay v√¨ 3-5 gi·ªù)

---

## Setup

```bash
pip install polars numpy scikit-learn
```

---

## B∆∞·ªõc 1: Kh√°m ph√° nhanh v·ªõi Polars

```python
# explore_fast.py
import polars as pl

# Lazy scan - kh√¥ng load v√†o RAM
df = pl.scan_csv('HI-Large_Trans.csv')

# Xem schema
print(df.schema)

# Sample 100k rows
sample = df.head(100000).collect()
print(sample)
print(sample.describe())

# Check laundering distribution
print("\nLaundering distribution:")
print(df.select(pl.col('Is Laundering').value_counts()).collect())

# Check currencies
print("\nTop currencies:")
print(df.select(pl.col('Receiving Currency').value_counts().head(10)).collect())
```

**Ch·∫°y**: `python explore_fast.py`  
**Th·ªùi gian**: 10-20 gi√¢y

---

## B∆∞·ªõc 2: Feature Engineering v·ªõi Polars (FAST)

```python
# prepare_polars.py
import polars as pl
import numpy as np

print("Loading with Polars...")
df = pl.read_csv('HI-Large_Trans.csv')

print("Feature engineering...")
df_features = df.select([
    # Parse timestamp
    pl.col('Timestamp').str.strptime(pl.Datetime, format='%Y-%m-%d %H:%M:%S').alias('datetime'),
    
    # Basic features
    pl.col('Amount Received').alias('amount_received'),
    pl.col('Amount Paid').alias('amount_paid'),
    
    # Derived features
    (pl.col('Amount Received') / (pl.col('Amount Paid') + 1e-6)).alias('amount_ratio'),
    
    # Time features
    pl.col('Timestamp').str.strptime(pl.Datetime, format='%Y-%m-%d %H:%M:%S').dt.hour().alias('hour'),
    pl.col('Timestamp').str.strptime(pl.Datetime, format='%Y-%m-%d %H:%M:%S').dt.weekday().alias('day_of_week'),
    
    # Route hash
    (pl.col('From Bank').hash() ^ pl.col('To Bank').hash()).alias('route_hash'),
    
    # Categorical
    pl.col('Receiving Currency').alias('recv_curr'),
    pl.col('Payment Currency').alias('payment_curr'),
    pl.col('Payment Format').alias('payment_format'),
])

print("Encoding categoricals...")
# Label encoding
df_features = df_features.with_columns([
    pl.col('recv_curr').cast(pl.Categorical).to_physical().alias('recv_curr_encoded'),
    pl.col('payment_curr').cast(pl.Categorical).to_physical().alias('payment_curr_encoded'),
    pl.col('payment_format').cast(pl.Categorical).to_physical().alias('payment_format_encoded'),
])

# Select numeric features
df_numeric = df_features.select([
    'amount_received',
    'amount_paid', 
    'amount_ratio',
    'hour',
    'day_of_week',
    'route_hash',
    'recv_curr_encoded',
    'payment_curr_encoded',
    'payment_format_encoded',
])

print("Normalizing...")
# Normalize (Polars style)
df_normalized = df_numeric.select([
    ((pl.col(c) - pl.col(c).mean()) / pl.col(c).std()).alias(c)
    for c in df_numeric.columns
])

print("Writing to hadoop_input.txt...")
df_normalized.write_csv('hadoop_input.txt', has_header=False)

print(f"‚úÖ Created hadoop_input.txt with {len(df_normalized)} rows")
print(f"   Features: {df_normalized.columns}")
```

**Ch·∫°y**: `python prepare_polars.py`  
**Th·ªùi gian**: 10-15 ph√∫t (vs 30-60 ph√∫t v·ªõi Pandas)

---

## B∆∞·ªõc 3: Kh·ªüi t·∫°o centroids

```python
# init_centroids.py
import numpy as np
import polars as pl

print("Sampling for centroid initialization...")
# Polars sampling
df = pl.read_csv('hadoop_input.txt', has_header=False, 
                 new_columns=[f'f{i}' for i in range(9)])

# Sample 100k rows
sample = df.sample(n=min(100000, len(df)))
data = sample.to_numpy()

k = 5  # S·ªë clusters
print(f"Initializing {k} centroids from {len(data)} samples...")

indices = np.random.choice(len(data), k, replace=False)
centroids = data[indices]

np.savetxt('centroids.txt', centroids, delimiter=',', fmt='%.6f')
print(f"‚úÖ Saved {k} centroids to centroids.txt")
```

**Ch·∫°y**: `python init_centroids.py`  
**Th·ªùi gian**: 30 gi√¢y

---

## B∆∞·ªõc 4: MapReduce Scripts

### mapper.py
```python
#!/usr/bin/env python3
import sys
import numpy as np

# Load centroids
centroids = np.loadtxt('centroids.txt', delimiter=',')

for line in sys.stdin:
    line = line.strip()
    if not line:
        continue
    
    try:
        point = np.array([float(x) for x in line.split(',')])
        
        # Find closest centroid
        distances = np.linalg.norm(centroids - point, axis=1)
        closest = np.argmin(distances)
        
        print(f"{closest}\t{line}")
    except:
        continue
```

### reducer.py
```python
#!/usr/bin/env python3
import sys
import numpy as np

current_cluster = None
sum_vector = None
count = 0

for line in sys.stdin:
    line = line.strip()
    try:
        cluster, point_str = line.split('\t')
    except:
        continue
    
    if current_cluster != cluster and current_cluster is not None:
        # Output new centroid
        new_centroid = sum_vector / count
        print(','.join(map(str, new_centroid)))
        sum_vector = None
        count = 0
    
    current_cluster = cluster
    point = np.array([float(x) for x in point_str.split(',')])
    
    if sum_vector is None:
        sum_vector = point
    else:
        sum_vector += point
    count += 1

# Last cluster
if count > 0:
    new_centroid = sum_vector / count
    print(','.join(map(str, new_centroid)))
```

```bash
chmod +x mapper.py reducer.py
```

---

## B∆∞·ªõc 5: Ch·∫°y Hadoop (Optimized)

```bash
#!/bin/bash
# run_hadoop_optimized.sh

echo "=== Polars + Hadoop Pipeline for HI-Large_Trans.csv ==="

# Upload to HDFS
echo "Uploading to HDFS..."
hdfs dfs -mkdir -p /user/hadoop/hi_large/input
hdfs dfs -put -f hadoop_input.txt /user/hadoop/hi_large/input/
hdfs dfs -put -f centroids.txt /user/hadoop/hi_large/

# Get CPU cores for tuning
CPU_CORES=$(nproc)
MAP_TASKS=$((CPU_CORES * 2))
REDUCE_TASKS=$((CPU_CORES))

echo "CPU cores: $CPU_CORES"
echo "Map tasks: $MAP_TASKS"
echo "Reduce tasks: $REDUCE_TASKS"

# Iterations
MAX_ITER=15

for i in $(seq 1 $MAX_ITER); do
  echo ""
  echo "=== Iteration $i/$MAX_ITER ==="
  
  # Clean old output
  hdfs dfs -rm -r -f /user/hadoop/hi_large/output_iter_$i
  
  # Run MapReduce with optimizations
  hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \
    -D mapred.map.tasks=$MAP_TASKS \
    -D mapred.reduce.tasks=$REDUCE_TASKS \
    -D mapreduce.input.fileinputformat.split.maxsize=134217728 \
    -D mapreduce.job.reduces=$REDUCE_TASKS \
    -files mapper.py,reducer.py,centroids.txt \
    -mapper "python3 mapper.py" \
    -reducer "python3 reducer.py" \
    -input /user/hadoop/hi_large/input/hadoop_input.txt \
    -output /user/hadoop/hi_large/output_iter_$i
  
  if [ $? -ne 0 ]; then
    echo "‚ùå MapReduce failed at iteration $i"
    exit 1
  fi
  
  # Download new centroids
  hdfs dfs -get /user/hadoop/hi_large/output_iter_$i/part-00000 centroids_new.txt
  
  # Check convergence
  if [ $i -gt 1 ]; then
    diff centroids.txt centroids_new.txt > /dev/null 2>&1
    if [ $? -eq 0 ]; then
      echo "‚úÖ Converged at iteration $i!"
      mv centroids_new.txt final_centroids.txt
      break
    fi
  fi
  
  # Update centroids
  mv centroids_new.txt centroids.txt
  hdfs dfs -put -f centroids.txt /user/hadoop/hi_large/
done

# Save final centroids
if [ ! -f final_centroids.txt ]; then
  cp centroids.txt final_centroids.txt
fi

echo ""
echo "‚úÖ Training complete! Final centroids in final_centroids.txt"
```

**Ch·∫°y**: `bash run_hadoop_optimized.sh`  
**Th·ªùi gian**: 45-60 ph√∫t

---

## B∆∞·ªõc 6: G√°n clusters v·ªõi Polars (FAST)

```python
# assign_clusters_polars.py
import polars as pl
import numpy as np

print("Loading centroids...")
centroids = np.loadtxt('final_centroids.txt', delimiter=',')

print("Loading data...")
df = pl.read_csv('hadoop_input.txt', has_header=False,
                 new_columns=[f'f{i}' for i in range(9)])

print("Computing distances and assigning clusters...")
data = df.to_numpy()

# Vectorized distance calculation (FAST)
distances = np.sqrt(((data[:, None, :] - centroids[None, :, :]) ** 2).sum(axis=2))
clusters = np.argmin(distances, axis=1)

print("Saving results...")
np.savetxt('clustered_results.txt', clusters, fmt='%d')

print(f"‚úÖ Assigned {len(clusters)} transactions to clusters")
print(f"   Results saved to clustered_results.txt")
```

**Ch·∫°y**: `python assign_clusters_polars.py`  
**Th·ªùi gian**: 5-8 ph√∫t

---

## B∆∞·ªõc 7: Ph√¢n t√≠ch v·ªõi Polars

```python
# analyze_polars.py
import polars as pl
import numpy as np

print("Loading results...")
clusters = np.loadtxt('clustered_results.txt', dtype=int)
df_original = pl.read_csv('HI-Large_Trans.csv')

# Add cluster column
df_result = df_original.with_columns(pl.Series('cluster', clusters))

print("\n=== CLUSTER ANALYSIS ===\n")

# Overall statistics
print(f"Total transactions: {len(df_result):,}")
print(f"Number of clusters: {df_result['cluster'].n_unique()}")

# Cluster distribution
print("\n--- Cluster Sizes ---")
cluster_counts = df_result.group_by('cluster').agg(
    pl.count().alias('count')
).sort('cluster')
print(cluster_counts)

# Laundering analysis
print("\n--- Laundering Rate per Cluster ---")
laundering_stats = df_result.group_by('cluster').agg([
    pl.count().alias('total'),
    pl.col('Is Laundering').sum().alias('laundering_count'),
    (pl.col('Is Laundering').sum() / pl.count() * 100).alias('laundering_rate')
]).sort('cluster')

print(laundering_stats)

# Identify high-risk clusters
high_risk = laundering_stats.filter(pl.col('laundering_rate') > 10.0)
print(f"\n‚ö†Ô∏è  HIGH RISK CLUSTERS (>10% laundering):")
print(high_risk)

# Feature analysis per cluster
print("\n--- Feature Averages per Cluster ---")
feature_stats = df_result.group_by('cluster').agg([
    pl.col('Amount Received').mean().alias('avg_amount_received'),
    pl.col('Amount Paid').mean().alias('avg_amount_paid'),
    (pl.col('Amount Received') / pl.col('Amount Paid')).mean().alias('avg_ratio'),
]).sort('cluster')
print(feature_stats)

# Export suspicious transactions
if len(high_risk) > 0:
    high_risk_ids = high_risk['cluster'].to_list()
    print(f"\nüì§ Exporting suspicious transactions from clusters: {high_risk_ids}")
    
    suspicious = df_result.filter(pl.col('cluster').is_in(high_risk_ids))
    suspicious.write_csv('suspicious_transactions.csv')
    print(f"   ‚úÖ Saved {len(suspicious):,} suspicious transactions")

print("\n‚úÖ Analysis complete!")
```

**Ch·∫°y**: `python analyze_polars.py`  
**Th·ªùi gian**: 2-3 ph√∫t

---

## T·ªïng H·ª£p Pipeline

```bash
# full_pipeline.sh
#!/bin/bash

echo "=== POLARS + HADOOP PIPELINE ==="
echo ""

# Step 1
echo "Step 1: Explore data"
python explore_fast.py
echo ""

# Step 2
echo "Step 2: Prepare features with Polars (10-15 min)"
python prepare_polars.py
echo ""

# Step 3
echo "Step 3: Initialize centroids"
python init_centroids.py
echo ""

# Step 4
echo "Step 4: Run Hadoop MapReduce (45-60 min)"
bash run_hadoop_optimized.sh
echo ""

# Step 5
echo "Step 5: Assign clusters with Polars (5-8 min)"
python assign_clusters_polars.py
echo ""

# Step 6
echo "Step 6: Analyze results"
python analyze_polars.py
echo ""

echo "=== DONE! Total time: ~1.5 hours ==="
```

**Ch·∫°y to√†n b·ªô**: `bash full_pipeline.sh`

---

## Th·ªùi gian ∆∞·ªõc t√≠nh

| B∆∞·ªõc | Th·ªùi gian | Tool |
|------|-----------|------|
| 1. Explore | 20 gi√¢y | Polars |
| 2. Prepare | 10-15 ph√∫t | Polars ‚ö° |
| 3. Init centroids | 30 gi√¢y | Polars |
| 4. Hadoop MapReduce | 45-60 ph√∫t | Hadoop |
| 5. Assign clusters | 5-8 ph√∫t | Polars ‚ö° |
| 6. Analyze | 2-3 ph√∫t | Polars ‚ö° |
| **TOTAL** | **~1.5 gi·ªù** | vs 3-5 gi·ªù |

---

## L·ª£i √≠ch Polars

‚úÖ **Nhanh h∆°n 5-10x** so v·ªõi Pandas  
‚úÖ **√çt RAM h∆°n** (streaming-friendly)  
‚úÖ **Lazy evaluation** (ch·ªâ x·ª≠ l√Ω khi c·∫ßn)  
‚úÖ **Parallel by default**  
‚úÖ **Syntax ƒë∆°n gi·∫£n**

---

## Troubleshooting

### Memory issues
```python
# D√πng lazy mode
df = pl.scan_csv('HI-Large_Trans.csv')
df.select([...]).sink_csv('output.txt')  # Stream to disk
```

### Hadoop kh√¥ng t√¨m th·∫•y file
```bash
hdfs dfs -ls /user/hadoop/hi_large/
```

### Slow encoding
```python
# Gi·∫£m s·ªë categories
df = df.with_columns(
    pl.col('Receiving Currency').fill_null('Unknown')
)
```

---

## Output mong ƒë·ª£i

```
Cluster 0: 45M trans, 2.1% laundering   ‚Üí NORMAL
Cluster 1: 28M trans, 16.8% laundering  ‚Üí HIGH RISK ‚ö†Ô∏è
Cluster 2: 38M trans, 3.2% laundering   ‚Üí NORMAL
Cluster 3: 50M trans, 1.9% laundering   ‚Üí NORMAL
Cluster 4: 18M trans, 11.5% laundering  ‚Üí SUSPICIOUS ‚ö†Ô∏è

Exported 46M suspicious transactions to suspicious_transactions.csv
```

---

## Files t·∫°o ra

- `hadoop_input.txt` (~3-4GB normalized features)
- `centroids.txt` (K centroids)
- `final_centroids.txt` (converged centroids)
- `clustered_results.txt` (cluster labels cho m·ªói row)
- `suspicious_transactions.csv` (high-risk transactions)
