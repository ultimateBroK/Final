# Project Overview: HI-Large Transaction Analysis

## ğŸ“‹ MÃ´ táº£ Project

Pipeline phÃ¢n tÃ­ch dá»¯ liá»‡u giao dá»‹ch HI-Large_Trans.csv (16GB, 179M transactions) Ä‘á»ƒ phÃ¡t hiá»‡n giao dá»‹ch rá»­a tiá»n sá»­ dá»¥ng K-means clustering.

### CÃ´ng nghá»‡ sá»­ dá»¥ng:
- **Polars**: Xá»­ lÃ½ dá»¯ liá»‡u nhanh (feature engineering)
- **Apache Spark**: Distributed K-means clustering
- **HDFS**: LÆ°u trá»¯ dá»¯ liá»‡u (tuÃ¢n thá»§ quy Ä‘á»‹nh khÃ´ng lÆ°u local)

---

## ğŸ¯ Má»¥c tiÃªu

1. PhÃ¢n tÃ­ch 179M giao dá»‹ch tá»« file CSV 16GB
2. Clustering báº±ng K-means Ä‘á»ƒ nhÃ³m cÃ¡c pattern giao dá»‹ch
3. PhÃ¡t hiá»‡n clusters cÃ³ tá»· lá»‡ rá»­a tiá»n cao
4. **KHÃ”NG lÆ°u dá»¯ liá»‡u lá»›n á»Ÿ local** (chá»‰ trÃªn HDFS)

---

## ğŸ—ï¸ Kiáº¿n trÃºc

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HDFS-Only Architecture                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  INPUT              PROCESSING           STORAGE          COMPUTE
    â”‚                    â”‚                   â”‚                â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚16GB CSVâ”‚ â”€â”€â”€â”€â”€â”€> â”‚ Polars â”‚ â”€â”€â”€â”€â”€â”€> â”‚  HDFS   â”‚ â”€â”€â”€> â”‚ Spark  â”‚
â”‚Raw Dataâ”‚         â”‚Featuresâ”‚  Upload â”‚ 33GB    â”‚ Read â”‚K-means â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚                                    â”‚
                        â”‚ (auto delete)                      â”‚
                        â–¼                                    â–¼
                  [Temp Deleted]                    [Results HDFS]
                                                           â”‚
                                                           â–¼
                                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                   â”‚data/results/ â”‚
                                                   â”‚(small files) â”‚
                                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### NguyÃªn táº¯c thiáº¿t káº¿:

âœ… **Temp files tá»± Ä‘á»™ng xÃ³a** - KhÃ´ng lÆ°u dá»¯ liá»‡u lá»›n local  
âœ… **HDFS-only storage** - TuÃ¢n thá»§ quy Ä‘á»‹nh  
âœ… **Distributed processing** - Spark xá»­ lÃ½ song song  
âœ… **Fault-tolerant** - HDFS replication

---

## ğŸ“‚ Cáº¥u trÃºc Project

```
Final/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Input: HI-Large_Trans.csv (16GB)
â”‚   â”œâ”€â”€ processed/              # Temp (tá»± Ä‘á»™ng xÃ³a sau upload)
â”‚   â””â”€â”€ results/                # Káº¿t quáº£ nhá» tá»« HDFS
â”‚
|â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ polars/                 # Data processing
â”‚   â”‚   â”œâ”€â”€ explore_fast.py         â†’ KhÃ¡m phÃ¡ dá»¯ liá»‡u
â”‚   â”‚   â”œâ”€â”€ prepare_polars.py       â†’ Feature engineering
â”‚   â”‚   â”œâ”€â”€ assign_clusters_polars.py â†’ GÃ¡n labels
â”‚   â”‚   â””â”€â”€ analyze_polars.py       â†’ PhÃ¢n tÃ­ch káº¿t quáº£
â”‚   â”‚
â”‚   â”œâ”€â”€ spark/                  # Distributed computing
â”‚   â”‚   â”œâ”€â”€ setup_hdfs.sh           â†’ Upload & delete temps
â”‚   â”‚   â”œâ”€â”€ run_spark.sh            â†’ Cháº¡y Spark job
â”‚   â”‚   â”œâ”€â”€ kmeans_spark.py         â†’ PySpark K-means
â”‚   â”‚   â””â”€â”€ download_from_hdfs.sh   â†’ Táº£i káº¿t quáº£
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline/               # Orchestration
â”‚   â”‚   â”œâ”€â”€ full_pipeline_spark.sh  â†’ ToÃ n bá»™ pipeline
â”‚   â”‚   â”œâ”€â”€ clean_spark.sh          â†’ Dá»n dáº¹p project
â”‚   â”‚   â””â”€â”€ reset_pipeline.sh       â†’ Reset checkpoints
â”‚   â”‚
â”‚   â””â”€â”€ setup/
â”‚       â””â”€â”€ (manual per docs)
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ tong-quan.md                â†’ Document nÃ y
â”‚   â”œâ”€â”€ hadoop-alternatives.md      â†’ So sÃ¡nh phÆ°Æ¡ng phÃ¡p
â”‚   â””â”€â”€ jupyter.md                  â†’ Notebook/Jupyter hÆ°á»›ng dáº«n
â”‚
â”œâ”€â”€ logs/                       # Pipeline logs
â”œâ”€â”€ archive/hadoop/             # Legacy MapReduce code
â”œâ”€â”€ CHANGELOG.md
â””â”€â”€ README.md
```

---

## ğŸ”„ Pipeline Workflow

### Tá»•ng quan 7 bÆ°á»›c:

```
[1] Explore â†’ [2] Prepare â†’ [3] Upload â†’ [4] Spark (MLlib) â†’ [5] Download â†’ [6] Assign â†’ [7] Analyze
     9s         33s         41s        6m 6s               3s          3m 21s      33s
```

âš ï¸ **Thay Ä‘á»•i quan trá»ng**: BÆ°á»›c khá»Ÿi táº¡o centroids (bÆ°á»›c 3 cÅ©) Ä‘Ã£ **loáº¡i bá»** vÃ¬ MLlib K-means tá»± Ä‘á»™ng dÃ¹ng **k-means++**!

### Chi tiáº¿t tá»«ng bÆ°á»›c:

#### 1. Explore Data (`explore_fast.py`)
- Scan CSV vá»›i Polars (lazy loading)
- Xem schema, sample, distribution
- **Thá»i gian thá»±c táº¿**: 9 giÃ¢y

#### 2. Feature Engineering (`prepare_polars.py`)
- Parse timestamp â†’ hour, day_of_week
- TÃ­nh amount_ratio, route_hash
- Encode categorical features
- Normalize táº¥t cáº£ features
- **Output**: `data/processed/hadoop_input_temp.txt` (temp)
- **Thá»i gian thá»±c táº¿**: 33 giÃ¢y

#### ~~3. Initialize Centroids~~ âŒ **ÄÃƒ LOáº I Bá»**
- MLlib K-means tá»± Ä‘á»™ng sá»­ dá»¥ng k-means++ Ä‘á»ƒ khá»Ÿi táº¡o centroids
- KhÃ´ng cáº§n file `centroids.txt` ná»¯a
- Tiáº¿t kiá»‡m 30 giÃ¢y vÃ  cho káº¿t quáº£ tá»‘t hÆ¡n

#### 3. Upload to HDFS (`setup_hdfs.sh`)
- Upload `hadoop_input_temp.txt` â†’ `/user/spark/hi_large/input/hadoop_input.txt`
- **XÃ“A tá»± Ä‘á»™ng** temp files tá»« `data/processed/`
- **Thá»i gian thá»±c táº¿**: 41 giÃ¢y

#### 4. Spark K-means (`run_spark.sh`)
- Cháº¡y `kmeans_spark.py` vá»›i spark-submit
- Äá»c dá»¯ liá»‡u tá»« HDFS
- **MLlib K-means vá»›i k-means++ initialization**
- LÆ°u final centroids trÃªn HDFS
- **Thá»i gian thá»±c táº¿**: 6 phÃºt 6 giÃ¢y

#### 5. Download Results (`download_from_hdfs.sh`)
- Táº£i final centroids tá»« HDFS
- LÆ°u vÃ o `data/results/final_centroids.txt`
- **Thá»i gian thá»±c táº¿**: 3 giÃ¢y

#### 6. Assign Clusters (`assign_clusters_polars.py`)
- Äá»c raw CSV + final centroids
- TÃ­nh khoáº£ng cÃ¡ch, gÃ¡n cluster cho má»—i transaction
- **Output**: `data/results/clustered_results.txt`
- **Thá»i gian thá»±c táº¿**: 3 phÃºt 21 giÃ¢y

#### 7. Analyze (`analyze_polars.py`)
- PhÃ¢n tÃ­ch tá»· lá»‡ laundering per cluster
- TÃ¬m high-risk clusters (>10% laundering)
- Export suspicious transactions
- **Output**: Reports, suspicious_transactions.csv
- **Thá»i gian thá»±c táº¿**: 33 giÃ¢y

---

## âš¡ Quick Start

### CÃ i Ä‘áº·t:

```bash
# 1. Äáº£m báº£o Spark/Hadoop Ä‘Ã£ cÃ i vÃ  HDFS Ä‘ang cháº¡y
#    (yÃªu cáº§u cÃ³ sáºµn spark-submit, hdfs trong PATH)

# 2. CÃ i Python packages
pip install polars numpy pyspark

# 3. Äáº·t CSV vÃ o data/raw/
cp /path/to/HI-Large_Trans.csv data/raw/
```

### Cháº¡y Pipeline:

```bash
# ToÃ n bá»™ pipeline tá»± Ä‘á»™ng
./scripts/pipeline/full_pipeline_spark.sh

# Log sáº½ lÆ°u táº¡i: logs/pipeline_log_*.md
```

### Dá»n dáº¹p:

```bash
# Reset toÃ n bá»™
./scripts/pipeline/clean_all.sh

# Chá»‰ reset checkpoints
./scripts/pipeline/reset_pipeline.sh
```

---

## ğŸ“Š HDFS Data Structure

```
/user/spark/hi_large/
â”œâ”€â”€ input/
â”‚   â””â”€â”€ hadoop_input.txt        # 33GB normalized data
â”œâ”€â”€ centroids.txt               # (legacy) initial centroids (khÃ´ng báº¯t buá»™c)
â””â”€â”€ output_centroids/           # Final centroids after convergence
    â””â”€â”€ part-00000
```

### Kiá»ƒm tra:

```bash
# List files
hdfs dfs -ls -R /user/spark/hi_large/

# Kiá»ƒm tra kÃ­ch thÆ°á»›c
hdfs dfs -du -h /user/spark/hi_large/

# Xem centroids
hdfs dfs -cat /user/spark/hi_large/output_centroids/part-00000
```

---

## ğŸ¯ Performance

### Thá»i gian thá»±c táº¿ (theo log `pipeline_log_20251030_093506.md`):

| BÆ°á»›c | Thá»i gian | Tool |
|------|-----------|------|
| 1. Explore | 9s | Polars |
| 2. Prepare | 33s | Polars |
| ~~3. Init~~ | ~~30s~~ (loáº¡i bá») | ~~NumPy~~ |
| 3. Upload | 41s | HDFS |
| 4. K-means | 6m 6s | Spark MLlib |
| 5. Download | 3s | HDFS |
| 6. Assign | 3m 21s | Polars + NumPy |
| 7. Analyze | 33s | Polars |
| **TOTAL** | **11m 27s** | (log thá»±c táº¿) |

### So sÃ¡nh vá»›i Hadoop MapReduce:

- Hadoop legacy: **1-2 giá»** (chá»‰ K-means)
- Spark RDD (cÅ©): **15-30 phÃºt** (chá»‰ K-means)
- **Spark MLlib (má»›i)**: **10-25 phÃºt** (cháº¡y K-means)
- **TÄƒng tá»‘c tá»•ng**: 5-12x so vá»›i Hadoop, 30-50% so vá»›i RDD

---

## ğŸ†• Snapshot gáº§n nháº¥t

- TÃªn snapshot: `snapshot_20251030_095037`
- Thá»i gian: `2025-10-30 09:50:37`
- ThÆ° má»¥c: `snapshots/snapshot_20251030_095037/`
- ThÃ nh pháº§n:
  - `final_centroids.txt` (~436 B)
  - `clustered_results.txt` (~342.75 MB)
  - `suspicious_transactions.csv` (~558 B)
  - `pipeline_log.md`
  - `metadata.json`
- Tá»•ng dung lÆ°á»£ng (Æ°á»›c tÃ­nh): ~342.75 MB

Tham kháº£o bÃ¡o cÃ¡o chi tiáº¿t: `bao_cao_du_an.md` (Ä‘Ã£ Ä‘á»“ng bá»™ theo snapshot nÃ y).

## ğŸ›¡ï¸ Quy táº¯c HDFS-Only

### âœ… ÄÆ¯á»¢C PHÃ‰P lÆ°u local:

- Raw CSV ban Ä‘áº§u (`data/raw/`)
- Káº¿t quáº£ nhá» tá»« HDFS (`data/results/`)
- Logs (`logs/`)
- Code, docs

### âŒ KHÃ”NG ÄÆ¯á»¢C lÆ°u local:

- Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ (hadoop_input.txt)
- Intermediate results
- Large temporary files

### ğŸ”„ Workflow tuÃ¢n thá»§:

1. Polars táº¡o temp files trong `data/processed/`
2. `setup_hdfs.sh` upload lÃªn HDFS
3. Script **Tá»° Äá»˜NG XÃ“A** temp files ngay sau upload
4. Dá»¯ liá»‡u chá»‰ tá»“n táº¡i trÃªn HDFS
5. MLlib K-means tá»± Ä‘á»™ng khá»Ÿi táº¡o centroids vá»›i k-means++
6. Chá»‰ táº£i vá» káº¿t quáº£ nhá» (centroids, reports)

---

## ğŸ”§ Troubleshooting

### Pipeline fails á»Ÿ bÆ°á»›c 4 (upload HDFS):

```bash
# Kiá»ƒm tra HDFS running
hdfs dfsadmin -report

# Kiá»ƒm tra permissions
hdfs dfs -ls /user/spark/

# Táº¡o directory náº¿u chÆ°a cÃ³
hdfs dfs -mkdir -p /user/spark/hi_large/input
```

### Out of memory trong Spark:

Sá»­a `scripts/spark/run_spark.sh`:

```bash
--driver-memory 8g \        # TÄƒng tá»« 4g
--executor-memory 8g \      # TÄƒng tá»« 4g
```

### Temp files khÃ´ng tá»± Ä‘á»™ng xÃ³a:

Kiá»ƒm tra `scripts/spark/setup_hdfs.sh`:

```bash
# Äáº£m báº£o cÃ³ dÃ²ng nÃ y á»Ÿ cuá»‘i script:
rm -rf "$PROJECT_ROOT/data/processed/"*
```

### Polars bÃ¡o lá»—i memory:

Giáº£m batch size hoáº·c dÃ¹ng lazy mode:

```python
# Thay vÃ¬ read_csv
df = pl.scan_csv('file.csv')  # Lazy
df.sink_csv('output.csv')     # Stream to disk
```

---

## ğŸ“ˆ Monitoring

### Xem logs pipeline:

```bash
# Log má»›i nháº¥t
tail -f logs/pipeline_log_*.md

# Grep errors
grep "ERROR\|FAILED" logs/pipeline_log_*.md
```

### Monitor Spark job:

```bash
# Spark UI (náº¿u cháº¡y)
# Má»Ÿ browser: `http://localhost:4040`

# Xem Spark history
spark-history-server start
# Browser: `http://localhost:18080`
```

### Checkpoints:

Pipeline sá»­ dá»¥ng checkpoints Ä‘á»ƒ trÃ¡nh cháº¡y láº¡i cÃ¡c bÆ°á»›c Ä‘Ã£ hoÃ n thÃ nh:

```bash
# Xem checkpoints
ls -la .pipeline_checkpoints/

# Reset Ä‘á»ƒ cháº¡y láº¡i tá»« Ä‘áº§u
./scripts/pipeline/reset_pipeline.sh
```

---

## ğŸ“ TÃ i liá»‡u tham kháº£o

- **README.md**: Quick start guide
- **hadoop-alternatives.md**: So sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p clustering
- **jupyter.md**: HÆ°á»›ng dáº«n Jupyter/Notebook
- **changelog.md**: Lá»‹ch sá»­ thay Ä‘á»•i project

---

## ğŸ‘¥ Development Guidelines

### ThÃªm bÆ°á»›c má»›i vÃ o pipeline:

1. Táº¡o script trong thÆ° má»¥c phÃ¹ há»£p (`scripts/polars/` hoáº·c `scripts/spark/`)
2. Cáº­p nháº­t `scripts/pipeline/full_pipeline_spark.sh`
3. ThÃªm checkpoint náº¿u cáº§n
4. Update docs (README.md, tong-quan.md)

### Testing:

```bash
# Test tá»«ng bÆ°á»›c riÃªng láº»
python scripts/polars/explore_fast.py

# Test vá»›i sample nhá»
head -n 100000 data/raw/HI-Large_Trans.csv > data/raw/sample.csv
# Sá»­a scripts Ä‘á»ƒ Ä‘á»c sample.csv
```

### Git workflow:

```bash
# KhÃ´ng commit large files
git add scripts/ docs/ README.md CHANGELOG.md
git commit -m "Update documentation"

# Ignore data files (Ä‘Ã£ cÃ³ trong .gitignore)
```

---

## ğŸ“ Support

Náº¿u gáº·p váº¥n Ä‘á»:

1. Kiá»ƒm tra logs: `logs/pipeline_log_*.md`
2. Xem troubleshooting section á»Ÿ trÃªn
3. Kiá»ƒm tra HDFS status: `hdfs dfsadmin -report`
4. Check Spark UI: `http://localhost:4040`

---

**Last Updated**: 2025-10-31  
**Project Version**: 2.1 (Spark MLlib)
