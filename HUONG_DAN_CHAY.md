# ğŸš€ HÆ¯á»šNG DáºªN CHáº Y PIPELINE

## Má»¥c lá»¥c
- [Giá»›i thiá»‡u](#gioi-thieu)
- [Chuáº©n bá»‹](#chuan-bi)
- [Pipeline 8 bÆ°á»›c](#pipeline-8-buoc)
- [Cháº¡y tá»± Ä‘á»™ng](#chay-tu-dong)
- [Xá»­ lÃ½ lá»—i thÆ°á»ng gáº·p](#loi-thuong-gap)
- [Xem káº¿t quáº£](#xem-ket-qua)
- [Dá»n dáº¹p sau khi xong](#don-dep)
- [Checklist trÆ°á»›c khi cháº¡y](#checklist)
- [TÃ i liá»‡u tham kháº£o](#tai-lieu)

<a id="gioi-thieu"></a>
## Giá»›i thiá»‡u

Pipeline nÃ y xá»­ lÃ½ 179 triá»‡u giao dá»‹ch (16GB) Ä‘á»ƒ phÃ¡t hiá»‡n rá»­a tiá»n báº±ng K-means clustering.

**Thá»i gian tá»•ng**: ~40-60 phÃºt  
**CÃ´ng nghá»‡**: Polars + Apache Spark + HDFS

---

<a id="chuan-bi"></a>
## ğŸ“‹ Chuáº©n bá»‹

### 1. Kiá»ƒm tra mÃ´i trÆ°á»ng

```bash
# Java
java -version  # Cáº§n version 11 hoáº·c 17

# HDFS
hdfs dfsadmin -report  # Pháº£i tháº¥y "Live datanodes"

# Python packages
python -c "import polars, numpy, pyspark"  # KhÃ´ng lá»—i

# File CSV
ls -lh data/raw/HI-Large_Trans.csv  # Pháº£i ~16GB
```

### 2. Khá»Ÿi Ä‘á»™ng HDFS (náº¿u chÆ°a cháº¡y)

```bash
# Start HDFS
start-dfs.sh

# Äá»£i 10 giÃ¢y, rá»“i kiá»ƒm tra
hdfs dfsadmin -report
```

---

<a id="pipeline-8-buoc"></a>
## ğŸ”„ PIPELINE 8 BÆ¯á»šC

### BÆ¯á»šC 1: KhÃ¡m phÃ¡ dá»¯ liá»‡u ğŸ”

**Má»¥c Ä‘Ã­ch**: Hiá»ƒu cáº¥u trÃºc CSV, xem thá»‘ng kÃª  
**Thá»i gian**: ~30 giÃ¢y

```bash
cd /home/ultimatebrok/Downloads/Final
python scripts/polars/explore_fast.py
```

**Output**: In ra mÃ n hÃ¬nh
- Schema (cáº¥u trÃºc 11 cá»™t)
- Sample 100K dÃ²ng
- Thá»‘ng kÃª mÃ´ táº£
- Tá»· lá»‡ rá»­a tiá»n (~0.13%)
- Top 10 loáº¡i tiá»n tá»‡

---

### BÆ¯á»šC 2: Xá»­ lÃ½ vÃ  trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng ğŸ”§

**Má»¥c Ä‘Ã­ch**: Chuyá»ƒn dá»¯ liá»‡u thÃ´ thÃ nh dáº¡ng sá»‘  
**Thá»i gian**: ~10 phÃºt

```bash
python scripts/polars/prepare_polars.py
```

**CÃ´ng viá»‡c**:
1. Parse timestamp â†’ hour, day_of_week
2. TÃ­nh amount_ratio
3. Hash route (From Bank + To Bank)
4. Encode categorical (currency, payment format)
5. Normalize táº¥t cáº£ vá» [0, 1]

**Output**: `data/processed/hadoop_input_temp.txt` (33GB, Táº M THá»œI)

âš ï¸  **LÆ¯U Ã**: File nÃ y sáº½ Bá»Š XÃ“A tá»± Ä‘á»™ng á»Ÿ bÆ°á»›c 4!

---

### BÆ¯á»šC 3: Khá»Ÿi táº¡o tÃ¢m cá»¥m ğŸ¯

**Má»¥c Ä‘Ã­ch**: Chá»n 5 Ä‘iá»ƒm lÃ m tÃ¢m cá»¥m ban Ä‘áº§u  
**Thá»i gian**: ~30 giÃ¢y

```bash
python scripts/polars/init_centroids.py
```

**Thuáº­t toÃ¡n**:
- Sample 100K dÃ²ng
- Chá»n ngáº«u nhiÃªn K=5 Ä‘iá»ƒm
- Má»—i Ä‘iá»ƒm cÃ³ 9 features

**Output**: `data/processed/centroids_temp.txt` (~440 bytes, Táº M THá»œI)

---

### BÆ¯á»šC 4: Upload lÃªn HDFS â˜ï¸

**Má»¥c Ä‘Ã­ch**: Chuyá»ƒn dá»¯ liá»‡u lÃªn há»‡ thá»‘ng phÃ¢n tÃ¡n  
**Thá»i gian**: ~5 phÃºt

```bash
./scripts/spark/setup_hdfs.sh
```

**CÃ´ng viá»‡c**:
1. Kiá»ƒm tra HDFS running
2. Táº¡o thÆ° má»¥c /user/spark/hi_large/
3. Upload hadoop_input_temp.txt (33GB)
4. Upload centroids_temp.txt (440 bytes)
5. **ğŸ—‘ï¸  Tá»° Äá»˜NG XÃ“A temp files**
6. Verify uploads thÃ nh cÃ´ng

**Káº¿t quáº£**:
- Dá»¯ liá»‡u CHá»ˆ tá»“n táº¡i trÃªn HDFS
- KHÃ”NG cÃ²n file lá»›n á»Ÿ local
- TuÃ¢n thá»§ quy Ä‘á»‹nh báº£o máº­t âœ…

**Kiá»ƒm tra**:
```bash
# Xem dá»¯ liá»‡u trÃªn HDFS
hdfs dfs -ls -R /user/spark/hi_large/

# Kiá»ƒm tra kÃ­ch thÆ°á»›c
hdfs dfs -du -h /user/spark/hi_large/
```

---

### BÆ¯á»šC 5: Cháº¡y K-means trÃªn Spark ğŸš€

**Má»¥c Ä‘Ã­ch**: PhÃ¢n cá»¥m 179M giao dá»‹ch  
**Thá»i gian**: 15-30 phÃºt (tÃ¹y pháº§n cá»©ng)

```bash
./scripts/spark/run_spark.sh
```

**Thuáº­t toÃ¡n K-means**:
```
KHá»I Táº O:
  - K=5 tÃ¢m cá»¥m ban Ä‘áº§u
  - Max iterations = 15

Láº¶P Láº I:
  1. GÃ¡n má»—i Ä‘iá»ƒm vÃ o cá»¥m gáº§n nháº¥t
     (tÃ­nh khoáº£ng cÃ¡ch Euclidean)
  
  2. Cáº­p nháº­t tÃ¢m cá»¥m
     (trung bÃ¬nh táº¥t cáº£ Ä‘iá»ƒm trong cá»¥m)
  
  3. Kiá»ƒm tra há»™i tá»¥
     (náº¿u shift < threshold â†’ dá»«ng)

Káº¾T QUáº¢:
  - 5 tÃ¢m cá»¥m cuá»‘i cÃ¹ng
  - PhÃ¢n phá»‘i giao dá»‹ch trong tá»«ng cá»¥m
```

**QuÃ¡ trÃ¬nh há»™i tá»¥** (tá»« log thá»±c táº¿):
```
Iteration  1: shift = 2.232  (chÆ°a á»•n Ä‘á»‹nh)
Iteration  5: shift = 0.383
Iteration 10: shift = 0.046
Iteration 15: shift = 0.010  (Ä‘Ã£ há»™i tá»¥ âœ“)
```

**Monitor**:
```bash
# Spark UI (khi Ä‘ang cháº¡y)
# Má»Ÿ browser: http://localhost:4040
```

---

### BÆ¯á»šC 6: Táº£i káº¿t quáº£ vá» ğŸ“¥

**Má»¥c Ä‘Ã­ch**: Láº¥y tÃ¢m cá»¥m cuá»‘i cÃ¹ng tá»« HDFS  
**Thá»i gian**: ~30 giÃ¢y

```bash
./scripts/spark/download_from_hdfs.sh
```

**Output**: `data/results/final_centroids.txt` (~4KB)

**Táº¡i sao Ä‘Æ°á»£c phÃ©p táº£i vá»?**
- File ráº¥t nhá» (~4KB)
- Chá»‰ chá»©a káº¿t quáº£ tá»•ng há»£p (5 tÃ¢m cá»¥m)
- KhÃ´ng pháº£i dá»¯ liá»‡u gá»‘c

---

### BÆ¯á»šC 7: GÃ¡n nhÃ£n cá»¥m ğŸ·ï¸

**Má»¥c Ä‘Ã­ch**: XÃ¡c Ä‘á»‹nh má»—i giao dá»‹ch thuá»™c cá»¥m nÃ o  
**Thá»i gian**: ~10 phÃºt

```bash
python scripts/polars/assign_clusters_polars.py
```

**Thuáº­t toÃ¡n**:
```python
FOR má»—i giao dá»‹ch:
    # TÃ­nh khoáº£ng cÃ¡ch Ä‘áº¿n 5 tÃ¢m cá»¥m
    distances = [dist(giao_dá»‹ch, tÃ¢m_i) for i in range(5)]
    
    # Chá»n cá»¥m gáº§n nháº¥t
    cluster_id = argmin(distances)
```

**Tá»‘i Æ°u**: Xá»­ lÃ½ 1M giao dá»‹ch/batch thay vÃ¬ tá»«ng cÃ¡i

**Output**: `data/results/clustered_results.txt`

---

### BÆ¯á»šC 8: PhÃ¢n tÃ­ch káº¿t quáº£ ğŸ“Š

**Má»¥c Ä‘Ã­ch**: TÃ¬m cá»¥m cÃ³ tá»· lá»‡ rá»­a tiá»n cao  
**Thá»i gian**: ~2 phÃºt

```bash
python scripts/polars/analyze_polars.py
```

**PhÃ¢n tÃ­ch**:
1. KÃ­ch thÆ°á»›c má»—i cá»¥m
2. Tá»· lá»‡ rá»­a tiá»n trong tá»«ng cá»¥m
3. High-risk clusters (>10% laundering)
4. Feature averages per cluster

**Káº¿t quáº£ máº«u**:
```
â•”â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Cluster  â•‘ Giao dá»‹ch   â•‘ Rá»­a tiá»n  â•‘ Tá»· lá»‡ (%)       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘    0     â•‘ 40,034,832  â•‘  52,327   â•‘ 0.13%           â•‘
â•‘    1     â•‘ 42,665,746  â•‘  70,450   â•‘ 0.17% â† CAO     â•‘
â•‘    2     â•‘ 24,884,738  â•‘  16,686   â•‘ 0.07%           â•‘
â•‘    3     â•‘ 50,933,651  â•‘  82,943   â•‘ 0.16%           â•‘
â•‘    4     â•‘ 21,183,262  â•‘   3,140   â•‘ 0.01% â† THáº¤P    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

<a id="chay-tu-dong"></a>
## ğŸ¯ CHáº Y Tá»° Äá»˜NG (Khuyáº¿n nghá»‹)

Thay vÃ¬ cháº¡y tá»«ng bÆ°á»›c, dÃ¹ng script tá»± Ä‘á»™ng:

```bash
./scripts/pipeline/full_pipeline_spark.sh
```

Pipeline sáº½ tá»± Ä‘á»™ng:
- Cháº¡y 8 bÆ°á»›c liÃªn tiáº¿p
- Checkpoint (bá» qua bÆ°á»›c Ä‘Ã£ hoÃ n thÃ nh)
- Log chi tiáº¿t vÃ o `logs/pipeline_log_*.md`

**Thá»i gian**: 40-60 phÃºt

---

<a id="loi-thuong-gap"></a>
## ğŸ”§ Xá»¬ LÃ Lá»–I THÆ¯á»œNG Gáº¶P

### Lá»—i 1: HDFS khÃ´ng cháº¡y

**Triá»‡u chá»©ng**:
```
Connection refused
```

**Giáº£i phÃ¡p**:
```bash
# Khá»Ÿi Ä‘á»™ng láº¡i HDFS
stop-dfs.sh
start-dfs.sh

# Äá»£i 10 giÃ¢y, kiá»ƒm tra
hdfs dfsadmin -report
```

---

### Lá»—i 2: Out of Memory

**Triá»‡u chá»©ng**:
```
java.lang.OutOfMemoryError: Java heap space
```

**Giáº£i phÃ¡p**: TÄƒng memory trong `scripts/spark/run_spark.sh`
```bash
# Sá»­a dÃ²ng:
--driver-memory 8g \      # TÄƒng tá»« 4g
--executor-memory 8g \    # TÄƒng tá»« 4g
```

---

### Lá»—i 3: File temp khÃ´ng tá»± Ä‘á»™ng xÃ³a

**Triá»‡u chá»©ng**: Váº«n tháº¥y file trong `data/processed/`

**Giáº£i phÃ¡p**:
```bash
# XÃ³a thá»§ cÃ´ng
rm -rf data/processed/*

# Hoáº·c dÃ¹ng script cleanup
./scripts/pipeline/clean_spark.sh
```

---

<a id="xem-ket-qua"></a>
## ğŸ“Š XEM Káº¾T QUáº¢

### Log pipeline
```bash
cat logs/pipeline_log_*.md
```

### TÃ¢m cá»¥m cuá»‘i cÃ¹ng
```bash
cat data/results/final_centroids.txt
```

### Dá»¯ liá»‡u Ä‘Ã£ gÃ¡n nhÃ£n (10 dÃ²ng Ä‘áº§u)
```bash
head data/results/clustered_results.txt
```

---

<a id="don-dep"></a>
## ğŸ§¹ Dá»ŒN Dáº¸P SAU KHI XONG

### Reset toÃ n bá»™
```bash
./scripts/pipeline/clean_spark.sh
```

XÃ³a:
- Logs
- Temp files
- Checkpoints
- Results

### Chá»‰ reset checkpoints (cháº¡y láº¡i tá»« Ä‘áº§u)
```bash
./scripts/pipeline/reset_pipeline.sh
```

---

## ğŸ’¡ Máº¸O & Gá»¢I Ã

### 1. TÄƒng tá»‘c Ä‘á»™
- TÄƒng RAM: Sá»­a `--executor-memory` trong `run_spark.sh`
- TÄƒng CPU cores: Sá»­a `TOTAL_CORES` trong `run_spark.sh`

### 2. Debug tá»«ng bÆ°á»›c
Náº¿u pipeline lá»—i, cháº¡y tá»«ng bÆ°á»›c riÃªng Ä‘á»ƒ tÃ¬m lá»—i:
```bash
python scripts/polars/explore_fast.py
python scripts/polars/prepare_polars.py
# ... vÃ  tiáº¿p tá»¥c
```

### 3. Test vá»›i sample nhá»
Äá»ƒ test nhanh, dÃ¹ng sample nhá»:
```bash
# Táº¡o sample 100K dÃ²ng
head -n 100000 data/raw/HI-Large_Trans.csv > data/raw/sample.csv

# Sá»­a DATA_RAW trong scripts Ä‘á»ƒ dÃ¹ng sample.csv
# Rá»“i cháº¡y pipeline
```

### 4. Theo dÃµi tiáº¿n trÃ¬nh
```bash
# Terminal 1: Cháº¡y pipeline
./scripts/pipeline/full_pipeline_spark.sh

# Terminal 2: Theo dÃµi log real-time
tail -f logs/pipeline_log_*.md
```

---

<a id="checklist"></a>
## âœ… CHECKLIST TRÆ¯á»šC KHI CHáº Y

- [ ] Java installed (version 11 hoáº·c 17)
- [ ] HDFS Ä‘ang cháº¡y (`hdfs dfsadmin -report`)
- [ ] Spark installed (`spark-submit --version`)
- [ ] Python packages installed (`polars`, `numpy`, `pyspark`)
- [ ] File CSV tá»“n táº¡i (`data/raw/HI-Large_Trans.csv`)
- [ ] Disk space Ä‘á»§ (>50GB trá»‘ng)
- [ ] RAM Ä‘á»§ (>16GB, khuyáº¿n nghá»‹ 32GB)

---

<a id="tai-lieu"></a>
## ğŸ“š TÃ€I LIá»†U THAM KHáº¢O

- **BAO_CAO_TIEU_LUAN.md**: BÃ¡o cÃ¡o chi tiáº¿t báº±ng tiáº¿ng Viá»‡t
- **README.md**: Quick start guide
- **docs/PROJECT_OVERVIEW.md**: Kiáº¿n trÃºc há»‡ thá»‘ng
- **CHANGELOG.md**: Lá»‹ch sá»­ thay Ä‘á»•i

---

## ğŸ†˜ Cáº¦N TRá»¢ GIÃšP?

1. Xem log: `logs/pipeline_log_*.md`
2. Xem troubleshooting trong `BAO_CAO_TIEU_LUAN.md`
3. Check HDFS: `hdfs dfsadmin -report`
4. Check Spark UI: `http://localhost:4040`

---

**ChÃºc báº¡n cháº¡y thÃ nh cÃ´ng! ğŸ‰**
